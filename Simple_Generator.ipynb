{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphing with Generator and Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import glob\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg19, resnet18\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_patch(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator_patch, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, kernel_size=4, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [nn.Conv2d(512, 1, kernel_size=4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Discriminator_classify(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator_classify, self).__init__()\n",
    "\n",
    "        # 之前的卷积层保持不变\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # 添加一个全局平均池化层\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # 最后一个卷积层，将特征图压缩为1个值\n",
    "        self.final_conv = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "        # 选择性添加，如果您需要输出概率\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.sigmoid(x)  # 如果您需要输出概率\n",
    "        return x.view(-1)  # 改变输出形状以匹配期望的输出形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(128*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 根据你的模型调整尺寸\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"%sA\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"%sB\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        # if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "        #     image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        # else:\n",
    "        #     image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        # item_B = self.transform(image_B)\n",
    "        return item_A\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"combined\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=BATCH_SIZE,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform2, unaligned=True, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=BATCH_SIZE,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm1 = nn.InstanceNorm2d(in_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm2 = nn.InstanceNorm2d(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        return identity + out\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(DownsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downsample_blocks = nn.ModuleList([\n",
    "            DownsampleBlock(64, 32),\n",
    "            # DownsampleBlock(32, 16),\n",
    "            DownsampleBlock(32, 2)\n",
    "        ])\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(2) for _ in range(n_residual_blocks)]\n",
    "        )\n",
    "        self.upsample_blocks = nn.ModuleList([\n",
    "            UpsampleBlock(2, 32),\n",
    "            # UpsampleBlock(16, 32),\n",
    "            UpsampleBlock(32, 64)\n",
    "        ])\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        # print(\"1\",x.shape)\n",
    "        for down_block in self.downsample_blocks:\n",
    "            x = down_block(x)\n",
    "        # print(\"2\",x.shape)\n",
    "        x = self.residual_blocks(x)\n",
    "        # print(\"3\",x.shape)\n",
    "        for up_block in self.upsample_blocks:\n",
    "            x = up_block(x)\n",
    "        # print(\"4\",x.shape)\n",
    "        x = self.output_conv(x)\n",
    "        # print(\"5\",x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)\n",
    "G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator_classify(input_nc=3).to(device)\n",
    "D_B = Discriminator_classify(input_nc=3).to(device)\n",
    "\n",
    "D_A_P = Discriminator_patch(input_nc=3).to(device)\n",
    "D_B_P = Discriminator_patch(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(G_AB.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "\n",
    "optimizer_D_A_P = Adam(D_A_P.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "optimizer_D_B_P = Adam(D_B_P.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "\n",
    "G_AB.train()\n",
    "G_BA.train()\n",
    "D_A.train()\n",
    "D_B.train()\n",
    "D_A_P.train()\n",
    "D_B_P.train()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 加载ResNet模型\n",
    "resnet = resnet18(pretrained=True)\n",
    "\n",
    "# 获取全连接层之前的特征提取部分\n",
    "features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# 定义新的全连接层和ReLU激活函数\n",
    "num_ftrs = resnet.fc.in_features\n",
    "fc_layer = nn.Linear(num_ftrs, 256)\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "# 定义模型结构\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, features, fc_layer, relu, num_classes=1000):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.features = features\n",
    "        self.fc_layer = fc_layer\n",
    "        self.relu = relu\n",
    "        self.fc_out = nn.Linear(256, num_classes)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet18(pretrained=True)\n",
    "# num_ftrs = resnet.fc.in_features\n",
    "# resnet.fc = nn.Linear(num_ftrs, 256)\n",
    "# resnet.fc = nn.Linear(32168, 65536)\n",
    "# resnet = torch.load(\"models\\\\animal_rec.pth\")\n",
    "# resnet.fc_out = torch.nn.Identity()\n",
    "# resnet.relu = torch.nn.Identity()\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        # self.vgg = vgg19(pretrained=True).features[:21]  # 只使用到第三个池化层之前的部分\n",
    "        self.resnet = resnet\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        # 冻结VGG参数\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "\n",
    "        self.resnet = self.resnet.to(device)\n",
    "        gen_features = self.resnet(generated)\n",
    "\n",
    "        target_features = self.resnet(target)\n",
    "\n",
    "        return self.loss(gen_features, target_features)\n",
    "\n",
    "# 实例化感知损失\n",
    "perceptual_loss = PerceptualLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet1 = torch.load(\"models\\\\my_animal_rec.pth\")\n",
    "# resnet1 = resnet1.to(device)\n",
    "# resnet1.fc_out = torch.nn.Identity()\n",
    "# resnet1.flatten = torch.nn.Identity()\n",
    "# resnet1.fc_outpout = torch.nn.Identity()\n",
    "# resnet1.average_pool1 = torch.nn.Identity()\n",
    "# resnet1.average_pool2 = torch.nn.Identity()\n",
    "\n",
    "# resnet1.eval()\n",
    "# print(resnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Sample: 0, Loss_G: 2.5264790058135986, Loss_D_A: 0.001318928087130189\n",
      "Epoch: 0, Batch: 10, Sample: 320, Loss_G: 2.287508249282837, Loss_D_A: 0.0007821845938451588\n",
      "Epoch: 0, Batch: 20, Sample: 640, Loss_G: 2.728877067565918, Loss_D_A: 0.001232016016729176\n",
      "Epoch: 0, Batch: 30, Sample: 960, Loss_G: 2.860361099243164, Loss_D_A: 0.0008277383749373257\n",
      "Epoch: 0, Batch: 40, Sample: 1280, Loss_G: 2.3529469966888428, Loss_D_A: 0.0007132506580092013\n",
      "Epoch: 0, Batch: 50, Sample: 1600, Loss_G: 2.566781997680664, Loss_D_A: 0.0007759677828289568\n",
      "Epoch: 0, Batch: 60, Sample: 1920, Loss_G: 2.197385787963867, Loss_D_A: 0.0007282739970833063\n",
      "Epoch: 0, Batch: 70, Sample: 2240, Loss_G: 2.3435182571411133, Loss_D_A: 0.0008177623385563493\n",
      "Epoch: 0, Batch: 80, Sample: 2560, Loss_G: 2.4956488609313965, Loss_D_A: 0.0006511450046673417\n",
      "Epoch: 0, Batch: 90, Sample: 2880, Loss_G: 2.633295774459839, Loss_D_A: 0.0008009803132154047\n",
      "Epoch: 0, Batch: 100, Sample: 3200, Loss_G: 2.741966724395752, Loss_D_A: 0.0008261178154498339\n",
      "Epoch: 0, Batch: 110, Sample: 3520, Loss_G: 2.2744619846343994, Loss_D_A: 0.0006661135121248662\n",
      "Epoch: 0, Batch: 120, Sample: 3840, Loss_G: 2.5794460773468018, Loss_D_A: 0.0008135053794831038\n",
      "Epoch: 0, Batch: 130, Sample: 4160, Loss_G: 2.5189990997314453, Loss_D_A: 0.0006426854524761438\n",
      "Epoch: 0, Batch: 140, Sample: 4480, Loss_G: 2.5182361602783203, Loss_D_A: 0.0009542264742776752\n",
      "Epoch: 0, Batch: 150, Sample: 4800, Loss_G: 2.48634934425354, Loss_D_A: 0.0006349011091515422\n",
      "Epoch: 0, Batch: 160, Sample: 5120, Loss_G: 2.056933641433716, Loss_D_A: 0.0005995950195938349\n",
      "Epoch: 0, Batch: 170, Sample: 5440, Loss_G: 2.451526165008545, Loss_D_A: 0.0005565772880800068\n",
      "Epoch: 0, Batch: 180, Sample: 5760, Loss_G: 2.8766555786132812, Loss_D_A: 0.0005271933041512966\n",
      "Epoch: 0, Batch: 190, Sample: 6080, Loss_G: 2.63112735748291, Loss_D_A: 0.005631022155284882\n",
      "Epoch: 0, Batch: 200, Sample: 6400, Loss_G: 2.3384346961975098, Loss_D_A: 0.059105850756168365\n",
      "Epoch: 0, Batch: 210, Sample: 6720, Loss_G: 2.5713584423065186, Loss_D_A: 0.014695764519274235\n",
      "Epoch: 0, Batch: 220, Sample: 7040, Loss_G: 2.731390953063965, Loss_D_A: 0.0063063958659768105\n",
      "Epoch: 0, Batch: 230, Sample: 7360, Loss_G: 2.6817336082458496, Loss_D_A: 0.004639269318431616\n",
      "Epoch: 0, Batch: 240, Sample: 7680, Loss_G: 2.1052279472351074, Loss_D_A: 0.0031017675064504147\n",
      "Epoch: 0, Batch: 250, Sample: 8000, Loss_G: 2.2695984840393066, Loss_D_A: 0.0027409368194639683\n",
      "Epoch: 0, Batch: 260, Sample: 8320, Loss_G: 2.1312639713287354, Loss_D_A: 0.0021035366225987673\n",
      "Epoch: 0, Batch: 270, Sample: 8640, Loss_G: 2.4849703311920166, Loss_D_A: 0.0020134400110691786\n",
      "Epoch: 0, Batch: 280, Sample: 8960, Loss_G: 2.462080478668213, Loss_D_A: 0.0020582624711096287\n",
      "Epoch: 0, Batch: 290, Sample: 9280, Loss_G: 2.2786922454833984, Loss_D_A: 0.018306057900190353\n",
      "Epoch: 0, Batch: 300, Sample: 9600, Loss_G: 1.9931342601776123, Loss_D_A: 0.002143008867278695\n",
      "Epoch: 0, Batch: 310, Sample: 9920, Loss_G: 2.2735307216644287, Loss_D_A: 0.001764405518770218\n",
      "Epoch: 0, Batch: 320, Sample: 10240, Loss_G: 2.2689366340637207, Loss_D_A: 0.0013928563566878438\n",
      "Epoch: 0, Batch: 330, Sample: 10560, Loss_G: 2.1383473873138428, Loss_D_A: 0.001397728337906301\n",
      "Epoch: 0, Batch: 340, Sample: 10880, Loss_G: 2.0601646900177, Loss_D_A: 0.0013071622233837843\n",
      "Epoch: 0, Batch: 350, Sample: 11200, Loss_G: 2.2910125255584717, Loss_D_A: 0.0009074431727640331\n",
      "Epoch: 0, Batch: 360, Sample: 11520, Loss_G: 2.5256052017211914, Loss_D_A: 0.0013888718094676733\n",
      "Epoch: 0, Batch: 370, Sample: 11840, Loss_G: 2.2252895832061768, Loss_D_A: 0.0007627715822309256\n",
      "Epoch: 0, Batch: 380, Sample: 12160, Loss_G: 2.4691405296325684, Loss_D_A: 0.001316028879955411\n",
      "Epoch: 0, Batch: 390, Sample: 12480, Loss_G: 2.2723989486694336, Loss_D_A: 0.0009407567558810115\n",
      "Epoch: 0, Batch: 400, Sample: 12800, Loss_G: 2.6012024879455566, Loss_D_A: 0.0011555553646758199\n",
      "Epoch: 0, Batch: 410, Sample: 13120, Loss_G: 2.443636894226074, Loss_D_A: 0.0006362256244756281\n",
      "Epoch: 0, Batch: 420, Sample: 13440, Loss_G: 1.8649133443832397, Loss_D_A: 0.0007244774024002254\n",
      "Epoch: 0, Batch: 430, Sample: 13760, Loss_G: 2.7172460556030273, Loss_D_A: 0.0008852275786921382\n",
      "Epoch: 0, Batch: 440, Sample: 14080, Loss_G: 2.49908185005188, Loss_D_A: 0.0007295687682926655\n",
      "Epoch: 0, Batch: 450, Sample: 14400, Loss_G: 2.0461699962615967, Loss_D_A: 0.0005538333207368851\n",
      "Epoch: 0, Batch: 460, Sample: 14720, Loss_G: 2.1199023723602295, Loss_D_A: 0.000508213066495955\n"
     ]
    }
   ],
   "source": [
    "G_AB.train()\n",
    "for epoch in range(1):\n",
    "    for i, (real_A) in enumerate(train_dataloader):\n",
    "        # print(real_A.shape)\n",
    "        real_A = real_A.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_A = G_AB(real_A)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        pred_fake_patch = D_A_P(fake_A)\n",
    "        loss_G_S = criterion_GAN(real_A,fake_A)\n",
    "        loss_G_P = perceptual_loss(real_A,fake_A)\n",
    "        # loss_A = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        # loss_A_P = criterion_GAN(pred_fake_patch, torch.ones_like(pred_fake_patch))\n",
    "        \n",
    "\n",
    "\n",
    "        # 总损失\n",
    "        # loss_G = loss_G_S+loss_G_P+loss_A+loss_A_P\n",
    "        loss_G = loss_G_S+loss_G_P\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        \n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_A_P.zero_grad()\n",
    "\n",
    "        pred_real = D_A_P(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A_P(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A_P = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A_P.backward()\n",
    "        optimizer_D_A_P.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Sample: {i*BATCH_SIZE}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}')\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     # 使用测试集中的数据生成图像\n",
    "    #     for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "    #         real_A = real_A.to(device)\n",
    "    #         fake_B = G_AB(real_A)\n",
    "    #         vutils.save_image(fake_B, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at models\\Simple_CNN.ckpt\n"
     ]
    }
   ],
   "source": [
    "save_model_path='models'\n",
    "checkpoint_path = os.path.join(save_model_path, \"Simple_CNN2.ckpt\")\n",
    "# checkpoint_path = os.path.join(save_model_path, \"Cycle_GAN_Monet2Photo_PerceptualLoss2.ckpt\")\n",
    "torch.save(G_AB.state_dict(), checkpoint_path)\n",
    "print(\"Model saved at %s\" % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"residual_blocks.6.conv1.weight\", \"residual_blocks.6.conv2.weight\", \"residual_blocks.7.conv1.weight\", \"residual_blocks.7.conv2.weight\", \"residual_blocks.8.conv1.weight\", \"residual_blocks.8.conv2.weight\". \n\tsize mismatch for downsample_blocks.0.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 3, 3]).\n\tsize mismatch for downsample_blocks.1.conv.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 16, 3, 3]).\n\tsize mismatch for residual_blocks.0.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.0.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.1.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.2.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.2.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.3.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.3.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.4.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.4.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.5.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.5.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for upsample_blocks.0.conv.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 16, 3, 3]).\n\tsize mismatch for upsample_blocks.1.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[200], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mG_AB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCycle_GAN_Human2Dog_PerceptualLoss2_id.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m G_AB\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"residual_blocks.6.conv1.weight\", \"residual_blocks.6.conv2.weight\", \"residual_blocks.7.conv1.weight\", \"residual_blocks.7.conv2.weight\", \"residual_blocks.8.conv1.weight\", \"residual_blocks.8.conv2.weight\". \n\tsize mismatch for downsample_blocks.0.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 3, 3]).\n\tsize mismatch for downsample_blocks.1.conv.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 16, 3, 3]).\n\tsize mismatch for residual_blocks.0.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.0.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.1.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.2.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.2.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.3.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.3.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.4.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.4.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.5.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for residual_blocks.5.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3]).\n\tsize mismatch for upsample_blocks.0.conv.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 16, 3, 3]).\n\tsize mismatch for upsample_blocks.1.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 3, 3])."
     ]
    }
   ],
   "source": [
    "G_AB.load_state_dict(torch.load(\"models\\Cycle_GAN_Human2Dog_PerceptualLoss2_id.ckpt\"))\n",
    "G_AB.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (initial_conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (downsample_blocks): ModuleList(\n",
       "    (0): DownsampleBlock(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DownsampleBlock(\n",
       "      (conv): Conv2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (residual_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (upsample_blocks): ModuleList(\n",
       "    (0): UpsampleBlock(\n",
       "      (conv): ConvTranspose2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): UpsampleBlock(\n",
       "      (conv): ConvTranspose2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_conv): Sequential(\n",
       "    (0): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AB.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # 根据你的模型调整尺寸\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # 添加批次维度\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image1,image2=None):\n",
    "    # image1 = load_image(image_path1).to(device)\n",
    "    # image2 = load_image(image_path2).to(device)\n",
    "\n",
    "    model= model.to(device)\n",
    "    for i in range(11):\n",
    "        with torch.no_grad():  # 不计算梯度\n",
    "            image=image1*(i/10)+image2*(1-i/10)\n",
    "            output = model(image)\n",
    "            # vutils.save_image(output, f'test18_3 {i}.png', normalize=True)\n",
    "            \n",
    "            output = output.cpu().detach()  # 将图像转移到CPU并脱离计算图\n",
    "            \n",
    "            output = (output + 1) / 2  # 将 [-1, 1] 范围调整为 [0, 1]\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            image = output.permute(1, 2, 0)  # CHW -> HWC\n",
    "            image = (image.numpy() * 255).astype(np.uint8)  # 转换为0-255范围的整数\n",
    "            image_pil = Image.fromarray(image)\n",
    "            \n",
    "            # 保存图像\n",
    "            image_filename = f'test19_3 {i}.png'\n",
    "            image_pil.save(image_filename)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = load_image('human_dog\\\\testA\\\\200600.jpg').to(device)\n",
    "image2 = load_image('human_dog\\\\testB\\\\flickr_dog_000043.jpg').to(device)\n",
    "\n",
    "# image1_feature = resnet1(image1)\n",
    "# image1_feature = image1_feature.unsqueeze(1).unsqueeze(1).expand(-1,-1,256,256)\n",
    "# image1_with_feature = torch.cat([image1,image1_feature],dim=1)\n",
    "\n",
    "# image2_feature = resnet1(image2)\n",
    "# image2_feature = image2_feature.unsqueeze(1).unsqueeze(1).expand(-1,-1,256,256)\n",
    "# image2_with_feature = torch.cat([image2,image2_feature],dim=1)\n",
    "\n",
    "output_image = predict(G_AB,image1 ,image2)\n",
    "\n",
    "# output_image = output_image - output_image.min()\n",
    "# output_image = output_image / output_image.max()\n",
    "\n",
    "# output_image = output_image.squeeze()  # 假设输出是图像格式，调整通道\n",
    "# output_image = output_image.permute(1,2,0)\n",
    "# output_image=output_image.to('cpu')\n",
    "# # 步骤 5: 可视化输出图像\n",
    "# plt.imshow(output_image.numpy())\n",
    "# plt.title('Output Image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (initial_conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (downsample_blocks): ModuleList(\n",
       "    (0): DownsampleBlock(\n",
       "      (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DownsampleBlock(\n",
       "      (conv): Conv2d(32, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (residual_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (upsample_blocks): ModuleList(\n",
       "    (0): UpsampleBlock(\n",
       "      (conv): ConvTranspose2d(2, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): UpsampleBlock(\n",
       "      (conv): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_conv): Sequential(\n",
       "    (0): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AB.load_state_dict(torch.load(\"models\\Simple_CNN.ckpt\"))\n",
    "G_AB.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # 根据你的模型调整尺寸\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # 添加批次维度\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import vgg19\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "\n",
    "image1 = load_image('human_dog\\\\testA\\\\200601.jpg').to(device)\n",
    "image2 = load_image('human_dog\\\\testB\\\\flickr_dog_000043.jpg').to(device)\n",
    "perceptual_distance = PerceptualLoss()\n",
    "\n",
    "def calculate_ppl(model, device, image1,image2, steps=10):\n",
    "    distances = []\n",
    "\n",
    "    model= model.to(device)\n",
    "\n",
    "    path_length = 0\n",
    "    for t in torch.linspace(0, 1, steps=steps):\n",
    "        with torch.no_grad():  # 不计算梯度\n",
    "            z_t = (1 - t) * image1 + t * image2\n",
    "            \n",
    "            img_t = model(z_t)\n",
    "            img_t_plus_1 = model((1 - t + 1/steps) * image1 + (t + 1/steps) * image2)\n",
    "\n",
    "            d = perceptual_distance(img_t, img_t_plus_1)\n",
    "            \n",
    "            path_length += d.item()\n",
    "\n",
    "    distances.append(path_length / (steps - 1))\n",
    "\n",
    "    ppl = torch.tensor(distances).mean().item()\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016179127618670464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ppl(G_AB,device,image1 ,image2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
