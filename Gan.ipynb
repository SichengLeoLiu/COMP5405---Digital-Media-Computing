{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import glob\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.Conv2d(64, output_nc, kernel_size=7, padding=3), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, kernel_size=4, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [nn.Conv2d(512, 1, kernel_size=4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(256*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"%sA\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"%sB\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"train\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=4,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=4,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)\n",
    "G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator(input_nc=3).to(device)\n",
    "D_B = Discriminator(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss_G: 15.61037826538086, Loss_D_A: 1.1615573167800903, Loss_D_B: 0.9101475477218628\n",
      "Epoch: 0, Batch: 10, Loss_G: 6.970175743103027, Loss_D_A: 0.3666701912879944, Loss_D_B: 0.24359501898288727\n",
      "Epoch: 0, Batch: 20, Loss_G: 5.965689659118652, Loss_D_A: 0.24649454653263092, Loss_D_B: 0.24288181960582733\n",
      "Epoch: 0, Batch: 30, Loss_G: 5.182547569274902, Loss_D_A: 0.24978424608707428, Loss_D_B: 0.2856913208961487\n",
      "Epoch: 0, Batch: 40, Loss_G: 7.472266674041748, Loss_D_A: 0.3758556544780731, Loss_D_B: 0.32502102851867676\n",
      "Epoch: 0, Batch: 50, Loss_G: 7.623085975646973, Loss_D_A: 0.23080357909202576, Loss_D_B: 0.24449694156646729\n",
      "Epoch: 0, Batch: 60, Loss_G: 6.9660868644714355, Loss_D_A: 0.26649582386016846, Loss_D_B: 0.23975417017936707\n",
      "Epoch: 0, Batch: 70, Loss_G: 5.563512325286865, Loss_D_A: 0.24589692056179047, Loss_D_B: 0.2512208819389343\n",
      "Epoch: 0, Batch: 80, Loss_G: 6.209434986114502, Loss_D_A: 0.30268868803977966, Loss_D_B: 0.2863529324531555\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_B = G_AB(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "        fake_A = G_BA(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "        # 循环一致性损失\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * 10.0\n",
    "        \n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}, Loss_D_B: {loss_D_B.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 使用测试集中的数据生成图像\n",
    "        for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "            real_A = real_A.to(device)\n",
    "            fake_B = G_AB(real_A)\n",
    "            vutils.save_image(fake_B, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Inversion\n",
    "\n",
    "Train the model to convert A to A. Then interpolation can be used to generate intermediate image. \n",
    "\n",
    "Using contrastive loss to make the model to learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, 'combined') + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, 'combined') + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"train\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=12,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=12,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "# G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator(input_nc=3).to(device)\n",
    "D_B = Discriminator(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(G_AA.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss_G: 6.2875518798828125, Loss_D_A: 0.6166337728500366\n",
      "Epoch: 0, Batch: 10, Loss_G: 3.730794906616211, Loss_D_A: 0.24838754534721375\n",
      "Epoch: 0, Batch: 20, Loss_G: 2.7318222522735596, Loss_D_A: 0.25245800614356995\n",
      "Epoch: 0, Batch: 30, Loss_G: 2.937140464782715, Loss_D_A: 0.344274640083313\n",
      "Epoch: 0, Batch: 40, Loss_G: 2.3008639812469482, Loss_D_A: 0.24993270635604858\n",
      "Epoch: 0, Batch: 50, Loss_G: 2.1108765602111816, Loss_D_A: 0.24796178936958313\n",
      "Epoch: 0, Batch: 60, Loss_G: 3.3187010288238525, Loss_D_A: 0.2761794924736023\n",
      "Epoch: 0, Batch: 70, Loss_G: 2.0487430095672607, Loss_D_A: 0.2466328740119934\n",
      "Epoch: 0, Batch: 80, Loss_G: 2.0849242210388184, Loss_D_A: 0.2562412917613983\n",
      "Epoch: 0, Batch: 90, Loss_G: 2.538743257522583, Loss_D_A: 0.2563542127609253\n",
      "Epoch: 0, Batch: 100, Loss_G: 2.0033926963806152, Loss_D_A: 0.24686303734779358\n",
      "Epoch: 0, Batch: 110, Loss_G: 2.5875730514526367, Loss_D_A: 0.2730735242366791\n",
      "Epoch: 0, Batch: 120, Loss_G: 2.3883023262023926, Loss_D_A: 0.2963287830352783\n",
      "Epoch: 0, Batch: 130, Loss_G: 2.847687005996704, Loss_D_A: 0.3868718147277832\n",
      "Epoch: 0, Batch: 140, Loss_G: 2.2599668502807617, Loss_D_A: 0.3447938561439514\n",
      "Epoch: 0, Batch: 150, Loss_G: 1.8020461797714233, Loss_D_A: 0.25630277395248413\n",
      "Epoch: 0, Batch: 160, Loss_G: 1.7732343673706055, Loss_D_A: 0.2549082338809967\n",
      "Epoch: 0, Batch: 170, Loss_G: 1.9625627994537354, Loss_D_A: 0.2496122270822525\n",
      "Epoch: 0, Batch: 180, Loss_G: 2.4018044471740723, Loss_D_A: 0.24929560720920563\n",
      "Epoch: 0, Batch: 190, Loss_G: 1.8442994356155396, Loss_D_A: 0.2504672408103943\n",
      "Epoch: 0, Batch: 200, Loss_G: 1.634864091873169, Loss_D_A: 0.24936208128929138\n",
      "Epoch: 0, Batch: 210, Loss_G: 1.7978179454803467, Loss_D_A: 0.2632372975349426\n",
      "Epoch: 0, Batch: 220, Loss_G: 1.8554773330688477, Loss_D_A: 0.2512657046318054\n",
      "Epoch: 0, Batch: 230, Loss_G: 1.7744100093841553, Loss_D_A: 0.2462649792432785\n",
      "Epoch: 0, Batch: 240, Loss_G: 2.0197691917419434, Loss_D_A: 0.2498674988746643\n",
      "Epoch: 0, Batch: 250, Loss_G: 1.8351244926452637, Loss_D_A: 0.24768294394016266\n",
      "Epoch: 0, Batch: 260, Loss_G: 1.7634217739105225, Loss_D_A: 0.2550742030143738\n",
      "Epoch: 0, Batch: 270, Loss_G: 1.8344258069992065, Loss_D_A: 0.25538012385368347\n",
      "Epoch: 0, Batch: 280, Loss_G: 1.5456163883209229, Loss_D_A: 0.29758164286613464\n",
      "Epoch: 0, Batch: 290, Loss_G: 1.415619969367981, Loss_D_A: 0.24843621253967285\n",
      "Epoch: 0, Batch: 300, Loss_G: 1.7290362119674683, Loss_D_A: 0.24496211111545563\n",
      "Epoch: 0, Batch: 310, Loss_G: 1.8786062002182007, Loss_D_A: 0.2585384249687195\n",
      "Epoch: 0, Batch: 320, Loss_G: 1.5385642051696777, Loss_D_A: 0.2527972459793091\n",
      "Epoch: 0, Batch: 330, Loss_G: 1.8094462156295776, Loss_D_A: 0.24803844094276428\n",
      "Epoch: 0, Batch: 340, Loss_G: 1.610411524772644, Loss_D_A: 0.281344473361969\n",
      "Epoch: 0, Batch: 350, Loss_G: 1.4165737628936768, Loss_D_A: 0.24563491344451904\n",
      "Epoch: 0, Batch: 360, Loss_G: 1.5997592210769653, Loss_D_A: 0.27506768703460693\n",
      "Epoch: 0, Batch: 370, Loss_G: 2.307663917541504, Loss_D_A: 0.2360263168811798\n",
      "Epoch: 0, Batch: 380, Loss_G: 1.3189566135406494, Loss_D_A: 0.25060558319091797\n",
      "Epoch: 0, Batch: 390, Loss_G: 1.4347060918807983, Loss_D_A: 0.2558172345161438\n",
      "Epoch: 0, Batch: 400, Loss_G: 1.6668211221694946, Loss_D_A: 0.25383293628692627\n",
      "Epoch: 0, Batch: 410, Loss_G: 1.8047258853912354, Loss_D_A: 0.2606842517852783\n",
      "Epoch: 0, Batch: 420, Loss_G: 1.7942482233047485, Loss_D_A: 0.38730329275131226\n",
      "Epoch: 0, Batch: 430, Loss_G: 18.180828094482422, Loss_D_A: 17.039113998413086\n",
      "Epoch: 0, Batch: 440, Loss_G: 3.5903658866882324, Loss_D_A: 0.27127203345298767\n",
      "Epoch: 0, Batch: 450, Loss_G: 2.9370758533477783, Loss_D_A: 0.2513512372970581\n",
      "Epoch: 0, Batch: 460, Loss_G: 2.537966251373291, Loss_D_A: 0.24018403887748718\n",
      "Epoch: 0, Batch: 470, Loss_G: 2.430168390274048, Loss_D_A: 0.2481817603111267\n",
      "Epoch: 0, Batch: 480, Loss_G: 2.9565162658691406, Loss_D_A: 0.2755211889743805\n",
      "Epoch: 0, Batch: 490, Loss_G: 2.432239532470703, Loss_D_A: 0.23160509765148163\n",
      "Epoch: 0, Batch: 500, Loss_G: 2.047358512878418, Loss_D_A: 0.24348697066307068\n",
      "Epoch: 0, Batch: 510, Loss_G: 2.8833277225494385, Loss_D_A: 0.24045078456401825\n",
      "Epoch: 0, Batch: 520, Loss_G: 1.9447208642959595, Loss_D_A: 0.22934913635253906\n",
      "Epoch: 0, Batch: 530, Loss_G: 1.9582215547561646, Loss_D_A: 0.24224171042442322\n",
      "Epoch: 0, Batch: 540, Loss_G: 2.241177558898926, Loss_D_A: 0.23306185007095337\n",
      "Epoch: 0, Batch: 550, Loss_G: 1.8340115547180176, Loss_D_A: 0.23877866566181183\n",
      "Epoch: 0, Batch: 560, Loss_G: 2.0156819820404053, Loss_D_A: 0.2399071753025055\n",
      "Epoch: 0, Batch: 570, Loss_G: 2.03529691696167, Loss_D_A: 0.2491583228111267\n",
      "Epoch: 0, Batch: 580, Loss_G: 2.041038990020752, Loss_D_A: 0.23817577958106995\n",
      "Epoch: 0, Batch: 590, Loss_G: 1.8767845630645752, Loss_D_A: 0.24767529964447021\n",
      "Epoch: 0, Batch: 600, Loss_G: 1.6954485177993774, Loss_D_A: 0.24946698546409607\n",
      "Epoch: 0, Batch: 610, Loss_G: 1.6937952041625977, Loss_D_A: 0.2348138689994812\n",
      "Epoch: 0, Batch: 620, Loss_G: 2.00081205368042, Loss_D_A: 0.2503063678741455\n",
      "Epoch: 0, Batch: 630, Loss_G: 1.8286153078079224, Loss_D_A: 0.25001370906829834\n",
      "Epoch: 0, Batch: 640, Loss_G: 1.7967883348464966, Loss_D_A: 0.25400614738464355\n",
      "Epoch: 0, Batch: 650, Loss_G: 2.310614824295044, Loss_D_A: 0.2869996428489685\n",
      "Epoch: 0, Batch: 660, Loss_G: 1.590947151184082, Loss_D_A: 0.24239641427993774\n",
      "Epoch: 0, Batch: 670, Loss_G: 2.052915573120117, Loss_D_A: 0.2598118484020233\n",
      "Epoch: 0, Batch: 680, Loss_G: 1.763202428817749, Loss_D_A: 0.24986597895622253\n",
      "Epoch: 0, Batch: 690, Loss_G: 1.95542573928833, Loss_D_A: 0.24286812543869019\n",
      "Epoch: 0, Batch: 700, Loss_G: 1.8973444700241089, Loss_D_A: 0.23891150951385498\n",
      "Epoch: 0, Batch: 710, Loss_G: 2.026237726211548, Loss_D_A: 0.28927624225616455\n",
      "Epoch: 0, Batch: 720, Loss_G: 2.0372250080108643, Loss_D_A: 0.2873860001564026\n",
      "Epoch: 0, Batch: 730, Loss_G: 1.873692274093628, Loss_D_A: 0.24346575140953064\n",
      "Epoch: 0, Batch: 740, Loss_G: 1.8383128643035889, Loss_D_A: 0.24213942885398865\n",
      "Epoch: 0, Batch: 750, Loss_G: 1.7316750288009644, Loss_D_A: 0.23811741173267365\n",
      "Epoch: 0, Batch: 760, Loss_G: 1.7188265323638916, Loss_D_A: 0.32765263319015503\n",
      "Epoch: 0, Batch: 770, Loss_G: 1.6633433103561401, Loss_D_A: 0.2396421879529953\n",
      "Epoch: 0, Batch: 780, Loss_G: 1.8714632987976074, Loss_D_A: 0.2308874875307083\n",
      "Epoch: 0, Batch: 790, Loss_G: 1.4949259757995605, Loss_D_A: 0.316551148891449\n",
      "Epoch: 0, Batch: 800, Loss_G: 1.4327678680419922, Loss_D_A: 0.24708858132362366\n",
      "Epoch: 0, Batch: 810, Loss_G: 1.8496137857437134, Loss_D_A: 0.3173506259918213\n",
      "Epoch: 0, Batch: 820, Loss_G: 2.3541831970214844, Loss_D_A: 0.281506210565567\n",
      "Epoch: 0, Batch: 830, Loss_G: 1.7569833993911743, Loss_D_A: 0.2553466856479645\n",
      "Epoch: 0, Batch: 840, Loss_G: 1.891211986541748, Loss_D_A: 0.2341039776802063\n",
      "Epoch: 0, Batch: 850, Loss_G: 2.0446646213531494, Loss_D_A: 0.2952776551246643\n",
      "Epoch: 0, Batch: 860, Loss_G: 1.794224500656128, Loss_D_A: 0.309292733669281\n",
      "Epoch: 0, Batch: 870, Loss_G: 1.7332854270935059, Loss_D_A: 0.2528570890426636\n",
      "Epoch: 0, Batch: 880, Loss_G: 2.056612968444824, Loss_D_A: 0.28324899077415466\n",
      "Epoch: 0, Batch: 890, Loss_G: 1.8572869300842285, Loss_D_A: 0.47885042428970337\n",
      "Epoch: 0, Batch: 900, Loss_G: 1.344050407409668, Loss_D_A: 0.2564160227775574\n",
      "Epoch: 0, Batch: 910, Loss_G: 1.416911005973816, Loss_D_A: 0.26758575439453125\n",
      "Epoch: 0, Batch: 920, Loss_G: 1.56072998046875, Loss_D_A: 0.35757142305374146\n",
      "Epoch: 0, Batch: 930, Loss_G: 2.2306063175201416, Loss_D_A: 0.24906691908836365\n",
      "Epoch: 0, Batch: 940, Loss_G: 1.5802700519561768, Loss_D_A: 0.243211567401886\n",
      "Epoch: 0, Batch: 950, Loss_G: 1.7685084342956543, Loss_D_A: 0.24012652039527893\n",
      "Epoch: 0, Batch: 960, Loss_G: 1.625985860824585, Loss_D_A: 0.23610159754753113\n",
      "Epoch: 0, Batch: 970, Loss_G: 1.460556983947754, Loss_D_A: 0.2581450939178467\n",
      "Epoch: 0, Batch: 980, Loss_G: 1.6591733694076538, Loss_D_A: 0.27637988328933716\n",
      "Epoch: 0, Batch: 990, Loss_G: 1.4918147325515747, Loss_D_A: 0.23349547386169434\n",
      "Epoch: 0, Batch: 1000, Loss_G: 1.8066000938415527, Loss_D_A: 0.461378812789917\n",
      "Epoch: 0, Batch: 1010, Loss_G: 1.3797807693481445, Loss_D_A: 0.2336849868297577\n",
      "Epoch: 0, Batch: 1020, Loss_G: 1.8768481016159058, Loss_D_A: 0.2433401346206665\n",
      "Epoch: 0, Batch: 1030, Loss_G: 1.8913031816482544, Loss_D_A: 0.2308005690574646\n",
      "Epoch: 0, Batch: 1040, Loss_G: 1.6635334491729736, Loss_D_A: 0.2387276142835617\n",
      "Epoch: 0, Batch: 1050, Loss_G: 1.3039438724517822, Loss_D_A: 0.2525573968887329\n",
      "Epoch: 0, Batch: 1060, Loss_G: 1.6578909158706665, Loss_D_A: 0.26803499460220337\n",
      "Epoch: 0, Batch: 1070, Loss_G: 1.5501737594604492, Loss_D_A: 0.22977465391159058\n",
      "Epoch: 0, Batch: 1080, Loss_G: 1.5525562763214111, Loss_D_A: 0.1990901529788971\n",
      "Epoch: 0, Batch: 1090, Loss_G: 1.6115773916244507, Loss_D_A: 0.20126044750213623\n",
      "Epoch: 0, Batch: 1100, Loss_G: 1.496601939201355, Loss_D_A: 0.2336139678955078\n",
      "Epoch: 0, Batch: 1110, Loss_G: 1.7016414403915405, Loss_D_A: 0.20002883672714233\n",
      "Epoch: 0, Batch: 1120, Loss_G: 1.2373464107513428, Loss_D_A: 0.3084441125392914\n",
      "Epoch: 0, Batch: 1130, Loss_G: 1.6808258295059204, Loss_D_A: 0.2858641445636749\n",
      "Epoch: 0, Batch: 1140, Loss_G: 1.4963107109069824, Loss_D_A: 0.26214295625686646\n",
      "Epoch: 0, Batch: 1150, Loss_G: 1.7223858833312988, Loss_D_A: 0.2609717845916748\n",
      "Epoch: 0, Batch: 1160, Loss_G: 1.3125547170639038, Loss_D_A: 0.22715215384960175\n",
      "Epoch: 0, Batch: 1170, Loss_G: 1.446040153503418, Loss_D_A: 0.22945672273635864\n",
      "Epoch: 0, Batch: 1180, Loss_G: 1.5063624382019043, Loss_D_A: 0.30178922414779663\n",
      "Epoch: 0, Batch: 1190, Loss_G: 1.4441990852355957, Loss_D_A: 0.2350923717021942\n",
      "Epoch: 0, Batch: 1200, Loss_G: 1.3946276903152466, Loss_D_A: 0.20397129654884338\n",
      "Epoch: 0, Batch: 1210, Loss_G: 1.2898731231689453, Loss_D_A: 0.18251416087150574\n",
      "Epoch: 0, Batch: 1220, Loss_G: 1.539136528968811, Loss_D_A: 0.21683400869369507\n",
      "Epoch: 1, Batch: 0, Loss_G: 2.2582736015319824, Loss_D_A: 0.22574009001255035\n",
      "Epoch: 1, Batch: 10, Loss_G: 1.5044357776641846, Loss_D_A: 0.21067994832992554\n",
      "Epoch: 1, Batch: 20, Loss_G: 1.4446797370910645, Loss_D_A: 0.18879881501197815\n",
      "Epoch: 1, Batch: 30, Loss_G: 1.5759990215301514, Loss_D_A: 0.23548607528209686\n",
      "Epoch: 1, Batch: 40, Loss_G: 1.3292173147201538, Loss_D_A: 0.2221408486366272\n",
      "Epoch: 1, Batch: 50, Loss_G: 1.493721604347229, Loss_D_A: 0.2663835287094116\n",
      "Epoch: 1, Batch: 60, Loss_G: 1.4937338829040527, Loss_D_A: 0.23755121231079102\n",
      "Epoch: 1, Batch: 70, Loss_G: 1.321038007736206, Loss_D_A: 0.26419979333877563\n",
      "Epoch: 1, Batch: 80, Loss_G: 1.5303248167037964, Loss_D_A: 0.17324195802211761\n",
      "Epoch: 1, Batch: 90, Loss_G: 1.512060284614563, Loss_D_A: 0.17824797332286835\n",
      "Epoch: 1, Batch: 100, Loss_G: 1.6369671821594238, Loss_D_A: 0.5212841629981995\n",
      "Epoch: 1, Batch: 110, Loss_G: 1.355893850326538, Loss_D_A: 0.2516937553882599\n",
      "Epoch: 1, Batch: 120, Loss_G: 1.2731702327728271, Loss_D_A: 0.2698865532875061\n",
      "Epoch: 1, Batch: 130, Loss_G: 1.2831194400787354, Loss_D_A: 0.262006551027298\n",
      "Epoch: 1, Batch: 140, Loss_G: 1.3068413734436035, Loss_D_A: 0.26184940338134766\n",
      "Epoch: 1, Batch: 150, Loss_G: 1.3775815963745117, Loss_D_A: 0.25559818744659424\n",
      "Epoch: 1, Batch: 160, Loss_G: 1.5027072429656982, Loss_D_A: 0.25323083996772766\n",
      "Epoch: 1, Batch: 170, Loss_G: 1.479385495185852, Loss_D_A: 0.25987058877944946\n",
      "Epoch: 1, Batch: 180, Loss_G: 1.232068419456482, Loss_D_A: 0.2559400200843811\n",
      "Epoch: 1, Batch: 190, Loss_G: 1.5057069063186646, Loss_D_A: 0.25486570596694946\n",
      "Epoch: 1, Batch: 200, Loss_G: 1.2287728786468506, Loss_D_A: 0.26006263494491577\n",
      "Epoch: 1, Batch: 210, Loss_G: 1.6146554946899414, Loss_D_A: 0.24936792254447937\n",
      "Epoch: 1, Batch: 220, Loss_G: 1.4317303895950317, Loss_D_A: 0.2707417905330658\n",
      "Epoch: 1, Batch: 230, Loss_G: 1.414006233215332, Loss_D_A: 0.25069528818130493\n",
      "Epoch: 1, Batch: 240, Loss_G: 1.4572724103927612, Loss_D_A: 0.25400957465171814\n",
      "Epoch: 1, Batch: 250, Loss_G: 1.3175851106643677, Loss_D_A: 0.25622135400772095\n",
      "Epoch: 1, Batch: 260, Loss_G: 1.2162779569625854, Loss_D_A: 0.28207629919052124\n",
      "Epoch: 1, Batch: 270, Loss_G: 1.1771951913833618, Loss_D_A: 0.24946340918540955\n",
      "Epoch: 1, Batch: 280, Loss_G: 1.2278951406478882, Loss_D_A: 0.26697275042533875\n",
      "Epoch: 1, Batch: 290, Loss_G: 1.5586316585540771, Loss_D_A: 0.28116849064826965\n",
      "Epoch: 1, Batch: 300, Loss_G: 1.5123562812805176, Loss_D_A: 0.28809037804603577\n",
      "Epoch: 1, Batch: 310, Loss_G: 1.203591227531433, Loss_D_A: 0.25975337624549866\n",
      "Epoch: 1, Batch: 320, Loss_G: 1.3225129842758179, Loss_D_A: 0.259456992149353\n",
      "Epoch: 1, Batch: 330, Loss_G: 1.7678332328796387, Loss_D_A: 0.26026004552841187\n",
      "Epoch: 1, Batch: 340, Loss_G: 1.2935925722122192, Loss_D_A: 0.25279247760772705\n",
      "Epoch: 1, Batch: 350, Loss_G: 1.4311597347259521, Loss_D_A: 0.24627330899238586\n",
      "Epoch: 1, Batch: 360, Loss_G: 1.4960455894470215, Loss_D_A: 0.2859240770339966\n",
      "Epoch: 1, Batch: 370, Loss_G: 1.254409670829773, Loss_D_A: 0.2972011864185333\n",
      "Epoch: 1, Batch: 380, Loss_G: 1.2068500518798828, Loss_D_A: 0.25090715289115906\n",
      "Epoch: 1, Batch: 390, Loss_G: 1.1479465961456299, Loss_D_A: 0.2573683559894562\n",
      "Epoch: 1, Batch: 400, Loss_G: 1.3295084238052368, Loss_D_A: 0.256159245967865\n",
      "Epoch: 1, Batch: 410, Loss_G: 1.2680772542953491, Loss_D_A: 0.2599288821220398\n",
      "Epoch: 1, Batch: 420, Loss_G: 1.4474599361419678, Loss_D_A: 0.2481841892004013\n",
      "Epoch: 1, Batch: 430, Loss_G: 1.064005970954895, Loss_D_A: 0.2571167051792145\n",
      "Epoch: 1, Batch: 440, Loss_G: 1.2185323238372803, Loss_D_A: 0.27025505900382996\n",
      "Epoch: 1, Batch: 450, Loss_G: 1.3224714994430542, Loss_D_A: 0.2832805812358856\n",
      "Epoch: 1, Batch: 460, Loss_G: 1.280336856842041, Loss_D_A: 0.31771671772003174\n",
      "Epoch: 1, Batch: 470, Loss_G: 1.5744743347167969, Loss_D_A: 0.3865354061126709\n",
      "Epoch: 1, Batch: 480, Loss_G: 1.4948492050170898, Loss_D_A: 0.3703611493110657\n",
      "Epoch: 1, Batch: 490, Loss_G: 1.124455213546753, Loss_D_A: 0.26514458656311035\n",
      "Epoch: 1, Batch: 500, Loss_G: 1.180415391921997, Loss_D_A: 0.23651419579982758\n",
      "Epoch: 1, Batch: 510, Loss_G: 1.3632347583770752, Loss_D_A: 0.28289005160331726\n",
      "Epoch: 1, Batch: 520, Loss_G: 1.4079208374023438, Loss_D_A: 0.24291184544563293\n",
      "Epoch: 1, Batch: 530, Loss_G: 1.4963942766189575, Loss_D_A: 0.26515063643455505\n",
      "Epoch: 1, Batch: 540, Loss_G: 1.0891270637512207, Loss_D_A: 0.45462191104888916\n",
      "Epoch: 1, Batch: 550, Loss_G: 1.3599357604980469, Loss_D_A: 0.274252325296402\n",
      "Epoch: 1, Batch: 560, Loss_G: 1.3961987495422363, Loss_D_A: 0.1945894956588745\n",
      "Epoch: 1, Batch: 570, Loss_G: 1.1412075757980347, Loss_D_A: 0.2475956380367279\n",
      "Epoch: 1, Batch: 580, Loss_G: 1.1802043914794922, Loss_D_A: 0.2703888416290283\n",
      "Epoch: 1, Batch: 590, Loss_G: 1.1036549806594849, Loss_D_A: 0.22063171863555908\n",
      "Epoch: 1, Batch: 600, Loss_G: 1.3835029602050781, Loss_D_A: 0.16805079579353333\n",
      "Epoch: 1, Batch: 610, Loss_G: 1.2057244777679443, Loss_D_A: 0.22600677609443665\n",
      "Epoch: 1, Batch: 620, Loss_G: 1.7025361061096191, Loss_D_A: 0.4491000175476074\n",
      "Epoch: 1, Batch: 630, Loss_G: 1.2414946556091309, Loss_D_A: 0.22516313195228577\n",
      "Epoch: 1, Batch: 640, Loss_G: 1.568755865097046, Loss_D_A: 0.2582072913646698\n",
      "Epoch: 1, Batch: 650, Loss_G: 1.3151181936264038, Loss_D_A: 0.26533055305480957\n",
      "Epoch: 1, Batch: 660, Loss_G: 1.2823058366775513, Loss_D_A: 0.5153622031211853\n",
      "Epoch: 1, Batch: 670, Loss_G: 1.451485514640808, Loss_D_A: 0.3811146020889282\n",
      "Epoch: 1, Batch: 680, Loss_G: 1.267607569694519, Loss_D_A: 0.26235467195510864\n",
      "Epoch: 1, Batch: 690, Loss_G: 1.6588634252548218, Loss_D_A: 0.2653961777687073\n",
      "Epoch: 1, Batch: 700, Loss_G: 1.2188143730163574, Loss_D_A: 0.2563108205795288\n",
      "Epoch: 1, Batch: 710, Loss_G: 1.5250461101531982, Loss_D_A: 0.23513932526111603\n",
      "Epoch: 1, Batch: 720, Loss_G: 1.9043502807617188, Loss_D_A: 0.5098119378089905\n",
      "Epoch: 1, Batch: 730, Loss_G: 1.8330808877944946, Loss_D_A: 0.22424417734146118\n",
      "Epoch: 1, Batch: 740, Loss_G: 1.156809687614441, Loss_D_A: 0.23569568991661072\n",
      "Epoch: 1, Batch: 750, Loss_G: 1.3160676956176758, Loss_D_A: 0.25382375717163086\n",
      "Epoch: 1, Batch: 760, Loss_G: 1.8429083824157715, Loss_D_A: 0.197555810213089\n",
      "Epoch: 1, Batch: 770, Loss_G: 1.0816404819488525, Loss_D_A: 0.21977268159389496\n",
      "Epoch: 1, Batch: 780, Loss_G: 1.9829035997390747, Loss_D_A: 0.3018243610858917\n",
      "Epoch: 1, Batch: 790, Loss_G: 1.4755892753601074, Loss_D_A: 0.24365690350532532\n",
      "Epoch: 1, Batch: 800, Loss_G: 1.4315733909606934, Loss_D_A: 0.21205784380435944\n",
      "Epoch: 1, Batch: 810, Loss_G: 1.5923666954040527, Loss_D_A: 0.29955920577049255\n",
      "Epoch: 1, Batch: 820, Loss_G: 1.3455469608306885, Loss_D_A: 0.2165221869945526\n",
      "Epoch: 1, Batch: 830, Loss_G: 1.258976697921753, Loss_D_A: 0.17417998611927032\n",
      "Epoch: 1, Batch: 840, Loss_G: 1.3153274059295654, Loss_D_A: 0.25970351696014404\n",
      "Epoch: 1, Batch: 850, Loss_G: 1.2643225193023682, Loss_D_A: 0.2742273509502411\n",
      "Epoch: 1, Batch: 860, Loss_G: 1.3440090417861938, Loss_D_A: 0.24908417463302612\n",
      "Epoch: 1, Batch: 870, Loss_G: 1.4635895490646362, Loss_D_A: 0.21628573536872864\n",
      "Epoch: 1, Batch: 880, Loss_G: 1.128046989440918, Loss_D_A: 0.23737512528896332\n",
      "Epoch: 1, Batch: 890, Loss_G: 1.514174222946167, Loss_D_A: 0.21275800466537476\n",
      "Epoch: 1, Batch: 900, Loss_G: 1.3918880224227905, Loss_D_A: 0.1772480309009552\n",
      "Epoch: 1, Batch: 910, Loss_G: 1.535698413848877, Loss_D_A: 0.21894598007202148\n",
      "Epoch: 1, Batch: 920, Loss_G: 1.3526474237442017, Loss_D_A: 0.25630664825439453\n",
      "Epoch: 1, Batch: 930, Loss_G: 16.464313507080078, Loss_D_A: 17.727279663085938\n",
      "Epoch: 1, Batch: 940, Loss_G: 2.6434826850891113, Loss_D_A: 0.3533087968826294\n",
      "Epoch: 1, Batch: 950, Loss_G: 1.793332576751709, Loss_D_A: 0.2657921314239502\n",
      "Epoch: 1, Batch: 960, Loss_G: 2.029505491256714, Loss_D_A: 0.2744958698749542\n",
      "Epoch: 1, Batch: 970, Loss_G: 1.603392243385315, Loss_D_A: 0.2563115656375885\n",
      "Epoch: 1, Batch: 980, Loss_G: 1.369408369064331, Loss_D_A: 0.2555234432220459\n",
      "Epoch: 1, Batch: 990, Loss_G: 1.2455275058746338, Loss_D_A: 0.25480037927627563\n",
      "Epoch: 1, Batch: 1000, Loss_G: 1.3490028381347656, Loss_D_A: 0.25556716322898865\n",
      "Epoch: 1, Batch: 1010, Loss_G: 1.554417371749878, Loss_D_A: 0.25644201040267944\n",
      "Epoch: 1, Batch: 1020, Loss_G: 1.5258578062057495, Loss_D_A: 0.25412696599960327\n",
      "Epoch: 1, Batch: 1030, Loss_G: 1.2709224224090576, Loss_D_A: 0.25372031331062317\n",
      "Epoch: 1, Batch: 1040, Loss_G: 1.2174229621887207, Loss_D_A: 0.2525865435600281\n",
      "Epoch: 1, Batch: 1050, Loss_G: 1.4036619663238525, Loss_D_A: 0.2574009895324707\n",
      "Epoch: 1, Batch: 1060, Loss_G: 1.2710578441619873, Loss_D_A: 0.2545860707759857\n",
      "Epoch: 1, Batch: 1070, Loss_G: 1.5671579837799072, Loss_D_A: 0.25524675846099854\n",
      "Epoch: 1, Batch: 1080, Loss_G: 1.1138741970062256, Loss_D_A: 0.2529815435409546\n",
      "Epoch: 1, Batch: 1090, Loss_G: 1.4181486368179321, Loss_D_A: 0.25051167607307434\n",
      "Epoch: 1, Batch: 1100, Loss_G: 1.0954012870788574, Loss_D_A: 0.25324204564094543\n",
      "Epoch: 1, Batch: 1110, Loss_G: 1.1289504766464233, Loss_D_A: 0.2583268880844116\n",
      "Epoch: 1, Batch: 1120, Loss_G: 1.0575124025344849, Loss_D_A: 0.25538498163223267\n",
      "Epoch: 1, Batch: 1130, Loss_G: 1.4082763195037842, Loss_D_A: 0.25428688526153564\n",
      "Epoch: 1, Batch: 1140, Loss_G: 1.0045840740203857, Loss_D_A: 0.2545703649520874\n",
      "Epoch: 1, Batch: 1150, Loss_G: 1.1242060661315918, Loss_D_A: 0.24898536503314972\n",
      "Epoch: 1, Batch: 1160, Loss_G: 1.1980565786361694, Loss_D_A: 0.2540312111377716\n",
      "Epoch: 1, Batch: 1170, Loss_G: 0.9951425790786743, Loss_D_A: 0.252414345741272\n",
      "Epoch: 1, Batch: 1180, Loss_G: 1.0679144859313965, Loss_D_A: 0.2515324652194977\n",
      "Epoch: 1, Batch: 1190, Loss_G: 0.9988912343978882, Loss_D_A: 0.2550279498100281\n",
      "Epoch: 1, Batch: 1200, Loss_G: 1.0733270645141602, Loss_D_A: 0.24919608235359192\n",
      "Epoch: 1, Batch: 1210, Loss_G: 1.4249582290649414, Loss_D_A: 0.2534061670303345\n",
      "Epoch: 1, Batch: 1220, Loss_G: 1.1399110555648804, Loss_D_A: 0.2533370852470398\n",
      "Epoch: 2, Batch: 0, Loss_G: 1.2827255725860596, Loss_D_A: 0.25843849778175354\n",
      "Epoch: 2, Batch: 10, Loss_G: 1.335640788078308, Loss_D_A: 0.2513743042945862\n",
      "Epoch: 2, Batch: 20, Loss_G: 0.8674225807189941, Loss_D_A: 0.25286665558815\n",
      "Epoch: 2, Batch: 30, Loss_G: 1.4052209854125977, Loss_D_A: 0.25148287415504456\n",
      "Epoch: 2, Batch: 40, Loss_G: 1.1320409774780273, Loss_D_A: 0.2543688416481018\n",
      "Epoch: 2, Batch: 50, Loss_G: 1.2711236476898193, Loss_D_A: 0.2516614496707916\n",
      "Epoch: 2, Batch: 60, Loss_G: 1.077587604522705, Loss_D_A: 0.2541593909263611\n",
      "Epoch: 2, Batch: 70, Loss_G: 1.3381391763687134, Loss_D_A: 0.254804790019989\n",
      "Epoch: 2, Batch: 80, Loss_G: 1.2968250513076782, Loss_D_A: 0.254006028175354\n",
      "Epoch: 2, Batch: 90, Loss_G: 1.3204832077026367, Loss_D_A: 0.2672405242919922\n",
      "Epoch: 2, Batch: 100, Loss_G: 1.3143559694290161, Loss_D_A: 0.25688713788986206\n",
      "Epoch: 2, Batch: 110, Loss_G: 1.2029458284378052, Loss_D_A: 0.25477540493011475\n",
      "Epoch: 2, Batch: 120, Loss_G: 1.145702838897705, Loss_D_A: 0.26312780380249023\n",
      "Epoch: 2, Batch: 130, Loss_G: 1.1373038291931152, Loss_D_A: 0.26068001985549927\n",
      "Epoch: 2, Batch: 140, Loss_G: 1.0908862352371216, Loss_D_A: 0.2601996064186096\n",
      "Epoch: 2, Batch: 150, Loss_G: 1.2106382846832275, Loss_D_A: 0.2676613926887512\n",
      "Epoch: 2, Batch: 160, Loss_G: 1.0383237600326538, Loss_D_A: 0.26212114095687866\n",
      "Epoch: 2, Batch: 170, Loss_G: 0.947279691696167, Loss_D_A: 0.3055025041103363\n",
      "Epoch: 2, Batch: 180, Loss_G: 1.0622419118881226, Loss_D_A: 0.25258415937423706\n",
      "Epoch: 2, Batch: 190, Loss_G: 1.0038692951202393, Loss_D_A: 0.2524094581604004\n",
      "Epoch: 2, Batch: 200, Loss_G: 1.1894322633743286, Loss_D_A: 0.2576238214969635\n",
      "Epoch: 2, Batch: 210, Loss_G: 1.140657663345337, Loss_D_A: 0.27798324823379517\n",
      "Epoch: 2, Batch: 220, Loss_G: 1.0938522815704346, Loss_D_A: 0.25997939705848694\n",
      "Epoch: 2, Batch: 230, Loss_G: 1.275158405303955, Loss_D_A: 0.2787363529205322\n",
      "Epoch: 2, Batch: 240, Loss_G: 1.0415892601013184, Loss_D_A: 0.2785417437553406\n",
      "Epoch: 2, Batch: 250, Loss_G: 1.2306272983551025, Loss_D_A: 0.2547777593135834\n",
      "Epoch: 2, Batch: 260, Loss_G: 1.0877633094787598, Loss_D_A: 0.2561015784740448\n",
      "Epoch: 2, Batch: 270, Loss_G: 1.1500011682510376, Loss_D_A: 0.27347254753112793\n",
      "Epoch: 2, Batch: 280, Loss_G: 1.2027959823608398, Loss_D_A: 0.32362765073776245\n",
      "Epoch: 2, Batch: 290, Loss_G: 1.224705457687378, Loss_D_A: 0.278985857963562\n",
      "Epoch: 2, Batch: 300, Loss_G: 1.0037572383880615, Loss_D_A: 0.2512638568878174\n",
      "Epoch: 2, Batch: 310, Loss_G: 1.0223989486694336, Loss_D_A: 0.254538357257843\n",
      "Epoch: 2, Batch: 320, Loss_G: 1.3005315065383911, Loss_D_A: 0.34511101245880127\n",
      "Epoch: 2, Batch: 330, Loss_G: 1.0618199110031128, Loss_D_A: 0.3576958477497101\n",
      "Epoch: 2, Batch: 340, Loss_G: 1.4090133905410767, Loss_D_A: 0.2572736144065857\n",
      "Epoch: 2, Batch: 350, Loss_G: 0.9554746150970459, Loss_D_A: 0.2563437819480896\n",
      "Epoch: 2, Batch: 360, Loss_G: 1.0698206424713135, Loss_D_A: 0.2566739320755005\n",
      "Epoch: 2, Batch: 370, Loss_G: 1.0790574550628662, Loss_D_A: 0.2536172866821289\n",
      "Epoch: 2, Batch: 380, Loss_G: 1.2036564350128174, Loss_D_A: 0.2523868680000305\n",
      "Epoch: 2, Batch: 390, Loss_G: 1.2011300325393677, Loss_D_A: 0.2514112591743469\n",
      "Epoch: 2, Batch: 400, Loss_G: 1.332527756690979, Loss_D_A: 0.2570902109146118\n",
      "Epoch: 2, Batch: 410, Loss_G: 0.9687176942825317, Loss_D_A: 0.25374850630760193\n",
      "Epoch: 2, Batch: 420, Loss_G: 1.2020244598388672, Loss_D_A: 0.25361159443855286\n",
      "Epoch: 2, Batch: 430, Loss_G: 1.036433458328247, Loss_D_A: 0.25334763526916504\n",
      "Epoch: 2, Batch: 440, Loss_G: 0.9651793241500854, Loss_D_A: 0.2518686056137085\n",
      "Epoch: 2, Batch: 450, Loss_G: 1.0027228593826294, Loss_D_A: 0.26155078411102295\n",
      "Epoch: 2, Batch: 460, Loss_G: 1.071596384048462, Loss_D_A: 0.2656972110271454\n",
      "Epoch: 2, Batch: 470, Loss_G: 1.0745766162872314, Loss_D_A: 0.24934902787208557\n",
      "Epoch: 2, Batch: 480, Loss_G: 1.1976397037506104, Loss_D_A: 0.25322800874710083\n",
      "Epoch: 2, Batch: 490, Loss_G: 1.0836193561553955, Loss_D_A: 0.25054362416267395\n",
      "Epoch: 2, Batch: 500, Loss_G: 1.1570481061935425, Loss_D_A: 0.2892530560493469\n",
      "Epoch: 2, Batch: 510, Loss_G: 2.029524803161621, Loss_D_A: 0.7333693504333496\n",
      "Epoch: 2, Batch: 520, Loss_G: 0.8575856685638428, Loss_D_A: 0.2553962767124176\n",
      "Epoch: 2, Batch: 530, Loss_G: 1.054008960723877, Loss_D_A: 0.2517934739589691\n",
      "Epoch: 2, Batch: 540, Loss_G: 1.287421464920044, Loss_D_A: 0.25407537817955017\n",
      "Epoch: 2, Batch: 550, Loss_G: 1.1755049228668213, Loss_D_A: 0.2598876953125\n",
      "Epoch: 2, Batch: 560, Loss_G: 1.1273250579833984, Loss_D_A: 0.25271129608154297\n",
      "Epoch: 2, Batch: 570, Loss_G: 1.3510364294052124, Loss_D_A: 0.2556518316268921\n",
      "Epoch: 2, Batch: 580, Loss_G: 1.392290711402893, Loss_D_A: 0.2581656873226166\n",
      "Epoch: 2, Batch: 590, Loss_G: 1.0095429420471191, Loss_D_A: 0.25343775749206543\n",
      "Epoch: 2, Batch: 600, Loss_G: 0.8481578230857849, Loss_D_A: 0.25599566102027893\n",
      "Epoch: 2, Batch: 610, Loss_G: 1.0013225078582764, Loss_D_A: 0.254206120967865\n",
      "Epoch: 2, Batch: 620, Loss_G: 1.2340188026428223, Loss_D_A: 0.256325364112854\n",
      "Epoch: 2, Batch: 630, Loss_G: 1.0549050569534302, Loss_D_A: 0.2628740072250366\n",
      "Epoch: 2, Batch: 640, Loss_G: 1.0360074043273926, Loss_D_A: 0.2848091423511505\n",
      "Epoch: 2, Batch: 650, Loss_G: 1.0557441711425781, Loss_D_A: 0.25306248664855957\n",
      "Epoch: 2, Batch: 660, Loss_G: 1.1323392391204834, Loss_D_A: 0.25231772661209106\n",
      "Epoch: 2, Batch: 670, Loss_G: 0.8654079437255859, Loss_D_A: 0.2531677484512329\n",
      "Epoch: 2, Batch: 680, Loss_G: 1.326240062713623, Loss_D_A: 0.3175961375236511\n",
      "Epoch: 2, Batch: 690, Loss_G: 0.9903885126113892, Loss_D_A: 0.25270795822143555\n",
      "Epoch: 2, Batch: 700, Loss_G: 1.5315309762954712, Loss_D_A: 0.28804296255111694\n",
      "Epoch: 2, Batch: 710, Loss_G: 1.2059166431427002, Loss_D_A: 0.25291842222213745\n",
      "Epoch: 2, Batch: 720, Loss_G: 1.411155343055725, Loss_D_A: 0.28480610251426697\n",
      "Epoch: 2, Batch: 730, Loss_G: 1.127729892730713, Loss_D_A: 0.2515501081943512\n",
      "Epoch: 2, Batch: 740, Loss_G: 1.3481988906860352, Loss_D_A: 0.26334846019744873\n",
      "Epoch: 2, Batch: 750, Loss_G: 0.9381793141365051, Loss_D_A: 0.2523699104785919\n",
      "Epoch: 2, Batch: 760, Loss_G: 1.493009090423584, Loss_D_A: 0.2536981701850891\n",
      "Epoch: 2, Batch: 770, Loss_G: 1.2232682704925537, Loss_D_A: 0.3373250663280487\n",
      "Epoch: 2, Batch: 780, Loss_G: 0.9152133464813232, Loss_D_A: 0.2534818649291992\n",
      "Epoch: 2, Batch: 790, Loss_G: 1.1305198669433594, Loss_D_A: 0.25488170981407166\n",
      "Epoch: 2, Batch: 800, Loss_G: 1.0474555492401123, Loss_D_A: 0.25112423300743103\n",
      "Epoch: 2, Batch: 810, Loss_G: 1.0150383710861206, Loss_D_A: 0.2529757618904114\n",
      "Epoch: 2, Batch: 820, Loss_G: 1.1023797988891602, Loss_D_A: 0.25558215379714966\n",
      "Epoch: 2, Batch: 830, Loss_G: 1.3491501808166504, Loss_D_A: 0.26259762048721313\n",
      "Epoch: 2, Batch: 840, Loss_G: 1.1278351545333862, Loss_D_A: 0.25331175327301025\n",
      "Epoch: 2, Batch: 850, Loss_G: 1.2550042867660522, Loss_D_A: 0.2577648162841797\n",
      "Epoch: 2, Batch: 860, Loss_G: 1.1561659574508667, Loss_D_A: 0.3246954679489136\n",
      "Epoch: 2, Batch: 870, Loss_G: 0.9686509966850281, Loss_D_A: 0.2581391930580139\n",
      "Epoch: 2, Batch: 880, Loss_G: 0.9378483295440674, Loss_D_A: 0.26359203457832336\n",
      "Epoch: 2, Batch: 890, Loss_G: 1.1874855756759644, Loss_D_A: 0.26205721497535706\n",
      "Epoch: 2, Batch: 900, Loss_G: 0.8653445839881897, Loss_D_A: 0.26990053057670593\n",
      "Epoch: 2, Batch: 910, Loss_G: 1.0041024684906006, Loss_D_A: 0.2527611255645752\n",
      "Epoch: 2, Batch: 920, Loss_G: 1.2920186519622803, Loss_D_A: 0.2558833062648773\n",
      "Epoch: 2, Batch: 930, Loss_G: 1.0962700843811035, Loss_D_A: 0.2535552382469177\n",
      "Epoch: 2, Batch: 940, Loss_G: 0.9141116142272949, Loss_D_A: 0.2526111304759979\n",
      "Epoch: 2, Batch: 950, Loss_G: 1.032652497291565, Loss_D_A: 0.25122153759002686\n",
      "Epoch: 2, Batch: 960, Loss_G: 1.205195426940918, Loss_D_A: 0.258137047290802\n",
      "Epoch: 2, Batch: 970, Loss_G: 1.4828827381134033, Loss_D_A: 1.002358317375183\n",
      "Epoch: 2, Batch: 980, Loss_G: 1.1142429113388062, Loss_D_A: 0.3236555755138397\n",
      "Epoch: 2, Batch: 990, Loss_G: 0.9409275054931641, Loss_D_A: 0.2628171443939209\n",
      "Epoch: 2, Batch: 1000, Loss_G: 0.9885128736495972, Loss_D_A: 0.25102508068084717\n",
      "Epoch: 2, Batch: 1010, Loss_G: 1.0702046155929565, Loss_D_A: 0.2542070150375366\n",
      "Epoch: 2, Batch: 1020, Loss_G: 1.1424906253814697, Loss_D_A: 0.2583731710910797\n",
      "Epoch: 2, Batch: 1030, Loss_G: 1.009137511253357, Loss_D_A: 0.24835962057113647\n",
      "Epoch: 2, Batch: 1040, Loss_G: 1.1370811462402344, Loss_D_A: 0.25471481680870056\n",
      "Epoch: 2, Batch: 1050, Loss_G: 1.1481369733810425, Loss_D_A: 0.2532711327075958\n",
      "Epoch: 2, Batch: 1060, Loss_G: 0.9147597551345825, Loss_D_A: 0.25214478373527527\n",
      "Epoch: 2, Batch: 1070, Loss_G: 1.1042495965957642, Loss_D_A: 0.2597762942314148\n",
      "Epoch: 2, Batch: 1080, Loss_G: 0.9238396883010864, Loss_D_A: 0.2541784942150116\n",
      "Epoch: 2, Batch: 1090, Loss_G: 1.00306236743927, Loss_D_A: 0.2543601393699646\n",
      "Epoch: 2, Batch: 1100, Loss_G: 0.9934340715408325, Loss_D_A: 0.25357475876808167\n",
      "Epoch: 2, Batch: 1110, Loss_G: 1.1480162143707275, Loss_D_A: 0.25766614079475403\n",
      "Epoch: 2, Batch: 1120, Loss_G: 1.0156376361846924, Loss_D_A: 0.25954920053482056\n",
      "Epoch: 2, Batch: 1130, Loss_G: 0.9858285784721375, Loss_D_A: 0.2518869638442993\n",
      "Epoch: 2, Batch: 1140, Loss_G: 1.0934981107711792, Loss_D_A: 0.25000056624412537\n",
      "Epoch: 2, Batch: 1150, Loss_G: 0.9828038215637207, Loss_D_A: 0.2530915141105652\n",
      "Epoch: 2, Batch: 1160, Loss_G: 1.1319773197174072, Loss_D_A: 0.25591468811035156\n",
      "Epoch: 2, Batch: 1170, Loss_G: 1.1680305004119873, Loss_D_A: 0.26034271717071533\n",
      "Epoch: 2, Batch: 1180, Loss_G: 1.2249820232391357, Loss_D_A: 0.2757834196090698\n",
      "Epoch: 2, Batch: 1190, Loss_G: 1.2088992595672607, Loss_D_A: 0.2618677616119385\n",
      "Epoch: 2, Batch: 1200, Loss_G: 0.9999467730522156, Loss_D_A: 0.2654706835746765\n",
      "Epoch: 2, Batch: 1210, Loss_G: 1.3140294551849365, Loss_D_A: 0.3743683099746704\n",
      "Epoch: 2, Batch: 1220, Loss_G: 1.3592406511306763, Loss_D_A: 0.2593995928764343\n",
      "Epoch: 3, Batch: 0, Loss_G: 1.3164420127868652, Loss_D_A: 0.2552022933959961\n",
      "Epoch: 3, Batch: 10, Loss_G: 0.9993729591369629, Loss_D_A: 0.25498512387275696\n",
      "Epoch: 3, Batch: 20, Loss_G: 1.1953293085098267, Loss_D_A: 0.2575225830078125\n",
      "Epoch: 3, Batch: 30, Loss_G: 1.0108550786972046, Loss_D_A: 0.2633216083049774\n",
      "Epoch: 3, Batch: 40, Loss_G: 0.9344286322593689, Loss_D_A: 0.25577089190483093\n",
      "Epoch: 3, Batch: 50, Loss_G: 1.0697758197784424, Loss_D_A: 0.2662239074707031\n",
      "Epoch: 3, Batch: 60, Loss_G: 0.9735965728759766, Loss_D_A: 0.2619748115539551\n",
      "Epoch: 3, Batch: 70, Loss_G: 1.1399445533752441, Loss_D_A: 0.25570282340049744\n",
      "Epoch: 3, Batch: 80, Loss_G: 1.1001776456832886, Loss_D_A: 0.25383439660072327\n",
      "Epoch: 3, Batch: 90, Loss_G: 1.545163631439209, Loss_D_A: 0.41915908455848694\n",
      "Epoch: 3, Batch: 100, Loss_G: 1.5858018398284912, Loss_D_A: 1.4273732900619507\n",
      "Epoch: 3, Batch: 110, Loss_G: 0.9355478286743164, Loss_D_A: 0.26928994059562683\n",
      "Epoch: 3, Batch: 120, Loss_G: 1.0250169038772583, Loss_D_A: 0.27188679575920105\n",
      "Epoch: 3, Batch: 130, Loss_G: 0.9360904097557068, Loss_D_A: 0.2663375735282898\n",
      "Epoch: 3, Batch: 140, Loss_G: 1.0977582931518555, Loss_D_A: 0.25902271270751953\n",
      "Epoch: 3, Batch: 150, Loss_G: 1.0602152347564697, Loss_D_A: 0.2519857883453369\n",
      "Epoch: 3, Batch: 160, Loss_G: 1.0502777099609375, Loss_D_A: 0.2500796914100647\n",
      "Epoch: 3, Batch: 170, Loss_G: 1.1792614459991455, Loss_D_A: 0.25717708468437195\n",
      "Epoch: 3, Batch: 180, Loss_G: 0.9869189262390137, Loss_D_A: 0.25141486525535583\n",
      "Epoch: 3, Batch: 190, Loss_G: 0.8835180997848511, Loss_D_A: 0.253653883934021\n",
      "Epoch: 3, Batch: 200, Loss_G: 0.9780116081237793, Loss_D_A: 0.2528291642665863\n",
      "Epoch: 3, Batch: 210, Loss_G: 1.012128233909607, Loss_D_A: 0.24975897371768951\n",
      "Epoch: 3, Batch: 220, Loss_G: 0.9695956707000732, Loss_D_A: 0.25444597005844116\n",
      "Epoch: 3, Batch: 230, Loss_G: 0.9837568998336792, Loss_D_A: 0.25802502036094666\n",
      "Epoch: 3, Batch: 240, Loss_G: 0.9045078754425049, Loss_D_A: 0.25521326065063477\n",
      "Epoch: 3, Batch: 250, Loss_G: 0.9946118593215942, Loss_D_A: 0.25793391466140747\n",
      "Epoch: 3, Batch: 260, Loss_G: 1.166949987411499, Loss_D_A: 0.2517071068286896\n",
      "Epoch: 3, Batch: 270, Loss_G: 1.1150678396224976, Loss_D_A: 0.2508384883403778\n",
      "Epoch: 3, Batch: 280, Loss_G: 0.845669686794281, Loss_D_A: 0.2549777626991272\n",
      "Epoch: 3, Batch: 290, Loss_G: 0.8271607160568237, Loss_D_A: 0.27740299701690674\n",
      "Epoch: 3, Batch: 300, Loss_G: 1.0082709789276123, Loss_D_A: 0.2523019015789032\n",
      "Epoch: 3, Batch: 310, Loss_G: 0.7969308495521545, Loss_D_A: 0.25375497341156006\n",
      "Epoch: 3, Batch: 320, Loss_G: 0.8674079179763794, Loss_D_A: 0.252497136592865\n",
      "Epoch: 3, Batch: 330, Loss_G: 0.9761936068534851, Loss_D_A: 0.25155240297317505\n",
      "Epoch: 3, Batch: 340, Loss_G: 0.977070152759552, Loss_D_A: 0.252720445394516\n",
      "Epoch: 3, Batch: 350, Loss_G: 0.9182113409042358, Loss_D_A: 0.25141772627830505\n",
      "Epoch: 3, Batch: 360, Loss_G: 1.0913405418395996, Loss_D_A: 0.2523055970668793\n",
      "Epoch: 3, Batch: 370, Loss_G: 0.9437534809112549, Loss_D_A: 0.28843221068382263\n",
      "Epoch: 3, Batch: 380, Loss_G: 1.0762391090393066, Loss_D_A: 0.25403913855552673\n",
      "Epoch: 3, Batch: 390, Loss_G: 0.9525749683380127, Loss_D_A: 0.2518395781517029\n",
      "Epoch: 3, Batch: 400, Loss_G: 1.0645158290863037, Loss_D_A: 0.25385892391204834\n",
      "Epoch: 3, Batch: 410, Loss_G: 1.103623390197754, Loss_D_A: 0.2531079947948456\n",
      "Epoch: 3, Batch: 420, Loss_G: 0.8535413146018982, Loss_D_A: 0.2521205544471741\n",
      "Epoch: 3, Batch: 430, Loss_G: 1.017951250076294, Loss_D_A: 0.2542473375797272\n",
      "Epoch: 3, Batch: 440, Loss_G: 1.0294640064239502, Loss_D_A: 0.259249210357666\n",
      "Epoch: 3, Batch: 450, Loss_G: 0.8806761503219604, Loss_D_A: 0.25213465094566345\n",
      "Epoch: 3, Batch: 460, Loss_G: 0.9759049415588379, Loss_D_A: 0.25190916657447815\n",
      "Epoch: 3, Batch: 470, Loss_G: 1.175092101097107, Loss_D_A: 0.26893776655197144\n",
      "Epoch: 3, Batch: 480, Loss_G: 1.1481348276138306, Loss_D_A: 0.255164235830307\n",
      "Epoch: 3, Batch: 490, Loss_G: 0.8753190636634827, Loss_D_A: 0.2520570755004883\n",
      "Epoch: 3, Batch: 500, Loss_G: 0.9458324313163757, Loss_D_A: 0.2556447982788086\n",
      "Epoch: 3, Batch: 510, Loss_G: 0.9772147536277771, Loss_D_A: 0.25707003474235535\n",
      "Epoch: 3, Batch: 520, Loss_G: 0.8519328832626343, Loss_D_A: 0.2536247968673706\n",
      "Epoch: 3, Batch: 530, Loss_G: 1.0385228395462036, Loss_D_A: 0.2545853853225708\n",
      "Epoch: 3, Batch: 540, Loss_G: 0.8221651315689087, Loss_D_A: 0.25053703784942627\n",
      "Epoch: 3, Batch: 550, Loss_G: 0.943520724773407, Loss_D_A: 0.2527792751789093\n",
      "Epoch: 3, Batch: 560, Loss_G: 0.933891236782074, Loss_D_A: 0.2505461871623993\n",
      "Epoch: 3, Batch: 570, Loss_G: 1.000720500946045, Loss_D_A: 0.26526838541030884\n",
      "Epoch: 3, Batch: 580, Loss_G: 1.1808793544769287, Loss_D_A: 0.32072654366493225\n",
      "Epoch: 3, Batch: 590, Loss_G: 0.8490166664123535, Loss_D_A: 0.2667653560638428\n",
      "Epoch: 3, Batch: 600, Loss_G: 1.152109980583191, Loss_D_A: 0.26542314887046814\n",
      "Epoch: 3, Batch: 610, Loss_G: 1.0615308284759521, Loss_D_A: 0.28213661909103394\n",
      "Epoch: 3, Batch: 620, Loss_G: 1.006835699081421, Loss_D_A: 0.25546854734420776\n",
      "Epoch: 3, Batch: 630, Loss_G: 1.1612004041671753, Loss_D_A: 0.2537011206150055\n",
      "Epoch: 3, Batch: 640, Loss_G: 0.94954913854599, Loss_D_A: 0.25507891178131104\n",
      "Epoch: 3, Batch: 650, Loss_G: 0.9257862567901611, Loss_D_A: 0.2512704133987427\n",
      "Epoch: 3, Batch: 660, Loss_G: 1.0211868286132812, Loss_D_A: 0.2501107454299927\n",
      "Epoch: 3, Batch: 670, Loss_G: 1.0818191766738892, Loss_D_A: 0.2551582455635071\n",
      "Epoch: 3, Batch: 680, Loss_G: 1.0686463117599487, Loss_D_A: 0.2870907485485077\n",
      "Epoch: 3, Batch: 690, Loss_G: 1.0338006019592285, Loss_D_A: 0.25937068462371826\n",
      "Epoch: 3, Batch: 700, Loss_G: 1.0814855098724365, Loss_D_A: 0.2761499285697937\n",
      "Epoch: 3, Batch: 710, Loss_G: 1.0945838689804077, Loss_D_A: 0.273996502161026\n",
      "Epoch: 3, Batch: 720, Loss_G: 0.8134483098983765, Loss_D_A: 0.2534436583518982\n",
      "Epoch: 3, Batch: 730, Loss_G: 1.008434772491455, Loss_D_A: 0.25696253776550293\n",
      "Epoch: 3, Batch: 740, Loss_G: 1.0061630010604858, Loss_D_A: 0.250309556722641\n",
      "Epoch: 3, Batch: 750, Loss_G: 0.8818157911300659, Loss_D_A: 0.2553010582923889\n",
      "Epoch: 3, Batch: 760, Loss_G: 1.0115164518356323, Loss_D_A: 0.2526116371154785\n",
      "Epoch: 3, Batch: 770, Loss_G: 1.0322681665420532, Loss_D_A: 0.257621169090271\n",
      "Epoch: 3, Batch: 780, Loss_G: 0.7755243182182312, Loss_D_A: 0.26466310024261475\n",
      "Epoch: 3, Batch: 790, Loss_G: 0.9161069393157959, Loss_D_A: 0.2565099596977234\n",
      "Epoch: 3, Batch: 800, Loss_G: 0.8225653171539307, Loss_D_A: 0.25242358446121216\n",
      "Epoch: 3, Batch: 810, Loss_G: 0.9439033269882202, Loss_D_A: 0.25316935777664185\n",
      "Epoch: 3, Batch: 820, Loss_G: 1.131760835647583, Loss_D_A: 0.36122697591781616\n",
      "Epoch: 3, Batch: 830, Loss_G: 1.28817880153656, Loss_D_A: 1.3145580291748047\n",
      "Epoch: 3, Batch: 840, Loss_G: 18.37911605834961, Loss_D_A: 14.77957534790039\n",
      "Epoch: 3, Batch: 850, Loss_G: 8.466813087463379, Loss_D_A: 0.3136448860168457\n",
      "Epoch: 3, Batch: 860, Loss_G: 6.608911037445068, Loss_D_A: 0.2837044298648834\n",
      "Epoch: 3, Batch: 870, Loss_G: 6.70371675491333, Loss_D_A: 0.2868567705154419\n",
      "Epoch: 3, Batch: 880, Loss_G: 6.515586853027344, Loss_D_A: 0.2565913796424866\n",
      "Epoch: 3, Batch: 890, Loss_G: 7.095836639404297, Loss_D_A: 0.23579584062099457\n",
      "Epoch: 3, Batch: 900, Loss_G: 6.615024089813232, Loss_D_A: 0.2329183667898178\n",
      "Epoch: 3, Batch: 910, Loss_G: 7.29477596282959, Loss_D_A: 0.21497827768325806\n",
      "Epoch: 3, Batch: 920, Loss_G: 6.095241546630859, Loss_D_A: 0.23522892594337463\n",
      "Epoch: 3, Batch: 930, Loss_G: 5.00001859664917, Loss_D_A: 0.2803317904472351\n",
      "Epoch: 3, Batch: 940, Loss_G: 4.257885456085205, Loss_D_A: 0.26342254877090454\n",
      "Epoch: 3, Batch: 950, Loss_G: 3.6234967708587646, Loss_D_A: 0.2673390805721283\n",
      "Epoch: 3, Batch: 960, Loss_G: 3.1796398162841797, Loss_D_A: 0.2517954707145691\n",
      "Epoch: 3, Batch: 970, Loss_G: 2.734112024307251, Loss_D_A: 0.2476474940776825\n",
      "Epoch: 3, Batch: 980, Loss_G: 2.538818836212158, Loss_D_A: 0.24044576287269592\n",
      "Epoch: 3, Batch: 990, Loss_G: 2.635411262512207, Loss_D_A: 0.26198452711105347\n",
      "Epoch: 3, Batch: 1000, Loss_G: 2.3629837036132812, Loss_D_A: 0.2638185918331146\n",
      "Epoch: 3, Batch: 1010, Loss_G: 2.4895548820495605, Loss_D_A: 0.2609449028968811\n",
      "Epoch: 3, Batch: 1020, Loss_G: 2.5314598083496094, Loss_D_A: 0.24118030071258545\n",
      "Epoch: 3, Batch: 1030, Loss_G: 1.9317114353179932, Loss_D_A: 0.2623867094516754\n",
      "Epoch: 3, Batch: 1040, Loss_G: 1.8493365049362183, Loss_D_A: 0.2659830152988434\n",
      "Epoch: 3, Batch: 1050, Loss_G: 2.3499648571014404, Loss_D_A: 0.26972952485084534\n",
      "Epoch: 3, Batch: 1060, Loss_G: 1.807569980621338, Loss_D_A: 0.2640869915485382\n",
      "Epoch: 3, Batch: 1070, Loss_G: 1.8506795167922974, Loss_D_A: 0.26181334257125854\n",
      "Epoch: 3, Batch: 1080, Loss_G: 1.8252609968185425, Loss_D_A: 0.25292858481407166\n",
      "Epoch: 3, Batch: 1090, Loss_G: 1.6431372165679932, Loss_D_A: 0.2657364010810852\n",
      "Epoch: 3, Batch: 1100, Loss_G: 1.7804185152053833, Loss_D_A: 0.25853925943374634\n",
      "Epoch: 3, Batch: 1110, Loss_G: 2.183043956756592, Loss_D_A: 0.2663133442401886\n",
      "Epoch: 3, Batch: 1120, Loss_G: 2.01031494140625, Loss_D_A: 0.25252318382263184\n",
      "Epoch: 3, Batch: 1130, Loss_G: 1.829962134361267, Loss_D_A: 0.2735491693019867\n",
      "Epoch: 3, Batch: 1140, Loss_G: 1.6516300439834595, Loss_D_A: 0.24780501425266266\n",
      "Epoch: 3, Batch: 1150, Loss_G: 1.7418442964553833, Loss_D_A: 0.2537766098976135\n",
      "Epoch: 3, Batch: 1160, Loss_G: 1.734330415725708, Loss_D_A: 0.26714932918548584\n",
      "Epoch: 3, Batch: 1170, Loss_G: 1.8725303411483765, Loss_D_A: 0.26501816511154175\n",
      "Epoch: 3, Batch: 1180, Loss_G: 1.808836817741394, Loss_D_A: 0.26320722699165344\n",
      "Epoch: 3, Batch: 1190, Loss_G: 1.689152717590332, Loss_D_A: 0.2544574439525604\n",
      "Epoch: 3, Batch: 1200, Loss_G: 1.6741241216659546, Loss_D_A: 0.25630366802215576\n",
      "Epoch: 3, Batch: 1210, Loss_G: 1.535789132118225, Loss_D_A: 0.26539215445518494\n",
      "Epoch: 3, Batch: 1220, Loss_G: 1.8240283727645874, Loss_D_A: 0.25974175333976746\n",
      "Epoch: 4, Batch: 0, Loss_G: 1.4999103546142578, Loss_D_A: 0.2627691924571991\n",
      "Epoch: 4, Batch: 10, Loss_G: 1.6358771324157715, Loss_D_A: 0.26148027181625366\n",
      "Epoch: 4, Batch: 20, Loss_G: 1.565850019454956, Loss_D_A: 0.25986653566360474\n",
      "Epoch: 4, Batch: 30, Loss_G: 1.4934005737304688, Loss_D_A: 0.25104597210884094\n",
      "Epoch: 4, Batch: 40, Loss_G: 1.4297962188720703, Loss_D_A: 0.2656608819961548\n",
      "Epoch: 4, Batch: 50, Loss_G: 1.3590261936187744, Loss_D_A: 0.25435352325439453\n",
      "Epoch: 4, Batch: 60, Loss_G: 1.5645289421081543, Loss_D_A: 0.24801036715507507\n",
      "Epoch: 4, Batch: 70, Loss_G: 1.4309619665145874, Loss_D_A: 0.25385782122612\n",
      "Epoch: 4, Batch: 80, Loss_G: 1.5159187316894531, Loss_D_A: 0.255308598279953\n",
      "Epoch: 4, Batch: 90, Loss_G: 1.4016953706741333, Loss_D_A: 0.2538371980190277\n",
      "Epoch: 4, Batch: 100, Loss_G: 1.6153982877731323, Loss_D_A: 0.2550213634967804\n",
      "Epoch: 4, Batch: 110, Loss_G: 1.661604404449463, Loss_D_A: 0.26070019602775574\n",
      "Epoch: 4, Batch: 120, Loss_G: 1.3872039318084717, Loss_D_A: 0.2540341019630432\n",
      "Epoch: 4, Batch: 130, Loss_G: 1.653761863708496, Loss_D_A: 0.2601158916950226\n",
      "Epoch: 4, Batch: 140, Loss_G: 1.7341855764389038, Loss_D_A: 0.2497750073671341\n",
      "Epoch: 4, Batch: 150, Loss_G: 1.8049286603927612, Loss_D_A: 0.25824543833732605\n",
      "Epoch: 4, Batch: 160, Loss_G: 1.341216802597046, Loss_D_A: 0.25159257650375366\n",
      "Epoch: 4, Batch: 170, Loss_G: 1.455513834953308, Loss_D_A: 0.24813735485076904\n",
      "Epoch: 4, Batch: 180, Loss_G: 1.4283220767974854, Loss_D_A: 0.25317519903182983\n",
      "Epoch: 4, Batch: 190, Loss_G: 1.7357120513916016, Loss_D_A: 0.26146072149276733\n",
      "Epoch: 4, Batch: 200, Loss_G: 1.463341236114502, Loss_D_A: 0.2520046830177307\n",
      "Epoch: 4, Batch: 210, Loss_G: 2.002244710922241, Loss_D_A: 0.2593246102333069\n",
      "Epoch: 4, Batch: 220, Loss_G: 1.4277130365371704, Loss_D_A: 0.25715532898902893\n",
      "Epoch: 4, Batch: 230, Loss_G: 1.5099337100982666, Loss_D_A: 0.25981414318084717\n",
      "Epoch: 4, Batch: 240, Loss_G: 1.5805659294128418, Loss_D_A: 0.25497275590896606\n",
      "Epoch: 4, Batch: 250, Loss_G: 1.5580353736877441, Loss_D_A: 0.26380500197410583\n",
      "Epoch: 4, Batch: 260, Loss_G: 1.3837294578552246, Loss_D_A: 0.25293511152267456\n",
      "Epoch: 4, Batch: 270, Loss_G: 1.5151426792144775, Loss_D_A: 0.25657784938812256\n",
      "Epoch: 4, Batch: 280, Loss_G: 1.558242917060852, Loss_D_A: 0.2534869611263275\n",
      "Epoch: 4, Batch: 290, Loss_G: 1.2768808603286743, Loss_D_A: 0.2567976415157318\n",
      "Epoch: 4, Batch: 300, Loss_G: 1.3989722728729248, Loss_D_A: 0.25328120589256287\n",
      "Epoch: 4, Batch: 310, Loss_G: 1.1599422693252563, Loss_D_A: 0.2525094151496887\n",
      "Epoch: 4, Batch: 320, Loss_G: 1.6949074268341064, Loss_D_A: 0.25089699029922485\n",
      "Epoch: 4, Batch: 330, Loss_G: 1.2113826274871826, Loss_D_A: 0.25471797585487366\n",
      "Epoch: 4, Batch: 340, Loss_G: 1.2936737537384033, Loss_D_A: 0.2577897906303406\n",
      "Epoch: 4, Batch: 350, Loss_G: 1.5518591403961182, Loss_D_A: 0.2561863958835602\n",
      "Epoch: 4, Batch: 360, Loss_G: 1.339044213294983, Loss_D_A: 0.2597465515136719\n",
      "Epoch: 4, Batch: 370, Loss_G: 1.3058090209960938, Loss_D_A: 0.2562873959541321\n",
      "Epoch: 4, Batch: 380, Loss_G: 1.125859260559082, Loss_D_A: 0.2512105107307434\n",
      "Epoch: 4, Batch: 390, Loss_G: 1.5102726221084595, Loss_D_A: 0.25493961572647095\n",
      "Epoch: 4, Batch: 400, Loss_G: 1.3163572549819946, Loss_D_A: 0.25422245264053345\n",
      "Epoch: 4, Batch: 410, Loss_G: 1.380770206451416, Loss_D_A: 0.2583402395248413\n",
      "Epoch: 4, Batch: 420, Loss_G: 1.3863770961761475, Loss_D_A: 0.24913290143013\n",
      "Epoch: 4, Batch: 430, Loss_G: 2.037292003631592, Loss_D_A: 0.26104986667633057\n",
      "Epoch: 4, Batch: 440, Loss_G: 1.411012887954712, Loss_D_A: 0.2528921067714691\n",
      "Epoch: 4, Batch: 450, Loss_G: 1.6408464908599854, Loss_D_A: 0.26208439469337463\n",
      "Epoch: 4, Batch: 460, Loss_G: 1.814467430114746, Loss_D_A: 0.26270928978919983\n",
      "Epoch: 4, Batch: 470, Loss_G: 1.6611884832382202, Loss_D_A: 0.33781713247299194\n",
      "Epoch: 4, Batch: 480, Loss_G: 1.432424783706665, Loss_D_A: 0.26918163895606995\n",
      "Epoch: 4, Batch: 490, Loss_G: 1.0098079442977905, Loss_D_A: 0.2722468376159668\n",
      "Epoch: 4, Batch: 500, Loss_G: 1.131040334701538, Loss_D_A: 0.28157490491867065\n",
      "Epoch: 4, Batch: 510, Loss_G: 1.36142098903656, Loss_D_A: 0.2583577036857605\n",
      "Epoch: 4, Batch: 520, Loss_G: 1.3152620792388916, Loss_D_A: 0.2658207416534424\n",
      "Epoch: 4, Batch: 530, Loss_G: 1.195696473121643, Loss_D_A: 0.25174444913864136\n",
      "Epoch: 4, Batch: 540, Loss_G: 1.5704580545425415, Loss_D_A: 0.255108118057251\n",
      "Epoch: 4, Batch: 550, Loss_G: 1.3650158643722534, Loss_D_A: 0.25285637378692627\n",
      "Epoch: 4, Batch: 560, Loss_G: 1.5871632099151611, Loss_D_A: 0.2546493709087372\n",
      "Epoch: 4, Batch: 570, Loss_G: 1.283817172050476, Loss_D_A: 0.25186091661453247\n",
      "Epoch: 4, Batch: 580, Loss_G: 1.434643268585205, Loss_D_A: 0.2648783326148987\n",
      "Epoch: 4, Batch: 590, Loss_G: 1.4736087322235107, Loss_D_A: 0.33598795533180237\n",
      "Epoch: 4, Batch: 600, Loss_G: 1.3184754848480225, Loss_D_A: 0.2893878221511841\n",
      "Epoch: 4, Batch: 610, Loss_G: 1.2744243144989014, Loss_D_A: 0.25423160195350647\n",
      "Epoch: 4, Batch: 620, Loss_G: 1.2621972560882568, Loss_D_A: 0.25433921813964844\n",
      "Epoch: 4, Batch: 630, Loss_G: 1.0603373050689697, Loss_D_A: 0.25348255038261414\n",
      "Epoch: 4, Batch: 640, Loss_G: 1.1074934005737305, Loss_D_A: 0.24855957925319672\n",
      "Epoch: 4, Batch: 650, Loss_G: 1.084076166152954, Loss_D_A: 0.2549813389778137\n",
      "Epoch: 4, Batch: 660, Loss_G: 1.0583105087280273, Loss_D_A: 0.2817838788032532\n",
      "Epoch: 4, Batch: 670, Loss_G: 1.2342289686203003, Loss_D_A: 0.265694797039032\n",
      "Epoch: 4, Batch: 680, Loss_G: 1.2775148153305054, Loss_D_A: 0.2595407962799072\n",
      "Epoch: 4, Batch: 690, Loss_G: 1.1727129220962524, Loss_D_A: 0.25778451561927795\n",
      "Epoch: 4, Batch: 700, Loss_G: 1.0390310287475586, Loss_D_A: 0.2549680471420288\n",
      "Epoch: 4, Batch: 710, Loss_G: 0.981484591960907, Loss_D_A: 0.2536075711250305\n",
      "Epoch: 4, Batch: 720, Loss_G: 1.0746943950653076, Loss_D_A: 0.25279501080513\n",
      "Epoch: 4, Batch: 730, Loss_G: 1.2018321752548218, Loss_D_A: 0.2530747354030609\n",
      "Epoch: 4, Batch: 740, Loss_G: 1.5614008903503418, Loss_D_A: 0.3346009850502014\n",
      "Epoch: 4, Batch: 750, Loss_G: 1.3340648412704468, Loss_D_A: 0.5138943195343018\n",
      "Epoch: 4, Batch: 760, Loss_G: 1.1350624561309814, Loss_D_A: 0.2547270655632019\n",
      "Epoch: 4, Batch: 770, Loss_G: 1.0855695009231567, Loss_D_A: 0.25247907638549805\n",
      "Epoch: 4, Batch: 780, Loss_G: 1.2902441024780273, Loss_D_A: 0.2504537105560303\n",
      "Epoch: 4, Batch: 790, Loss_G: 1.4575711488723755, Loss_D_A: 0.2560384273529053\n",
      "Epoch: 4, Batch: 800, Loss_G: 1.0910518169403076, Loss_D_A: 0.2536020576953888\n",
      "Epoch: 4, Batch: 810, Loss_G: 0.9148555994033813, Loss_D_A: 0.252701073884964\n",
      "Epoch: 4, Batch: 820, Loss_G: 1.138619065284729, Loss_D_A: 0.25157955288887024\n",
      "Epoch: 4, Batch: 830, Loss_G: 1.1592119932174683, Loss_D_A: 0.25186601281166077\n",
      "Epoch: 4, Batch: 840, Loss_G: 1.027769684791565, Loss_D_A: 0.2651304304599762\n",
      "Epoch: 4, Batch: 850, Loss_G: 1.299797773361206, Loss_D_A: 0.3946993052959442\n",
      "Epoch: 4, Batch: 860, Loss_G: 1.1729357242584229, Loss_D_A: 0.4259098768234253\n",
      "Epoch: 4, Batch: 870, Loss_G: 1.1312140226364136, Loss_D_A: 0.25291907787323\n",
      "Epoch: 4, Batch: 880, Loss_G: 1.1310160160064697, Loss_D_A: 0.26003849506378174\n",
      "Epoch: 4, Batch: 890, Loss_G: 1.0108184814453125, Loss_D_A: 0.2501956522464752\n",
      "Epoch: 4, Batch: 900, Loss_G: 0.9866253137588501, Loss_D_A: 0.26051944494247437\n",
      "Epoch: 4, Batch: 910, Loss_G: 1.1670140027999878, Loss_D_A: 0.2635723650455475\n",
      "Epoch: 4, Batch: 920, Loss_G: 1.0559008121490479, Loss_D_A: 0.2735801935195923\n",
      "Epoch: 4, Batch: 930, Loss_G: 1.0081586837768555, Loss_D_A: 0.252788782119751\n",
      "Epoch: 4, Batch: 940, Loss_G: 0.9243261814117432, Loss_D_A: 0.258760929107666\n",
      "Epoch: 4, Batch: 950, Loss_G: 1.4586799144744873, Loss_D_A: 0.2859797775745392\n",
      "Epoch: 4, Batch: 960, Loss_G: 1.3536485433578491, Loss_D_A: 0.27718615531921387\n",
      "Epoch: 4, Batch: 970, Loss_G: 1.0969586372375488, Loss_D_A: 0.2556089162826538\n",
      "Epoch: 4, Batch: 980, Loss_G: 1.1337780952453613, Loss_D_A: 0.2526096701622009\n",
      "Epoch: 4, Batch: 990, Loss_G: 1.3626947402954102, Loss_D_A: 0.25353574752807617\n",
      "Epoch: 4, Batch: 1000, Loss_G: 1.16658353805542, Loss_D_A: 0.29224878549575806\n",
      "Epoch: 4, Batch: 1010, Loss_G: 1.3928178548812866, Loss_D_A: 0.5304474830627441\n",
      "Epoch: 4, Batch: 1020, Loss_G: 1.3152157068252563, Loss_D_A: 0.2591102719306946\n",
      "Epoch: 4, Batch: 1030, Loss_G: 1.1503229141235352, Loss_D_A: 0.25667500495910645\n",
      "Epoch: 4, Batch: 1040, Loss_G: 1.1281300783157349, Loss_D_A: 0.25479498505592346\n",
      "Epoch: 4, Batch: 1050, Loss_G: 1.1300747394561768, Loss_D_A: 0.27002227306365967\n",
      "Epoch: 4, Batch: 1060, Loss_G: 1.2003945112228394, Loss_D_A: 0.28327250480651855\n",
      "Epoch: 4, Batch: 1070, Loss_G: 1.02402925491333, Loss_D_A: 0.29982924461364746\n",
      "Epoch: 4, Batch: 1080, Loss_G: 1.130728840827942, Loss_D_A: 0.2600799798965454\n",
      "Epoch: 4, Batch: 1090, Loss_G: 1.214832067489624, Loss_D_A: 0.2604008615016937\n",
      "Epoch: 4, Batch: 1100, Loss_G: 1.1008150577545166, Loss_D_A: 0.2615538537502289\n",
      "Epoch: 4, Batch: 1110, Loss_G: 0.9421520233154297, Loss_D_A: 0.2666143774986267\n",
      "Epoch: 4, Batch: 1120, Loss_G: 1.1011735200881958, Loss_D_A: 0.2669132947921753\n",
      "Epoch: 4, Batch: 1130, Loss_G: 1.0518298149108887, Loss_D_A: 0.29117676615715027\n",
      "Epoch: 4, Batch: 1140, Loss_G: 1.0176903009414673, Loss_D_A: 0.38504958152770996\n",
      "Epoch: 4, Batch: 1150, Loss_G: 1.0709774494171143, Loss_D_A: 0.4057971239089966\n",
      "Epoch: 4, Batch: 1160, Loss_G: 1.2037562131881714, Loss_D_A: 0.277052104473114\n",
      "Epoch: 4, Batch: 1170, Loss_G: 1.1204676628112793, Loss_D_A: 0.2528487741947174\n",
      "Epoch: 4, Batch: 1180, Loss_G: 1.143250584602356, Loss_D_A: 0.2538166046142578\n",
      "Epoch: 4, Batch: 1190, Loss_G: 1.0058799982070923, Loss_D_A: 0.25199437141418457\n",
      "Epoch: 4, Batch: 1200, Loss_G: 1.018115758895874, Loss_D_A: 0.2572595775127411\n",
      "Epoch: 4, Batch: 1210, Loss_G: 1.1054881811141968, Loss_D_A: 0.25238096714019775\n",
      "Epoch: 4, Batch: 1220, Loss_G: 0.9429280757904053, Loss_D_A: 0.256475567817688\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_A = G_AA(real_A)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "\n",
    "        \n",
    "        # 循环一致性损失\n",
    "        loss_cycle_BAB = criterion_cycle(fake_A, real_A) * 10.0\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_GAN_A2B + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 使用测试集中的数据生成图像\n",
    "        for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "            real_A = real_A.to(device)\n",
    "            fake_A = G_AA(real_A)\n",
    "            vutils.save_image(fake_A, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at models\\Generator.ckpt\n"
     ]
    }
   ],
   "source": [
    "# save_model_path='models'\n",
    "# checkpoint_path = os.path.join(save_model_path, \"Generator.ckpt\")\n",
    "# torch.save(G_AA.state_dict(), checkpoint_path)\n",
    "# print(\"Model saved at %s\" % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AA = Generator(input_nc=3, output_nc=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (10): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (19): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (22): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (25): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AA.load_state_dict(torch.load(\"models\\Generator.ckpt\"))\n",
    "G_AA.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # 根据你的模型调整尺寸\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # 添加批次维度\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path1,image_path2=None):\n",
    "    image1 = load_image(image_path1).to(device)\n",
    "    image2 = load_image(image_path2).to(device)\n",
    "\n",
    "    model= model.to(device)\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():  # 不计算梯度\n",
    "            image=image1*(i/10)+image2*(1-i/10)\n",
    "            output = model(image)\n",
    "            vutils.save_image(output, f'test {i}.png', normalize=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = predict(G_AA, 'human_dog\\\\testA\\\\200600.jpg','human_dog\\\\testB\\\\flickr_dog_000043.jpg')\n",
    "# output_image = output_image - output_image.min()\n",
    "# output_image = output_image / output_image.max()\n",
    "\n",
    "# output_image = output_image.squeeze()  # 假设输出是图像格式，调整通道\n",
    "# output_image = output_image.permute(1,2,0)\n",
    "# output_image=output_image.to('cpu')\n",
    "# # 步骤 5: 可视化输出图像\n",
    "# plt.imshow(output_image.numpy())\n",
    "# plt.title('Output Image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(output_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
