{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import glob\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.Conv2d(64, output_nc, kernel_size=7, padding=3), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, kernel_size=4, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [nn.Conv2d(512, 1, kernel_size=4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(256*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"%sA\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"%sB\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"train\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=4,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=4,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)\n",
    "G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator(input_nc=3).to(device)\n",
    "D_B = Discriminator(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss_G: 15.61037826538086, Loss_D_A: 1.1615573167800903, Loss_D_B: 0.9101475477218628\n",
      "Epoch: 0, Batch: 10, Loss_G: 6.970175743103027, Loss_D_A: 0.3666701912879944, Loss_D_B: 0.24359501898288727\n",
      "Epoch: 0, Batch: 20, Loss_G: 5.965689659118652, Loss_D_A: 0.24649454653263092, Loss_D_B: 0.24288181960582733\n",
      "Epoch: 0, Batch: 30, Loss_G: 5.182547569274902, Loss_D_A: 0.24978424608707428, Loss_D_B: 0.2856913208961487\n",
      "Epoch: 0, Batch: 40, Loss_G: 7.472266674041748, Loss_D_A: 0.3758556544780731, Loss_D_B: 0.32502102851867676\n",
      "Epoch: 0, Batch: 50, Loss_G: 7.623085975646973, Loss_D_A: 0.23080357909202576, Loss_D_B: 0.24449694156646729\n",
      "Epoch: 0, Batch: 60, Loss_G: 6.9660868644714355, Loss_D_A: 0.26649582386016846, Loss_D_B: 0.23975417017936707\n",
      "Epoch: 0, Batch: 70, Loss_G: 5.563512325286865, Loss_D_A: 0.24589692056179047, Loss_D_B: 0.2512208819389343\n",
      "Epoch: 0, Batch: 80, Loss_G: 6.209434986114502, Loss_D_A: 0.30268868803977966, Loss_D_B: 0.2863529324531555\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_B = G_AB(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "        fake_A = G_BA(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "        # 循环一致性损失\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * 10.0\n",
    "        \n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}, Loss_D_B: {loss_D_B.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 使用测试集中的数据生成图像\n",
    "        for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "            real_A = real_A.to(device)\n",
    "            fake_B = G_AB(real_A)\n",
    "            vutils.save_image(fake_B, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Inversion\n",
    "\n",
    "Train the model to convert A to A. Then interpolation can be used to generate intermediate image. \n",
    "\n",
    "Using contrastive loss to make the model to learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, 'combined') + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, 'combined') + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"train\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=12,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog\", transforms_=transform, unaligned=True, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=12,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "# G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator(input_nc=3).to(device)\n",
    "D_B = Discriminator(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(G_AA.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg19\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.vgg = vgg19(pretrained=True).features[:21]  # 只使用到第三个池化层之前的部分\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        # 冻结VGG参数\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        self.vgg = self.vgg.to(device)\n",
    "        gen_features = self.vgg(generated)\n",
    "        target_features = self.vgg(target)\n",
    "\n",
    "        return self.loss(gen_features, target_features)\n",
    "\n",
    "# 实例化感知损失\n",
    "perceptual_loss = PerceptualLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss_G: 13.508612632751465, Loss_D_A: 0.7687002420425415\n",
      "Epoch: 0, Batch: 10, Loss_G: 10.49593734741211, Loss_D_A: 0.22693656384944916\n",
      "Epoch: 0, Batch: 20, Loss_G: 9.593791961669922, Loss_D_A: 0.18843263387680054\n",
      "Epoch: 0, Batch: 30, Loss_G: 8.251578330993652, Loss_D_A: 0.5896660685539246\n",
      "Epoch: 0, Batch: 40, Loss_G: 8.62150764465332, Loss_D_A: 0.13573087751865387\n",
      "Epoch: 0, Batch: 50, Loss_G: 6.574091911315918, Loss_D_A: 0.16125990450382233\n",
      "Epoch: 0, Batch: 60, Loss_G: 5.9123640060424805, Loss_D_A: 0.1083391010761261\n",
      "Epoch: 0, Batch: 70, Loss_G: 5.615291595458984, Loss_D_A: 0.17617309093475342\n",
      "Epoch: 0, Batch: 80, Loss_G: 5.771149158477783, Loss_D_A: 0.04491275176405907\n",
      "Epoch: 0, Batch: 90, Loss_G: 6.482577323913574, Loss_D_A: 0.09334409236907959\n",
      "Epoch: 0, Batch: 100, Loss_G: 4.873598575592041, Loss_D_A: 0.4065566658973694\n",
      "Epoch: 0, Batch: 110, Loss_G: 3.9274675846099854, Loss_D_A: 0.2514950633049011\n",
      "Epoch: 0, Batch: 120, Loss_G: 3.61431622505188, Loss_D_A: 0.2714191675186157\n",
      "Epoch: 0, Batch: 130, Loss_G: 3.7875800132751465, Loss_D_A: 0.24852189421653748\n",
      "Epoch: 0, Batch: 140, Loss_G: 3.707345962524414, Loss_D_A: 0.2800234854221344\n",
      "Epoch: 0, Batch: 150, Loss_G: 3.9653191566467285, Loss_D_A: 0.24880018830299377\n",
      "Epoch: 0, Batch: 160, Loss_G: 3.166788101196289, Loss_D_A: 0.2527981102466583\n",
      "Epoch: 0, Batch: 170, Loss_G: 3.0042362213134766, Loss_D_A: 0.23726937174797058\n",
      "Epoch: 0, Batch: 180, Loss_G: 3.160749912261963, Loss_D_A: 0.23926770687103271\n",
      "Epoch: 0, Batch: 190, Loss_G: 3.267094612121582, Loss_D_A: 0.2773517966270447\n",
      "Epoch: 0, Batch: 200, Loss_G: 3.17592716217041, Loss_D_A: 0.23918655514717102\n",
      "Epoch: 0, Batch: 210, Loss_G: 3.614429473876953, Loss_D_A: 0.24368947744369507\n",
      "Epoch: 0, Batch: 220, Loss_G: 2.9987194538116455, Loss_D_A: 0.26105350255966187\n",
      "Epoch: 0, Batch: 230, Loss_G: 3.7777693271636963, Loss_D_A: 0.22957167029380798\n",
      "Epoch: 0, Batch: 240, Loss_G: 3.094529151916504, Loss_D_A: 0.22508499026298523\n",
      "Epoch: 0, Batch: 250, Loss_G: 2.7358903884887695, Loss_D_A: 0.24073883891105652\n",
      "Epoch: 0, Batch: 260, Loss_G: 2.983908176422119, Loss_D_A: 0.3002462685108185\n",
      "Epoch: 0, Batch: 270, Loss_G: 4.071053981781006, Loss_D_A: 1.5931947231292725\n",
      "Epoch: 0, Batch: 280, Loss_G: 4.5641350746154785, Loss_D_A: 0.5002391934394836\n",
      "Epoch: 0, Batch: 290, Loss_G: 2.8922290802001953, Loss_D_A: 0.25805309414863586\n",
      "Epoch: 0, Batch: 300, Loss_G: 2.8886327743530273, Loss_D_A: 0.25505223870277405\n",
      "Epoch: 0, Batch: 310, Loss_G: 2.6738057136535645, Loss_D_A: 0.25918641686439514\n",
      "Epoch: 0, Batch: 320, Loss_G: 2.7263309955596924, Loss_D_A: 0.24971143901348114\n",
      "Epoch: 0, Batch: 330, Loss_G: 2.4874911308288574, Loss_D_A: 0.2516140937805176\n",
      "Epoch: 0, Batch: 340, Loss_G: 3.1069235801696777, Loss_D_A: 0.2509338855743408\n",
      "Epoch: 0, Batch: 350, Loss_G: 2.8593757152557373, Loss_D_A: 0.2491263449192047\n",
      "Epoch: 0, Batch: 360, Loss_G: 2.824434280395508, Loss_D_A: 0.2461240291595459\n",
      "Epoch: 0, Batch: 370, Loss_G: 3.0838072299957275, Loss_D_A: 0.2468893826007843\n",
      "Epoch: 0, Batch: 380, Loss_G: 2.2391090393066406, Loss_D_A: 0.24749541282653809\n",
      "Epoch: 0, Batch: 390, Loss_G: 2.4463634490966797, Loss_D_A: 0.2503027319908142\n",
      "Epoch: 0, Batch: 400, Loss_G: 2.76723575592041, Loss_D_A: 0.24490341544151306\n",
      "Epoch: 0, Batch: 410, Loss_G: 2.4933223724365234, Loss_D_A: 0.24040712416172028\n",
      "Epoch: 0, Batch: 420, Loss_G: 2.7399539947509766, Loss_D_A: 0.23424948751926422\n",
      "Epoch: 0, Batch: 430, Loss_G: 2.737384796142578, Loss_D_A: 0.24247023463249207\n",
      "Epoch: 0, Batch: 440, Loss_G: 2.377263069152832, Loss_D_A: 0.24721509218215942\n",
      "Epoch: 0, Batch: 450, Loss_G: 2.804978370666504, Loss_D_A: 0.22723907232284546\n",
      "Epoch: 0, Batch: 460, Loss_G: 2.2082207202911377, Loss_D_A: 0.2395523190498352\n",
      "Epoch: 0, Batch: 470, Loss_G: 2.2128002643585205, Loss_D_A: 0.25078341364860535\n",
      "Epoch: 0, Batch: 480, Loss_G: 2.634437084197998, Loss_D_A: 0.21524998545646667\n",
      "Epoch: 0, Batch: 490, Loss_G: 2.326603412628174, Loss_D_A: 0.23116141557693481\n",
      "Epoch: 0, Batch: 500, Loss_G: 2.2683804035186768, Loss_D_A: 0.28783345222473145\n",
      "Epoch: 0, Batch: 510, Loss_G: 3.176823616027832, Loss_D_A: 0.260895311832428\n",
      "Epoch: 0, Batch: 520, Loss_G: 2.313915967941284, Loss_D_A: 0.25042450428009033\n",
      "Epoch: 0, Batch: 530, Loss_G: 2.081279754638672, Loss_D_A: 0.23520658910274506\n",
      "Epoch: 0, Batch: 540, Loss_G: 2.1342647075653076, Loss_D_A: 0.241167813539505\n",
      "Epoch: 0, Batch: 550, Loss_G: 2.245722532272339, Loss_D_A: 0.2594360411167145\n",
      "Epoch: 0, Batch: 560, Loss_G: 2.668050527572632, Loss_D_A: 0.23079414665699005\n",
      "Epoch: 0, Batch: 570, Loss_G: 2.4819459915161133, Loss_D_A: 0.2518315315246582\n",
      "Epoch: 0, Batch: 580, Loss_G: 2.0748374462127686, Loss_D_A: 0.24780717492103577\n",
      "Epoch: 0, Batch: 590, Loss_G: 2.2250218391418457, Loss_D_A: 0.24588653445243835\n",
      "Epoch: 0, Batch: 600, Loss_G: 2.617210626602173, Loss_D_A: 0.21506455540657043\n",
      "Epoch: 0, Batch: 610, Loss_G: 2.3978188037872314, Loss_D_A: 0.24789384007453918\n",
      "Epoch: 0, Batch: 620, Loss_G: 2.133519411087036, Loss_D_A: 0.24307197332382202\n",
      "Epoch: 0, Batch: 630, Loss_G: 3.2041215896606445, Loss_D_A: 0.3018076717853546\n",
      "Epoch: 0, Batch: 640, Loss_G: 2.1405527591705322, Loss_D_A: 0.23645630478858948\n",
      "Epoch: 0, Batch: 650, Loss_G: 2.2401137351989746, Loss_D_A: 0.23543673753738403\n",
      "Epoch: 0, Batch: 660, Loss_G: 2.4882659912109375, Loss_D_A: 0.24037623405456543\n",
      "Epoch: 0, Batch: 670, Loss_G: 2.0108978748321533, Loss_D_A: 0.23981206119060516\n",
      "Epoch: 0, Batch: 680, Loss_G: 2.786224126815796, Loss_D_A: 0.24266305565834045\n",
      "Epoch: 0, Batch: 690, Loss_G: 2.4337382316589355, Loss_D_A: 0.23140715062618256\n",
      "Epoch: 0, Batch: 700, Loss_G: 3.0971078872680664, Loss_D_A: 0.2586132884025574\n",
      "Epoch: 0, Batch: 710, Loss_G: 2.5413169860839844, Loss_D_A: 0.42097532749176025\n",
      "Epoch: 0, Batch: 720, Loss_G: 2.0930163860321045, Loss_D_A: 0.2594528794288635\n",
      "Epoch: 0, Batch: 730, Loss_G: 2.356917381286621, Loss_D_A: 0.2529795467853546\n",
      "Epoch: 0, Batch: 740, Loss_G: 2.4953441619873047, Loss_D_A: 0.25450652837753296\n",
      "Epoch: 0, Batch: 750, Loss_G: 2.4624576568603516, Loss_D_A: 0.2538753151893616\n",
      "Epoch: 0, Batch: 760, Loss_G: 1.7104054689407349, Loss_D_A: 0.25300920009613037\n",
      "Epoch: 0, Batch: 770, Loss_G: 1.6442946195602417, Loss_D_A: 0.24893757700920105\n",
      "Epoch: 0, Batch: 780, Loss_G: 1.9701296091079712, Loss_D_A: 0.2504449784755707\n",
      "Epoch: 0, Batch: 790, Loss_G: 1.738088846206665, Loss_D_A: 0.25289416313171387\n",
      "Epoch: 0, Batch: 800, Loss_G: 2.4898147583007812, Loss_D_A: 0.24316491186618805\n",
      "Epoch: 0, Batch: 810, Loss_G: 2.305332660675049, Loss_D_A: 0.24734395742416382\n",
      "Epoch: 0, Batch: 820, Loss_G: 2.233001232147217, Loss_D_A: 0.2533915340900421\n",
      "Epoch: 0, Batch: 830, Loss_G: 2.0597329139709473, Loss_D_A: 0.2523181438446045\n",
      "Epoch: 0, Batch: 840, Loss_G: 1.752350091934204, Loss_D_A: 0.2475559413433075\n",
      "Epoch: 0, Batch: 850, Loss_G: 2.174790859222412, Loss_D_A: 0.2545780539512634\n",
      "Epoch: 0, Batch: 860, Loss_G: 1.7799651622772217, Loss_D_A: 0.24658848345279694\n",
      "Epoch: 0, Batch: 870, Loss_G: 2.759420871734619, Loss_D_A: 0.2413693368434906\n",
      "Epoch: 0, Batch: 880, Loss_G: 1.889817714691162, Loss_D_A: 0.2455635666847229\n",
      "Epoch: 0, Batch: 890, Loss_G: 1.573643445968628, Loss_D_A: 0.24730199575424194\n",
      "Epoch: 0, Batch: 900, Loss_G: 2.7379109859466553, Loss_D_A: 0.25370946526527405\n",
      "Epoch: 0, Batch: 910, Loss_G: 4.938164234161377, Loss_D_A: 0.23943084478378296\n",
      "Epoch: 0, Batch: 920, Loss_G: 2.741931200027466, Loss_D_A: 0.2238142490386963\n",
      "Epoch: 0, Batch: 930, Loss_G: 2.6505608558654785, Loss_D_A: 0.2594990134239197\n",
      "Epoch: 0, Batch: 940, Loss_G: 2.625469207763672, Loss_D_A: 0.22914358973503113\n",
      "Epoch: 0, Batch: 950, Loss_G: 2.4113030433654785, Loss_D_A: 0.24999535083770752\n",
      "Epoch: 0, Batch: 960, Loss_G: 2.225984811782837, Loss_D_A: 0.22288314998149872\n",
      "Epoch: 0, Batch: 970, Loss_G: 1.8781650066375732, Loss_D_A: 0.22513988614082336\n",
      "Epoch: 0, Batch: 980, Loss_G: 1.8159122467041016, Loss_D_A: 0.27116936445236206\n",
      "Epoch: 0, Batch: 990, Loss_G: 1.8984392881393433, Loss_D_A: 0.2456435263156891\n",
      "Epoch: 0, Batch: 1000, Loss_G: 1.7598893642425537, Loss_D_A: 0.24261024594306946\n",
      "Epoch: 0, Batch: 1010, Loss_G: 1.9345355033874512, Loss_D_A: 0.37898045778274536\n",
      "Epoch: 0, Batch: 1020, Loss_G: 2.3146629333496094, Loss_D_A: 0.25321775674819946\n",
      "Epoch: 0, Batch: 1030, Loss_G: 1.5971200466156006, Loss_D_A: 0.24315804243087769\n",
      "Epoch: 0, Batch: 1040, Loss_G: 2.118086814880371, Loss_D_A: 0.25450998544692993\n",
      "Epoch: 0, Batch: 1050, Loss_G: 2.0854036808013916, Loss_D_A: 0.22897282242774963\n",
      "Epoch: 0, Batch: 1060, Loss_G: 1.8742268085479736, Loss_D_A: 0.24584871530532837\n",
      "Epoch: 0, Batch: 1070, Loss_G: 2.1051506996154785, Loss_D_A: 0.24894824624061584\n",
      "Epoch: 0, Batch: 1080, Loss_G: 1.4700078964233398, Loss_D_A: 0.2288711667060852\n",
      "Epoch: 0, Batch: 1090, Loss_G: 1.937872290611267, Loss_D_A: 0.25383684039115906\n",
      "Epoch: 0, Batch: 1100, Loss_G: 2.6240787506103516, Loss_D_A: 0.2278469204902649\n",
      "Epoch: 0, Batch: 1110, Loss_G: 2.1132938861846924, Loss_D_A: 0.2608894109725952\n",
      "Epoch: 0, Batch: 1120, Loss_G: 2.154869556427002, Loss_D_A: 0.3011532723903656\n",
      "Epoch: 0, Batch: 1130, Loss_G: 1.8795039653778076, Loss_D_A: 0.2576981782913208\n",
      "Epoch: 0, Batch: 1140, Loss_G: 1.9509897232055664, Loss_D_A: 0.25002408027648926\n",
      "Epoch: 0, Batch: 1150, Loss_G: 2.2177600860595703, Loss_D_A: 0.27552497386932373\n",
      "Epoch: 0, Batch: 1160, Loss_G: 1.836483359336853, Loss_D_A: 0.26073700189590454\n",
      "Epoch: 0, Batch: 1170, Loss_G: 2.2747206687927246, Loss_D_A: 0.27733567357063293\n",
      "Epoch: 0, Batch: 1180, Loss_G: 1.8633015155792236, Loss_D_A: 0.26823705434799194\n",
      "Epoch: 0, Batch: 1190, Loss_G: 1.7423762083053589, Loss_D_A: 0.24560880661010742\n",
      "Epoch: 0, Batch: 1200, Loss_G: 1.6669299602508545, Loss_D_A: 0.25375744700431824\n",
      "Epoch: 0, Batch: 1210, Loss_G: 1.6007277965545654, Loss_D_A: 0.2638391852378845\n",
      "Epoch: 0, Batch: 1220, Loss_G: 1.925979733467102, Loss_D_A: 0.2407698631286621\n",
      "Epoch: 1, Batch: 0, Loss_G: 1.7659258842468262, Loss_D_A: 0.26772540807724\n",
      "Epoch: 1, Batch: 10, Loss_G: 2.0690791606903076, Loss_D_A: 0.26732054352760315\n",
      "Epoch: 1, Batch: 20, Loss_G: 2.0712947845458984, Loss_D_A: 0.24718350172042847\n",
      "Epoch: 1, Batch: 30, Loss_G: 1.887744665145874, Loss_D_A: 0.2725796103477478\n",
      "Epoch: 1, Batch: 40, Loss_G: 1.927274227142334, Loss_D_A: 0.25327402353286743\n",
      "Epoch: 1, Batch: 50, Loss_G: 2.129617929458618, Loss_D_A: 0.25484591722488403\n",
      "Epoch: 1, Batch: 60, Loss_G: 1.6940133571624756, Loss_D_A: 0.24759718775749207\n",
      "Epoch: 1, Batch: 70, Loss_G: 2.2216269969940186, Loss_D_A: 0.25323769450187683\n",
      "Epoch: 1, Batch: 80, Loss_G: 2.236915111541748, Loss_D_A: 0.24928128719329834\n",
      "Epoch: 1, Batch: 90, Loss_G: 2.0934345722198486, Loss_D_A: 0.25144779682159424\n",
      "Epoch: 1, Batch: 100, Loss_G: 1.478549599647522, Loss_D_A: 0.2478758990764618\n",
      "Epoch: 1, Batch: 110, Loss_G: 2.0265209674835205, Loss_D_A: 0.23414114117622375\n",
      "Epoch: 1, Batch: 120, Loss_G: 1.8061211109161377, Loss_D_A: 0.2576696574687958\n",
      "Epoch: 1, Batch: 130, Loss_G: 1.822671890258789, Loss_D_A: 0.27893710136413574\n",
      "Epoch: 1, Batch: 140, Loss_G: 1.7994701862335205, Loss_D_A: 0.2482038140296936\n",
      "Epoch: 1, Batch: 150, Loss_G: 1.8527538776397705, Loss_D_A: 0.24928605556488037\n",
      "Epoch: 1, Batch: 160, Loss_G: 1.584641695022583, Loss_D_A: 0.24819497764110565\n",
      "Epoch: 1, Batch: 170, Loss_G: 1.7202775478363037, Loss_D_A: 0.2624267041683197\n",
      "Epoch: 1, Batch: 180, Loss_G: 1.5704892873764038, Loss_D_A: 0.2439691424369812\n",
      "Epoch: 1, Batch: 190, Loss_G: 1.6603922843933105, Loss_D_A: 0.24517172574996948\n",
      "Epoch: 1, Batch: 200, Loss_G: 1.4630491733551025, Loss_D_A: 0.25945132970809937\n",
      "Epoch: 1, Batch: 210, Loss_G: 1.6309601068496704, Loss_D_A: 0.2493457794189453\n",
      "Epoch: 1, Batch: 220, Loss_G: 1.8035956621170044, Loss_D_A: 0.26172226667404175\n",
      "Epoch: 1, Batch: 230, Loss_G: 1.7989296913146973, Loss_D_A: 0.2771476209163666\n",
      "Epoch: 1, Batch: 240, Loss_G: 20.265029907226562, Loss_D_A: 16.11842918395996\n",
      "Epoch: 1, Batch: 250, Loss_G: 11.07178020477295, Loss_D_A: 0.745256781578064\n",
      "Epoch: 1, Batch: 260, Loss_G: 8.707514762878418, Loss_D_A: 0.25995373725891113\n",
      "Epoch: 1, Batch: 270, Loss_G: 7.405681133270264, Loss_D_A: 0.25525230169296265\n",
      "Epoch: 1, Batch: 280, Loss_G: 5.659035682678223, Loss_D_A: 0.2733563184738159\n",
      "Epoch: 1, Batch: 290, Loss_G: 4.054414749145508, Loss_D_A: 0.26328614354133606\n",
      "Epoch: 1, Batch: 300, Loss_G: 4.243343353271484, Loss_D_A: 0.2695665955543518\n",
      "Epoch: 1, Batch: 310, Loss_G: 3.991866111755371, Loss_D_A: 0.27215689420700073\n",
      "Epoch: 1, Batch: 320, Loss_G: 3.259852170944214, Loss_D_A: 0.2607589662075043\n",
      "Epoch: 1, Batch: 330, Loss_G: 3.371950387954712, Loss_D_A: 0.26179778575897217\n",
      "Epoch: 1, Batch: 340, Loss_G: 3.2505457401275635, Loss_D_A: 0.25261998176574707\n",
      "Epoch: 1, Batch: 350, Loss_G: 2.858032464981079, Loss_D_A: 0.25733059644699097\n",
      "Epoch: 1, Batch: 360, Loss_G: 3.426743507385254, Loss_D_A: 0.25553441047668457\n",
      "Epoch: 1, Batch: 370, Loss_G: 2.4330649375915527, Loss_D_A: 0.26139912009239197\n",
      "Epoch: 1, Batch: 380, Loss_G: 2.4561562538146973, Loss_D_A: 0.2599419355392456\n",
      "Epoch: 1, Batch: 390, Loss_G: 2.408447742462158, Loss_D_A: 0.2539637088775635\n",
      "Epoch: 1, Batch: 400, Loss_G: 2.6057827472686768, Loss_D_A: 0.25704091787338257\n",
      "Epoch: 1, Batch: 410, Loss_G: 2.230769395828247, Loss_D_A: 0.26533225178718567\n",
      "Epoch: 1, Batch: 420, Loss_G: 2.36407470703125, Loss_D_A: 0.26056382060050964\n",
      "Epoch: 1, Batch: 430, Loss_G: 2.9626200199127197, Loss_D_A: 0.2601433992385864\n",
      "Epoch: 1, Batch: 440, Loss_G: 2.5858774185180664, Loss_D_A: 0.2587955594062805\n",
      "Epoch: 1, Batch: 450, Loss_G: 2.995903968811035, Loss_D_A: 0.2590399384498596\n",
      "Epoch: 1, Batch: 460, Loss_G: 2.9859848022460938, Loss_D_A: 0.2582077980041504\n",
      "Epoch: 1, Batch: 470, Loss_G: 2.5783674716949463, Loss_D_A: 0.251717209815979\n",
      "Epoch: 1, Batch: 480, Loss_G: 2.430309534072876, Loss_D_A: 0.25284814834594727\n",
      "Epoch: 1, Batch: 490, Loss_G: 2.4583616256713867, Loss_D_A: 0.2507668137550354\n",
      "Epoch: 1, Batch: 500, Loss_G: 2.0440475940704346, Loss_D_A: 0.2536707818508148\n",
      "Epoch: 1, Batch: 510, Loss_G: 1.8890442848205566, Loss_D_A: 0.25786295533180237\n",
      "Epoch: 1, Batch: 520, Loss_G: 2.04243803024292, Loss_D_A: 0.2537301778793335\n",
      "Epoch: 1, Batch: 530, Loss_G: 2.1765573024749756, Loss_D_A: 0.2553931176662445\n",
      "Epoch: 1, Batch: 540, Loss_G: 2.1078381538391113, Loss_D_A: 0.2543417811393738\n",
      "Epoch: 1, Batch: 550, Loss_G: 1.971553087234497, Loss_D_A: 0.25142669677734375\n",
      "Epoch: 1, Batch: 560, Loss_G: 2.176229953765869, Loss_D_A: 0.254705011844635\n",
      "Epoch: 1, Batch: 570, Loss_G: 2.4763331413269043, Loss_D_A: 0.253395676612854\n",
      "Epoch: 1, Batch: 580, Loss_G: 3.0041310787200928, Loss_D_A: 0.2536863386631012\n",
      "Epoch: 1, Batch: 590, Loss_G: 1.9150478839874268, Loss_D_A: 0.2498505711555481\n",
      "Epoch: 1, Batch: 600, Loss_G: 2.267465829849243, Loss_D_A: 0.25783878564834595\n",
      "Epoch: 1, Batch: 610, Loss_G: 2.049920082092285, Loss_D_A: 0.2528470456600189\n",
      "Epoch: 1, Batch: 620, Loss_G: 1.7306703329086304, Loss_D_A: 0.2531466484069824\n",
      "Epoch: 1, Batch: 630, Loss_G: 2.2017507553100586, Loss_D_A: 0.25424638390541077\n",
      "Epoch: 1, Batch: 640, Loss_G: 1.8565495014190674, Loss_D_A: 0.2517581284046173\n",
      "Epoch: 1, Batch: 650, Loss_G: 1.7000738382339478, Loss_D_A: 0.25881049036979675\n",
      "Epoch: 1, Batch: 660, Loss_G: 2.0313942432403564, Loss_D_A: 0.25454193353652954\n",
      "Epoch: 1, Batch: 670, Loss_G: 2.4819488525390625, Loss_D_A: 0.25919389724731445\n",
      "Epoch: 1, Batch: 680, Loss_G: 2.5295605659484863, Loss_D_A: 0.2564955949783325\n",
      "Epoch: 1, Batch: 690, Loss_G: 2.511183977127075, Loss_D_A: 0.27496254444122314\n",
      "Epoch: 1, Batch: 700, Loss_G: 2.0739850997924805, Loss_D_A: 0.25871601700782776\n",
      "Epoch: 1, Batch: 710, Loss_G: 3.7451953887939453, Loss_D_A: 0.2525166869163513\n",
      "Epoch: 1, Batch: 720, Loss_G: 2.5249717235565186, Loss_D_A: 0.2514716684818268\n",
      "Epoch: 1, Batch: 730, Loss_G: 2.479924201965332, Loss_D_A: 0.3178533911705017\n",
      "Epoch: 1, Batch: 740, Loss_G: 1.7423968315124512, Loss_D_A: 0.27703386545181274\n",
      "Epoch: 1, Batch: 750, Loss_G: 2.168285608291626, Loss_D_A: 0.24875840544700623\n",
      "Epoch: 1, Batch: 760, Loss_G: 1.9845770597457886, Loss_D_A: 0.2571873068809509\n",
      "Epoch: 1, Batch: 770, Loss_G: 1.9290449619293213, Loss_D_A: 0.25330138206481934\n",
      "Epoch: 1, Batch: 780, Loss_G: 2.1623599529266357, Loss_D_A: 0.2545146644115448\n",
      "Epoch: 1, Batch: 790, Loss_G: 2.109327793121338, Loss_D_A: 0.249862402677536\n",
      "Epoch: 1, Batch: 800, Loss_G: 1.7718865871429443, Loss_D_A: 0.2523534893989563\n",
      "Epoch: 1, Batch: 810, Loss_G: 1.9999053478240967, Loss_D_A: 0.254378080368042\n",
      "Epoch: 1, Batch: 820, Loss_G: 2.1768949031829834, Loss_D_A: 0.25459983944892883\n",
      "Epoch: 1, Batch: 830, Loss_G: 1.7863472700119019, Loss_D_A: 0.2522227466106415\n",
      "Epoch: 1, Batch: 840, Loss_G: 1.906116247177124, Loss_D_A: 0.2681410312652588\n",
      "Epoch: 1, Batch: 850, Loss_G: 2.0558807849884033, Loss_D_A: 0.2786034047603607\n",
      "Epoch: 1, Batch: 860, Loss_G: 1.9630558490753174, Loss_D_A: 0.364238440990448\n",
      "Epoch: 1, Batch: 870, Loss_G: 2.0331711769104004, Loss_D_A: 0.25346502661705017\n",
      "Epoch: 1, Batch: 880, Loss_G: 1.8891100883483887, Loss_D_A: 0.2588677406311035\n",
      "Epoch: 1, Batch: 890, Loss_G: 2.0204105377197266, Loss_D_A: 0.254971444606781\n",
      "Epoch: 1, Batch: 900, Loss_G: 1.9426562786102295, Loss_D_A: 0.25464457273483276\n",
      "Epoch: 1, Batch: 910, Loss_G: 2.077035427093506, Loss_D_A: 0.24965518712997437\n",
      "Epoch: 1, Batch: 920, Loss_G: 2.115638017654419, Loss_D_A: 0.2541900873184204\n",
      "Epoch: 1, Batch: 930, Loss_G: 2.0248911380767822, Loss_D_A: 0.25378525257110596\n",
      "Epoch: 1, Batch: 940, Loss_G: 1.7180780172348022, Loss_D_A: 0.2553119659423828\n",
      "Epoch: 1, Batch: 950, Loss_G: 1.836695909500122, Loss_D_A: 0.25880634784698486\n",
      "Epoch: 1, Batch: 960, Loss_G: 1.8466826677322388, Loss_D_A: 0.25575146079063416\n",
      "Epoch: 1, Batch: 970, Loss_G: 1.6816041469573975, Loss_D_A: 0.25032252073287964\n",
      "Epoch: 1, Batch: 980, Loss_G: 2.0513968467712402, Loss_D_A: 0.25648754835128784\n",
      "Epoch: 1, Batch: 990, Loss_G: 1.994949460029602, Loss_D_A: 0.2498903125524521\n",
      "Epoch: 1, Batch: 1000, Loss_G: 2.300164222717285, Loss_D_A: 0.2577167749404907\n",
      "Epoch: 1, Batch: 1010, Loss_G: 2.0813615322113037, Loss_D_A: 0.2541518807411194\n",
      "Epoch: 1, Batch: 1020, Loss_G: 2.1551434993743896, Loss_D_A: 0.24938350915908813\n",
      "Epoch: 1, Batch: 1030, Loss_G: 1.656296968460083, Loss_D_A: 0.2502404451370239\n",
      "Epoch: 1, Batch: 1040, Loss_G: 2.367905378341675, Loss_D_A: 0.2704639434814453\n",
      "Epoch: 1, Batch: 1050, Loss_G: 2.4509196281433105, Loss_D_A: 0.26781776547431946\n",
      "Epoch: 1, Batch: 1060, Loss_G: 2.087334156036377, Loss_D_A: 0.2602055072784424\n",
      "Epoch: 1, Batch: 1070, Loss_G: 1.7807729244232178, Loss_D_A: 0.257804811000824\n",
      "Epoch: 1, Batch: 1080, Loss_G: 2.096848726272583, Loss_D_A: 0.2680209279060364\n",
      "Epoch: 1, Batch: 1090, Loss_G: 1.6303883790969849, Loss_D_A: 0.26869508624076843\n",
      "Epoch: 1, Batch: 1100, Loss_G: 2.1893463134765625, Loss_D_A: 0.2593601942062378\n",
      "Epoch: 1, Batch: 1110, Loss_G: 1.829349398612976, Loss_D_A: 0.2649255692958832\n",
      "Epoch: 1, Batch: 1120, Loss_G: 1.7188124656677246, Loss_D_A: 0.2531159520149231\n",
      "Epoch: 1, Batch: 1130, Loss_G: 1.7818881273269653, Loss_D_A: 0.25230103731155396\n",
      "Epoch: 1, Batch: 1140, Loss_G: 1.9455642700195312, Loss_D_A: 0.2567683160305023\n",
      "Epoch: 1, Batch: 1150, Loss_G: 1.4663141965866089, Loss_D_A: 0.2699790596961975\n",
      "Epoch: 1, Batch: 1160, Loss_G: 2.050424098968506, Loss_D_A: 0.25986769795417786\n",
      "Epoch: 1, Batch: 1170, Loss_G: 1.8133786916732788, Loss_D_A: 0.26620709896087646\n",
      "Epoch: 1, Batch: 1180, Loss_G: 2.0688376426696777, Loss_D_A: 0.2581588923931122\n",
      "Epoch: 1, Batch: 1190, Loss_G: 1.5558189153671265, Loss_D_A: 0.276379257440567\n",
      "Epoch: 1, Batch: 1200, Loss_G: 1.7655185461044312, Loss_D_A: 0.25652819871902466\n",
      "Epoch: 1, Batch: 1210, Loss_G: 1.7572485208511353, Loss_D_A: 0.2582673132419586\n",
      "Epoch: 1, Batch: 1220, Loss_G: 1.8202885389328003, Loss_D_A: 0.25488778948783875\n",
      "Epoch: 2, Batch: 0, Loss_G: 2.012695074081421, Loss_D_A: 0.25123000144958496\n",
      "Epoch: 2, Batch: 10, Loss_G: 1.83805513381958, Loss_D_A: 0.2676335275173187\n",
      "Epoch: 2, Batch: 20, Loss_G: 1.8167369365692139, Loss_D_A: 0.261948823928833\n",
      "Epoch: 2, Batch: 30, Loss_G: 1.4803352355957031, Loss_D_A: 0.25325894355773926\n",
      "Epoch: 2, Batch: 40, Loss_G: 1.8527040481567383, Loss_D_A: 0.2527117431163788\n",
      "Epoch: 2, Batch: 50, Loss_G: 1.9474188089370728, Loss_D_A: 0.2538986802101135\n",
      "Epoch: 2, Batch: 60, Loss_G: 1.4065065383911133, Loss_D_A: 0.27245837450027466\n",
      "Epoch: 2, Batch: 70, Loss_G: 2.1127986907958984, Loss_D_A: 0.293069452047348\n",
      "Epoch: 2, Batch: 80, Loss_G: 1.686110496520996, Loss_D_A: 0.3059089779853821\n",
      "Epoch: 2, Batch: 90, Loss_G: 1.5361326932907104, Loss_D_A: 0.255862295627594\n",
      "Epoch: 2, Batch: 100, Loss_G: 1.5348528623580933, Loss_D_A: 0.26083463430404663\n",
      "Epoch: 2, Batch: 110, Loss_G: 1.4994244575500488, Loss_D_A: 0.2521265745162964\n",
      "Epoch: 2, Batch: 120, Loss_G: 1.9421067237854004, Loss_D_A: 0.25164878368377686\n",
      "Epoch: 2, Batch: 130, Loss_G: 1.58130943775177, Loss_D_A: 0.25471559166908264\n",
      "Epoch: 2, Batch: 140, Loss_G: 1.601863145828247, Loss_D_A: 0.2508314847946167\n",
      "Epoch: 2, Batch: 150, Loss_G: 1.7042956352233887, Loss_D_A: 0.3063661754131317\n",
      "Epoch: 2, Batch: 160, Loss_G: 1.8784070014953613, Loss_D_A: 0.26828423142433167\n",
      "Epoch: 2, Batch: 170, Loss_G: 1.4828225374221802, Loss_D_A: 0.2592034637928009\n",
      "Epoch: 2, Batch: 180, Loss_G: 2.288973093032837, Loss_D_A: 0.2528840899467468\n",
      "Epoch: 2, Batch: 190, Loss_G: 1.767073392868042, Loss_D_A: 0.26105207204818726\n",
      "Epoch: 2, Batch: 200, Loss_G: 1.7718874216079712, Loss_D_A: 0.25428101420402527\n",
      "Epoch: 2, Batch: 210, Loss_G: 1.6265943050384521, Loss_D_A: 0.2537393271923065\n",
      "Epoch: 2, Batch: 220, Loss_G: 1.5551211833953857, Loss_D_A: 0.25470632314682007\n",
      "Epoch: 2, Batch: 230, Loss_G: 1.7196520566940308, Loss_D_A: 0.2504689693450928\n",
      "Epoch: 2, Batch: 240, Loss_G: 1.5557653903961182, Loss_D_A: 0.2910504639148712\n",
      "Epoch: 2, Batch: 250, Loss_G: 1.8121883869171143, Loss_D_A: 0.27894529700279236\n",
      "Epoch: 2, Batch: 260, Loss_G: 1.6257193088531494, Loss_D_A: 0.2537682056427002\n",
      "Epoch: 2, Batch: 270, Loss_G: 1.722954511642456, Loss_D_A: 0.25462085008621216\n",
      "Epoch: 2, Batch: 280, Loss_G: 1.529078483581543, Loss_D_A: 0.2508141100406647\n",
      "Epoch: 2, Batch: 290, Loss_G: 1.7449264526367188, Loss_D_A: 0.25680726766586304\n",
      "Epoch: 2, Batch: 300, Loss_G: 1.6078685522079468, Loss_D_A: 0.25337257981300354\n",
      "Epoch: 2, Batch: 310, Loss_G: 2.000781536102295, Loss_D_A: 0.257322758436203\n",
      "Epoch: 2, Batch: 320, Loss_G: 1.256817102432251, Loss_D_A: 0.2588616609573364\n",
      "Epoch: 2, Batch: 330, Loss_G: 1.5244083404541016, Loss_D_A: 0.2636682391166687\n",
      "Epoch: 2, Batch: 340, Loss_G: 1.644208312034607, Loss_D_A: 0.25280290842056274\n",
      "Epoch: 2, Batch: 350, Loss_G: 1.6126432418823242, Loss_D_A: 0.25190412998199463\n",
      "Epoch: 2, Batch: 360, Loss_G: 1.4949662685394287, Loss_D_A: 0.2541125416755676\n",
      "Epoch: 2, Batch: 370, Loss_G: 1.5828465223312378, Loss_D_A: 0.30006322264671326\n",
      "Epoch: 2, Batch: 380, Loss_G: 1.8710486888885498, Loss_D_A: 0.35039129853248596\n",
      "Epoch: 2, Batch: 390, Loss_G: 1.571990728378296, Loss_D_A: 0.2526129186153412\n",
      "Epoch: 2, Batch: 400, Loss_G: 1.308110237121582, Loss_D_A: 0.25255870819091797\n",
      "Epoch: 2, Batch: 410, Loss_G: 1.3403220176696777, Loss_D_A: 0.25292348861694336\n",
      "Epoch: 2, Batch: 420, Loss_G: 1.5103453397750854, Loss_D_A: 0.2524682581424713\n",
      "Epoch: 2, Batch: 430, Loss_G: 1.7102267742156982, Loss_D_A: 0.2517086863517761\n",
      "Epoch: 2, Batch: 440, Loss_G: 1.3610724210739136, Loss_D_A: 0.2529758810997009\n",
      "Epoch: 2, Batch: 450, Loss_G: 1.4789600372314453, Loss_D_A: 0.25212422013282776\n",
      "Epoch: 2, Batch: 460, Loss_G: 1.6143288612365723, Loss_D_A: 0.2542613744735718\n",
      "Epoch: 2, Batch: 470, Loss_G: 1.479353904724121, Loss_D_A: 0.2506185472011566\n",
      "Epoch: 2, Batch: 480, Loss_G: 2.3434207439422607, Loss_D_A: 0.26379814743995667\n",
      "Epoch: 2, Batch: 490, Loss_G: 1.7876994609832764, Loss_D_A: 0.3340003490447998\n",
      "Epoch: 2, Batch: 500, Loss_G: 1.458907127380371, Loss_D_A: 0.25506237149238586\n",
      "Epoch: 2, Batch: 510, Loss_G: 1.6625045537948608, Loss_D_A: 0.25693178176879883\n",
      "Epoch: 2, Batch: 520, Loss_G: 1.4403204917907715, Loss_D_A: 0.2545723021030426\n",
      "Epoch: 2, Batch: 530, Loss_G: 1.7624561786651611, Loss_D_A: 0.2519994378089905\n",
      "Epoch: 2, Batch: 540, Loss_G: 1.664770245552063, Loss_D_A: 0.24960899353027344\n",
      "Epoch: 2, Batch: 550, Loss_G: 1.6234065294265747, Loss_D_A: 0.2661197781562805\n",
      "Epoch: 2, Batch: 560, Loss_G: 1.4008204936981201, Loss_D_A: 0.2837069630622864\n",
      "Epoch: 2, Batch: 570, Loss_G: 1.663893699645996, Loss_D_A: 0.2856888175010681\n",
      "Epoch: 2, Batch: 580, Loss_G: 1.2841475009918213, Loss_D_A: 0.2529938220977783\n",
      "Epoch: 2, Batch: 590, Loss_G: 1.555987000465393, Loss_D_A: 0.25686338543891907\n",
      "Epoch: 2, Batch: 600, Loss_G: 1.9193718433380127, Loss_D_A: 0.26394879817962646\n",
      "Epoch: 2, Batch: 610, Loss_G: 1.6980366706848145, Loss_D_A: 0.2683615982532501\n",
      "Epoch: 2, Batch: 620, Loss_G: 1.3674836158752441, Loss_D_A: 0.25429612398147583\n",
      "Epoch: 2, Batch: 630, Loss_G: 2.3087453842163086, Loss_D_A: 0.3744834065437317\n",
      "Epoch: 2, Batch: 640, Loss_G: 3.9654951095581055, Loss_D_A: 2.231034278869629\n",
      "Epoch: 2, Batch: 650, Loss_G: 2.7389397621154785, Loss_D_A: 1.0472993850708008\n",
      "Epoch: 2, Batch: 660, Loss_G: 1.9338287115097046, Loss_D_A: 0.24887827038764954\n",
      "Epoch: 2, Batch: 670, Loss_G: 1.612778902053833, Loss_D_A: 0.2532931864261627\n",
      "Epoch: 2, Batch: 680, Loss_G: 1.2884160280227661, Loss_D_A: 0.2547823488712311\n",
      "Epoch: 2, Batch: 690, Loss_G: 1.4070212841033936, Loss_D_A: 0.2518291771411896\n",
      "Epoch: 2, Batch: 700, Loss_G: 1.5461515188217163, Loss_D_A: 0.2539456784725189\n",
      "Epoch: 2, Batch: 710, Loss_G: 1.5272603034973145, Loss_D_A: 0.2506321966648102\n",
      "Epoch: 2, Batch: 720, Loss_G: 1.607931137084961, Loss_D_A: 0.24961194396018982\n",
      "Epoch: 2, Batch: 730, Loss_G: 1.845166563987732, Loss_D_A: 0.2551528811454773\n",
      "Epoch: 2, Batch: 740, Loss_G: 1.797998309135437, Loss_D_A: 0.2540411949157715\n",
      "Epoch: 2, Batch: 750, Loss_G: 2.2712576389312744, Loss_D_A: 0.25587084889411926\n",
      "Epoch: 2, Batch: 760, Loss_G: 1.3165243864059448, Loss_D_A: 0.2513485252857208\n",
      "Epoch: 2, Batch: 770, Loss_G: 1.3879494667053223, Loss_D_A: 0.25491881370544434\n",
      "Epoch: 2, Batch: 780, Loss_G: 1.3685591220855713, Loss_D_A: 0.25677230954170227\n",
      "Epoch: 2, Batch: 790, Loss_G: 1.77949059009552, Loss_D_A: 0.2537555694580078\n",
      "Epoch: 2, Batch: 800, Loss_G: 1.313896894454956, Loss_D_A: 0.2551339268684387\n",
      "Epoch: 2, Batch: 810, Loss_G: 1.4390661716461182, Loss_D_A: 0.2529533803462982\n",
      "Epoch: 2, Batch: 820, Loss_G: 1.6666274070739746, Loss_D_A: 0.2528798282146454\n",
      "Epoch: 2, Batch: 830, Loss_G: 1.0804495811462402, Loss_D_A: 0.2544712424278259\n",
      "Epoch: 2, Batch: 840, Loss_G: 1.560117483139038, Loss_D_A: 0.2528969645500183\n",
      "Epoch: 2, Batch: 850, Loss_G: 1.43489408493042, Loss_D_A: 0.24920034408569336\n",
      "Epoch: 2, Batch: 860, Loss_G: 1.6342525482177734, Loss_D_A: 0.2539673447608948\n",
      "Epoch: 2, Batch: 870, Loss_G: 1.4080867767333984, Loss_D_A: 0.2531135678291321\n",
      "Epoch: 2, Batch: 880, Loss_G: 1.5048530101776123, Loss_D_A: 0.2526281476020813\n",
      "Epoch: 2, Batch: 890, Loss_G: 1.4496301412582397, Loss_D_A: 0.2485509067773819\n",
      "Epoch: 2, Batch: 900, Loss_G: 1.1448509693145752, Loss_D_A: 0.2524200677871704\n",
      "Epoch: 2, Batch: 910, Loss_G: 1.2102614641189575, Loss_D_A: 0.2528423070907593\n",
      "Epoch: 2, Batch: 920, Loss_G: 1.4105955362319946, Loss_D_A: 0.25418105721473694\n",
      "Epoch: 2, Batch: 930, Loss_G: 1.0526779890060425, Loss_D_A: 0.25339967012405396\n",
      "Epoch: 2, Batch: 940, Loss_G: 1.4058148860931396, Loss_D_A: 0.2502018213272095\n",
      "Epoch: 2, Batch: 950, Loss_G: 1.4731420278549194, Loss_D_A: 0.25609976053237915\n",
      "Epoch: 2, Batch: 960, Loss_G: 1.1243557929992676, Loss_D_A: 0.250680148601532\n",
      "Epoch: 2, Batch: 970, Loss_G: 1.4644129276275635, Loss_D_A: 0.25120800733566284\n",
      "Epoch: 2, Batch: 980, Loss_G: 1.639270305633545, Loss_D_A: 0.25251978635787964\n",
      "Epoch: 2, Batch: 990, Loss_G: 1.399230718612671, Loss_D_A: 0.2520195543766022\n",
      "Epoch: 2, Batch: 1000, Loss_G: 1.8786931037902832, Loss_D_A: 0.24995633959770203\n",
      "Epoch: 2, Batch: 1010, Loss_G: 1.2955987453460693, Loss_D_A: 0.24849365651607513\n",
      "Epoch: 2, Batch: 1020, Loss_G: 1.3623573780059814, Loss_D_A: 0.25021740794181824\n",
      "Epoch: 2, Batch: 1030, Loss_G: 1.367727279663086, Loss_D_A: 0.2519933581352234\n",
      "Epoch: 2, Batch: 1040, Loss_G: 1.391494870185852, Loss_D_A: 0.2533359229564667\n",
      "Epoch: 2, Batch: 1050, Loss_G: 1.4885151386260986, Loss_D_A: 0.25699955224990845\n",
      "Epoch: 2, Batch: 1060, Loss_G: 1.2418627738952637, Loss_D_A: 0.25373080372810364\n",
      "Epoch: 2, Batch: 1070, Loss_G: 1.3313488960266113, Loss_D_A: 0.25201180577278137\n",
      "Epoch: 2, Batch: 1080, Loss_G: 1.6356775760650635, Loss_D_A: 0.2508324086666107\n",
      "Epoch: 2, Batch: 1090, Loss_G: 1.2259351015090942, Loss_D_A: 0.25254184007644653\n",
      "Epoch: 2, Batch: 1100, Loss_G: 1.4266692399978638, Loss_D_A: 0.2537836730480194\n",
      "Epoch: 2, Batch: 1110, Loss_G: 1.8449780941009521, Loss_D_A: 0.2508902847766876\n",
      "Epoch: 2, Batch: 1120, Loss_G: 1.5907951593399048, Loss_D_A: 0.2518743872642517\n",
      "Epoch: 2, Batch: 1130, Loss_G: 1.4995293617248535, Loss_D_A: 0.2525354027748108\n",
      "Epoch: 2, Batch: 1140, Loss_G: 1.484370470046997, Loss_D_A: 0.24872718751430511\n",
      "Epoch: 2, Batch: 1150, Loss_G: 1.3952168226242065, Loss_D_A: 0.2469259351491928\n",
      "Epoch: 2, Batch: 1160, Loss_G: 1.5896737575531006, Loss_D_A: 0.2434779703617096\n",
      "Epoch: 2, Batch: 1170, Loss_G: 1.9485867023468018, Loss_D_A: 0.25122079253196716\n",
      "Epoch: 2, Batch: 1180, Loss_G: 1.5645029544830322, Loss_D_A: 0.24808157980442047\n",
      "Epoch: 2, Batch: 1190, Loss_G: 1.3886682987213135, Loss_D_A: 0.2505984604358673\n",
      "Epoch: 2, Batch: 1200, Loss_G: 1.2209718227386475, Loss_D_A: 0.25252699851989746\n",
      "Epoch: 2, Batch: 1210, Loss_G: 1.4309008121490479, Loss_D_A: 0.2517088055610657\n",
      "Epoch: 2, Batch: 1220, Loss_G: 1.3792016506195068, Loss_D_A: 0.2483750879764557\n",
      "Epoch: 3, Batch: 0, Loss_G: 1.4372355937957764, Loss_D_A: 0.2536182701587677\n",
      "Epoch: 3, Batch: 10, Loss_G: 1.3312450647354126, Loss_D_A: 0.25211021304130554\n",
      "Epoch: 3, Batch: 20, Loss_G: 1.7503950595855713, Loss_D_A: 0.2503693997859955\n",
      "Epoch: 3, Batch: 30, Loss_G: 1.8040257692337036, Loss_D_A: 0.2551170587539673\n",
      "Epoch: 3, Batch: 40, Loss_G: 1.3226803541183472, Loss_D_A: 0.2495453804731369\n",
      "Epoch: 3, Batch: 50, Loss_G: 1.700746774673462, Loss_D_A: 0.2499672770500183\n",
      "Epoch: 3, Batch: 60, Loss_G: 1.2085314989089966, Loss_D_A: 0.2528270184993744\n",
      "Epoch: 3, Batch: 70, Loss_G: 1.3572379350662231, Loss_D_A: 0.25118207931518555\n",
      "Epoch: 3, Batch: 80, Loss_G: 1.6455618143081665, Loss_D_A: 0.254202276468277\n",
      "Epoch: 3, Batch: 90, Loss_G: 1.5212701559066772, Loss_D_A: 0.2486501783132553\n",
      "Epoch: 3, Batch: 100, Loss_G: 1.5422377586364746, Loss_D_A: 0.2474050372838974\n",
      "Epoch: 3, Batch: 110, Loss_G: 1.423244833946228, Loss_D_A: 0.25016847252845764\n",
      "Epoch: 3, Batch: 120, Loss_G: 1.483832597732544, Loss_D_A: 0.2529214918613434\n",
      "Epoch: 3, Batch: 130, Loss_G: 1.6767675876617432, Loss_D_A: 0.2523317039012909\n",
      "Epoch: 3, Batch: 140, Loss_G: 1.6280977725982666, Loss_D_A: 0.24838833510875702\n",
      "Epoch: 3, Batch: 150, Loss_G: 1.2057454586029053, Loss_D_A: 0.2590941786766052\n",
      "Epoch: 3, Batch: 160, Loss_G: 1.1377226114273071, Loss_D_A: 0.31600719690322876\n",
      "Epoch: 3, Batch: 170, Loss_G: 1.4814155101776123, Loss_D_A: 0.253328800201416\n",
      "Epoch: 3, Batch: 180, Loss_G: 1.4585603475570679, Loss_D_A: 0.2511700391769409\n",
      "Epoch: 3, Batch: 190, Loss_G: 1.459316611289978, Loss_D_A: 0.26022979617118835\n",
      "Epoch: 3, Batch: 200, Loss_G: 1.3466564416885376, Loss_D_A: 0.25646886229515076\n",
      "Epoch: 3, Batch: 210, Loss_G: 1.4162521362304688, Loss_D_A: 0.25319337844848633\n",
      "Epoch: 3, Batch: 220, Loss_G: 1.1148371696472168, Loss_D_A: 0.25369083881378174\n",
      "Epoch: 3, Batch: 230, Loss_G: 1.1711900234222412, Loss_D_A: 0.24919047951698303\n",
      "Epoch: 3, Batch: 240, Loss_G: 1.3234245777130127, Loss_D_A: 0.2520233690738678\n",
      "Epoch: 3, Batch: 250, Loss_G: 1.5334300994873047, Loss_D_A: 0.24844977259635925\n",
      "Epoch: 3, Batch: 260, Loss_G: 1.335113525390625, Loss_D_A: 0.25008103251457214\n",
      "Epoch: 3, Batch: 270, Loss_G: 1.3224282264709473, Loss_D_A: 0.25427788496017456\n",
      "Epoch: 3, Batch: 280, Loss_G: 1.1461102962493896, Loss_D_A: 0.2703235447406769\n",
      "Epoch: 3, Batch: 290, Loss_G: 1.5547895431518555, Loss_D_A: 0.2930809259414673\n",
      "Epoch: 3, Batch: 300, Loss_G: 1.417042851448059, Loss_D_A: 0.25624972581863403\n",
      "Epoch: 3, Batch: 310, Loss_G: 1.4937615394592285, Loss_D_A: 0.25042226910591125\n",
      "Epoch: 3, Batch: 320, Loss_G: 1.3642172813415527, Loss_D_A: 0.2602234482765198\n",
      "Epoch: 3, Batch: 330, Loss_G: 1.4154185056686401, Loss_D_A: 0.2505253255367279\n",
      "Epoch: 3, Batch: 340, Loss_G: 1.3115990161895752, Loss_D_A: 0.2514036297798157\n",
      "Epoch: 3, Batch: 350, Loss_G: 1.4112908840179443, Loss_D_A: 0.2612684667110443\n",
      "Epoch: 3, Batch: 360, Loss_G: 1.3768606185913086, Loss_D_A: 0.2558072805404663\n",
      "Epoch: 3, Batch: 370, Loss_G: 1.3004882335662842, Loss_D_A: 0.25518858432769775\n",
      "Epoch: 3, Batch: 380, Loss_G: 1.182815670967102, Loss_D_A: 0.25254327058792114\n",
      "Epoch: 3, Batch: 390, Loss_G: 1.3156473636627197, Loss_D_A: 0.4557274878025055\n",
      "Epoch: 3, Batch: 400, Loss_G: 1.3895015716552734, Loss_D_A: 0.25178560614585876\n",
      "Epoch: 3, Batch: 410, Loss_G: 1.1641693115234375, Loss_D_A: 0.2526136636734009\n",
      "Epoch: 3, Batch: 420, Loss_G: 1.592117428779602, Loss_D_A: 0.27107176184654236\n",
      "Epoch: 3, Batch: 430, Loss_G: 1.231553316116333, Loss_D_A: 0.2659159004688263\n",
      "Epoch: 3, Batch: 440, Loss_G: 1.0275437831878662, Loss_D_A: 0.2544170618057251\n",
      "Epoch: 3, Batch: 450, Loss_G: 1.3194777965545654, Loss_D_A: 0.2561168670654297\n",
      "Epoch: 3, Batch: 460, Loss_G: 1.27170991897583, Loss_D_A: 0.25285598635673523\n",
      "Epoch: 3, Batch: 470, Loss_G: 1.3391246795654297, Loss_D_A: 0.2558629512786865\n",
      "Epoch: 3, Batch: 480, Loss_G: 1.3974522352218628, Loss_D_A: 0.25640469789505005\n",
      "Epoch: 3, Batch: 490, Loss_G: 1.161935806274414, Loss_D_A: 0.25603795051574707\n",
      "Epoch: 3, Batch: 500, Loss_G: 1.13412606716156, Loss_D_A: 0.2685683071613312\n",
      "Epoch: 3, Batch: 510, Loss_G: 1.1966466903686523, Loss_D_A: 0.25811925530433655\n",
      "Epoch: 3, Batch: 520, Loss_G: 1.3179620504379272, Loss_D_A: 0.25578954815864563\n",
      "Epoch: 3, Batch: 530, Loss_G: 1.2272024154663086, Loss_D_A: 0.27761051058769226\n",
      "Epoch: 3, Batch: 540, Loss_G: 1.35463547706604, Loss_D_A: 0.25227686762809753\n",
      "Epoch: 3, Batch: 550, Loss_G: 1.891902208328247, Loss_D_A: 0.3014551103115082\n",
      "Epoch: 3, Batch: 560, Loss_G: 1.9999264478683472, Loss_D_A: 0.4987836480140686\n",
      "Epoch: 3, Batch: 570, Loss_G: 16.90019416809082, Loss_D_A: 13.599010467529297\n",
      "Epoch: 3, Batch: 580, Loss_G: 9.42984390258789, Loss_D_A: 1.0668973922729492\n",
      "Epoch: 3, Batch: 590, Loss_G: 2.7458410263061523, Loss_D_A: 0.2617592215538025\n",
      "Epoch: 3, Batch: 600, Loss_G: 2.535693883895874, Loss_D_A: 0.25989672541618347\n",
      "Epoch: 3, Batch: 610, Loss_G: 2.207521438598633, Loss_D_A: 0.2800448536872864\n",
      "Epoch: 3, Batch: 620, Loss_G: 1.545243501663208, Loss_D_A: 0.2568458914756775\n",
      "Epoch: 3, Batch: 630, Loss_G: 1.9017213582992554, Loss_D_A: 0.2512574791908264\n",
      "Epoch: 3, Batch: 640, Loss_G: 1.7256345748901367, Loss_D_A: 0.2643640637397766\n",
      "Epoch: 3, Batch: 650, Loss_G: 1.4941160678863525, Loss_D_A: 0.2663366198539734\n",
      "Epoch: 3, Batch: 660, Loss_G: 1.787501573562622, Loss_D_A: 0.2678016722202301\n",
      "Epoch: 3, Batch: 670, Loss_G: 1.8678966760635376, Loss_D_A: 0.25575730204582214\n",
      "Epoch: 3, Batch: 680, Loss_G: 1.347870945930481, Loss_D_A: 0.2583504915237427\n",
      "Epoch: 3, Batch: 690, Loss_G: 1.6763637065887451, Loss_D_A: 0.2585919201374054\n",
      "Epoch: 3, Batch: 700, Loss_G: 1.4981276988983154, Loss_D_A: 0.2565873861312866\n",
      "Epoch: 3, Batch: 710, Loss_G: 1.3173547983169556, Loss_D_A: 0.2536626160144806\n",
      "Epoch: 3, Batch: 720, Loss_G: 2.232537269592285, Loss_D_A: 0.2542889416217804\n",
      "Epoch: 3, Batch: 730, Loss_G: 1.610959529876709, Loss_D_A: 0.2541145980358124\n",
      "Epoch: 3, Batch: 740, Loss_G: 1.29416823387146, Loss_D_A: 0.25109854340553284\n",
      "Epoch: 3, Batch: 750, Loss_G: 1.581552505493164, Loss_D_A: 0.2542790472507477\n",
      "Epoch: 3, Batch: 760, Loss_G: 1.8493777513504028, Loss_D_A: 0.2509196400642395\n",
      "Epoch: 3, Batch: 770, Loss_G: 1.173801302909851, Loss_D_A: 0.2549090087413788\n",
      "Epoch: 3, Batch: 780, Loss_G: 1.6506065130233765, Loss_D_A: 0.25498947501182556\n",
      "Epoch: 3, Batch: 790, Loss_G: 1.5854510068893433, Loss_D_A: 0.2519880533218384\n",
      "Epoch: 3, Batch: 800, Loss_G: 1.777582049369812, Loss_D_A: 0.25847768783569336\n",
      "Epoch: 3, Batch: 810, Loss_G: 1.3209093809127808, Loss_D_A: 0.2541950047016144\n",
      "Epoch: 3, Batch: 820, Loss_G: 1.3120903968811035, Loss_D_A: 0.2510412931442261\n",
      "Epoch: 3, Batch: 830, Loss_G: 1.7145516872406006, Loss_D_A: 0.253563791513443\n",
      "Epoch: 3, Batch: 840, Loss_G: 1.2745517492294312, Loss_D_A: 0.25484129786491394\n",
      "Epoch: 3, Batch: 850, Loss_G: 1.2155312299728394, Loss_D_A: 0.25206729769706726\n",
      "Epoch: 3, Batch: 860, Loss_G: 1.4328408241271973, Loss_D_A: 0.25219476222991943\n",
      "Epoch: 3, Batch: 870, Loss_G: 1.2202646732330322, Loss_D_A: 0.25627797842025757\n",
      "Epoch: 3, Batch: 880, Loss_G: 1.451873779296875, Loss_D_A: 0.25309237837791443\n",
      "Epoch: 3, Batch: 890, Loss_G: 1.442857027053833, Loss_D_A: 0.25170043110847473\n",
      "Epoch: 3, Batch: 900, Loss_G: 1.3569681644439697, Loss_D_A: 0.25068116188049316\n",
      "Epoch: 3, Batch: 910, Loss_G: 1.31233811378479, Loss_D_A: 0.2523106634616852\n",
      "Epoch: 3, Batch: 920, Loss_G: 1.3829854726791382, Loss_D_A: 0.2535034418106079\n",
      "Epoch: 3, Batch: 930, Loss_G: 1.2916829586029053, Loss_D_A: 0.2504735589027405\n",
      "Epoch: 3, Batch: 940, Loss_G: 1.5601876974105835, Loss_D_A: 0.24863648414611816\n",
      "Epoch: 3, Batch: 950, Loss_G: 1.3542829751968384, Loss_D_A: 0.25052952766418457\n",
      "Epoch: 3, Batch: 960, Loss_G: 1.4081478118896484, Loss_D_A: 0.250278502702713\n",
      "Epoch: 3, Batch: 970, Loss_G: 1.3966145515441895, Loss_D_A: 0.2565033733844757\n",
      "Epoch: 3, Batch: 980, Loss_G: 1.3527143001556396, Loss_D_A: 0.25328677892684937\n",
      "Epoch: 3, Batch: 990, Loss_G: 1.33988356590271, Loss_D_A: 0.25604012608528137\n",
      "Epoch: 3, Batch: 1000, Loss_G: 1.3471736907958984, Loss_D_A: 0.25120431184768677\n",
      "Epoch: 3, Batch: 1010, Loss_G: 0.9186654090881348, Loss_D_A: 0.25234919786453247\n",
      "Epoch: 3, Batch: 1020, Loss_G: 1.304506778717041, Loss_D_A: 0.25837570428848267\n",
      "Epoch: 3, Batch: 1030, Loss_G: 1.0327353477478027, Loss_D_A: 0.2565414011478424\n",
      "Epoch: 3, Batch: 1040, Loss_G: 1.257529377937317, Loss_D_A: 0.26682114601135254\n",
      "Epoch: 3, Batch: 1050, Loss_G: 1.3602182865142822, Loss_D_A: 0.25158047676086426\n",
      "Epoch: 3, Batch: 1060, Loss_G: 1.276613712310791, Loss_D_A: 0.25237947702407837\n",
      "Epoch: 3, Batch: 1070, Loss_G: 1.0113868713378906, Loss_D_A: 0.2512328624725342\n",
      "Epoch: 3, Batch: 1080, Loss_G: 1.3187487125396729, Loss_D_A: 0.2508968114852905\n",
      "Epoch: 3, Batch: 1090, Loss_G: 1.1793514490127563, Loss_D_A: 0.2535116672515869\n",
      "Epoch: 3, Batch: 1100, Loss_G: 1.5518004894256592, Loss_D_A: 0.26620566844940186\n",
      "Epoch: 3, Batch: 1110, Loss_G: 1.5091445446014404, Loss_D_A: 0.2569735050201416\n",
      "Epoch: 3, Batch: 1120, Loss_G: 1.5184237957000732, Loss_D_A: 0.29489973187446594\n",
      "Epoch: 3, Batch: 1130, Loss_G: 1.4493157863616943, Loss_D_A: 0.26116514205932617\n",
      "Epoch: 3, Batch: 1140, Loss_G: 1.1128009557724, Loss_D_A: 0.25968775153160095\n",
      "Epoch: 3, Batch: 1150, Loss_G: 1.1032201051712036, Loss_D_A: 0.2518472671508789\n",
      "Epoch: 3, Batch: 1160, Loss_G: 1.4118785858154297, Loss_D_A: 0.2745799720287323\n",
      "Epoch: 3, Batch: 1170, Loss_G: 1.2630836963653564, Loss_D_A: 0.25490236282348633\n",
      "Epoch: 3, Batch: 1180, Loss_G: 1.4587454795837402, Loss_D_A: 0.25553178787231445\n",
      "Epoch: 3, Batch: 1190, Loss_G: 1.4687033891677856, Loss_D_A: 0.25329309701919556\n",
      "Epoch: 3, Batch: 1200, Loss_G: 1.3542909622192383, Loss_D_A: 0.2530919015407562\n",
      "Epoch: 3, Batch: 1210, Loss_G: 1.0109504461288452, Loss_D_A: 0.2867085933685303\n",
      "Epoch: 3, Batch: 1220, Loss_G: 1.2872350215911865, Loss_D_A: 0.25503867864608765\n",
      "Epoch: 4, Batch: 0, Loss_G: 1.4191036224365234, Loss_D_A: 0.26037514209747314\n",
      "Epoch: 4, Batch: 10, Loss_G: 1.157011866569519, Loss_D_A: 0.2522745430469513\n",
      "Epoch: 4, Batch: 20, Loss_G: 1.081939458847046, Loss_D_A: 0.2610325217247009\n",
      "Epoch: 4, Batch: 30, Loss_G: 1.4265955686569214, Loss_D_A: 0.2573966383934021\n",
      "Epoch: 4, Batch: 40, Loss_G: 1.2719624042510986, Loss_D_A: 0.2649257183074951\n",
      "Epoch: 4, Batch: 50, Loss_G: 1.193686842918396, Loss_D_A: 0.3412352204322815\n",
      "Epoch: 4, Batch: 60, Loss_G: 1.082438349723816, Loss_D_A: 0.25750723481178284\n",
      "Epoch: 4, Batch: 70, Loss_G: 1.1048715114593506, Loss_D_A: 0.25331011414527893\n",
      "Epoch: 4, Batch: 80, Loss_G: 1.1523423194885254, Loss_D_A: 0.24789994955062866\n",
      "Epoch: 4, Batch: 90, Loss_G: 1.5941734313964844, Loss_D_A: 0.24782028794288635\n",
      "Epoch: 4, Batch: 100, Loss_G: 1.137296199798584, Loss_D_A: 0.25755369663238525\n",
      "Epoch: 4, Batch: 110, Loss_G: 1.4172370433807373, Loss_D_A: 0.2674110531806946\n",
      "Epoch: 4, Batch: 120, Loss_G: 1.2125632762908936, Loss_D_A: 0.2590048909187317\n",
      "Epoch: 4, Batch: 130, Loss_G: 1.3346364498138428, Loss_D_A: 0.32350945472717285\n",
      "Epoch: 4, Batch: 140, Loss_G: 1.3953282833099365, Loss_D_A: 0.2718581259250641\n",
      "Epoch: 4, Batch: 150, Loss_G: 1.2060353755950928, Loss_D_A: 0.2517915666103363\n",
      "Epoch: 4, Batch: 160, Loss_G: 1.3495326042175293, Loss_D_A: 0.26291215419769287\n",
      "Epoch: 4, Batch: 170, Loss_G: 0.9854983687400818, Loss_D_A: 0.29025787115097046\n",
      "Epoch: 4, Batch: 180, Loss_G: 1.081333041191101, Loss_D_A: 0.2891698479652405\n",
      "Epoch: 4, Batch: 190, Loss_G: 1.010815978050232, Loss_D_A: 0.2501588463783264\n",
      "Epoch: 4, Batch: 200, Loss_G: 1.1781498193740845, Loss_D_A: 0.2549211084842682\n",
      "Epoch: 4, Batch: 210, Loss_G: 1.43280827999115, Loss_D_A: 0.25132131576538086\n",
      "Epoch: 4, Batch: 220, Loss_G: 1.3555790185928345, Loss_D_A: 0.2614900469779968\n",
      "Epoch: 4, Batch: 230, Loss_G: 1.1436983346939087, Loss_D_A: 0.26181820034980774\n",
      "Epoch: 4, Batch: 240, Loss_G: 1.1597007513046265, Loss_D_A: 0.25213122367858887\n",
      "Epoch: 4, Batch: 250, Loss_G: 1.222041130065918, Loss_D_A: 0.24948257207870483\n",
      "Epoch: 4, Batch: 260, Loss_G: 1.2289057970046997, Loss_D_A: 0.2927672266960144\n",
      "Epoch: 4, Batch: 270, Loss_G: 1.391108751296997, Loss_D_A: 0.25012636184692383\n",
      "Epoch: 4, Batch: 280, Loss_G: 1.0554577112197876, Loss_D_A: 0.2585465610027313\n",
      "Epoch: 4, Batch: 290, Loss_G: 1.124046802520752, Loss_D_A: 0.25505393743515015\n",
      "Epoch: 4, Batch: 300, Loss_G: 1.157047152519226, Loss_D_A: 0.24975837767124176\n",
      "Epoch: 4, Batch: 310, Loss_G: 1.5428580045700073, Loss_D_A: 0.24928376078605652\n",
      "Epoch: 4, Batch: 320, Loss_G: 1.050323247909546, Loss_D_A: 0.2528979778289795\n",
      "Epoch: 4, Batch: 330, Loss_G: 1.3531029224395752, Loss_D_A: 0.26073309779167175\n",
      "Epoch: 4, Batch: 340, Loss_G: 1.3464196920394897, Loss_D_A: 0.2503359913825989\n",
      "Epoch: 4, Batch: 350, Loss_G: 1.3518331050872803, Loss_D_A: 0.296985387802124\n",
      "Epoch: 4, Batch: 360, Loss_G: 1.0716602802276611, Loss_D_A: 0.2538605332374573\n",
      "Epoch: 4, Batch: 370, Loss_G: 1.3879873752593994, Loss_D_A: 0.24839358031749725\n",
      "Epoch: 4, Batch: 380, Loss_G: 1.0222282409667969, Loss_D_A: 0.25072795152664185\n",
      "Epoch: 4, Batch: 390, Loss_G: 1.4108725786209106, Loss_D_A: 0.26735761761665344\n",
      "Epoch: 4, Batch: 400, Loss_G: 1.4204398393630981, Loss_D_A: 0.2523677349090576\n",
      "Epoch: 4, Batch: 410, Loss_G: 1.109178900718689, Loss_D_A: 0.2524915337562561\n",
      "Epoch: 4, Batch: 420, Loss_G: 1.642916202545166, Loss_D_A: 0.43451255559921265\n",
      "Epoch: 4, Batch: 430, Loss_G: 1.336674690246582, Loss_D_A: 0.2584139406681061\n",
      "Epoch: 4, Batch: 440, Loss_G: 1.2104666233062744, Loss_D_A: 0.24703837931156158\n",
      "Epoch: 4, Batch: 450, Loss_G: 1.5880753993988037, Loss_D_A: 0.2577764391899109\n",
      "Epoch: 4, Batch: 460, Loss_G: 1.4379680156707764, Loss_D_A: 0.3415643870830536\n",
      "Epoch: 4, Batch: 470, Loss_G: 1.4430079460144043, Loss_D_A: 0.25326675176620483\n",
      "Epoch: 4, Batch: 480, Loss_G: 1.058959722518921, Loss_D_A: 0.2525426745414734\n",
      "Epoch: 4, Batch: 490, Loss_G: 1.2807854413986206, Loss_D_A: 0.25574085116386414\n",
      "Epoch: 4, Batch: 500, Loss_G: 1.250908613204956, Loss_D_A: 0.2596753239631653\n",
      "Epoch: 4, Batch: 510, Loss_G: 1.6945483684539795, Loss_D_A: 0.30150938034057617\n",
      "Epoch: 4, Batch: 520, Loss_G: 1.1650152206420898, Loss_D_A: 0.2565009593963623\n",
      "Epoch: 4, Batch: 530, Loss_G: 1.038288950920105, Loss_D_A: 0.25154292583465576\n",
      "Epoch: 4, Batch: 540, Loss_G: 1.1550976037979126, Loss_D_A: 0.25126001238822937\n",
      "Epoch: 4, Batch: 550, Loss_G: 1.1597180366516113, Loss_D_A: 0.25020405650138855\n",
      "Epoch: 4, Batch: 560, Loss_G: 1.09483003616333, Loss_D_A: 0.2504023313522339\n",
      "Epoch: 4, Batch: 570, Loss_G: 1.307112216949463, Loss_D_A: 0.251223087310791\n",
      "Epoch: 4, Batch: 580, Loss_G: 1.2459181547164917, Loss_D_A: 0.2671450972557068\n",
      "Epoch: 4, Batch: 590, Loss_G: 1.515486717224121, Loss_D_A: 0.25027647614479065\n",
      "Epoch: 4, Batch: 600, Loss_G: 1.474581003189087, Loss_D_A: 0.24851647019386292\n",
      "Epoch: 4, Batch: 610, Loss_G: 1.3039946556091309, Loss_D_A: 0.2529605031013489\n",
      "Epoch: 4, Batch: 620, Loss_G: 1.203813910484314, Loss_D_A: 0.249923974275589\n",
      "Epoch: 4, Batch: 630, Loss_G: 1.3731571435928345, Loss_D_A: 0.31430691480636597\n",
      "Epoch: 4, Batch: 640, Loss_G: 2.4043924808502197, Loss_D_A: 0.9381540417671204\n",
      "Epoch: 4, Batch: 650, Loss_G: 1.075361728668213, Loss_D_A: 0.25182202458381653\n",
      "Epoch: 4, Batch: 660, Loss_G: 1.0484254360198975, Loss_D_A: 0.25308939814567566\n",
      "Epoch: 4, Batch: 670, Loss_G: 1.2317581176757812, Loss_D_A: 0.27174508571624756\n",
      "Epoch: 4, Batch: 680, Loss_G: 1.0628142356872559, Loss_D_A: 0.2513183057308197\n",
      "Epoch: 4, Batch: 690, Loss_G: 1.4100908041000366, Loss_D_A: 0.25011759996414185\n",
      "Epoch: 4, Batch: 700, Loss_G: 1.3806636333465576, Loss_D_A: 0.2467840015888214\n",
      "Epoch: 4, Batch: 710, Loss_G: 1.23982834815979, Loss_D_A: 0.24797028303146362\n",
      "Epoch: 4, Batch: 720, Loss_G: 1.1260592937469482, Loss_D_A: 0.2484319806098938\n",
      "Epoch: 4, Batch: 730, Loss_G: 1.22590970993042, Loss_D_A: 0.25743159651756287\n",
      "Epoch: 4, Batch: 740, Loss_G: 1.150728702545166, Loss_D_A: 0.24616453051567078\n",
      "Epoch: 4, Batch: 750, Loss_G: 1.0343416929244995, Loss_D_A: 0.26587724685668945\n",
      "Epoch: 4, Batch: 760, Loss_G: 1.2023365497589111, Loss_D_A: 0.2531438171863556\n",
      "Epoch: 4, Batch: 770, Loss_G: 1.2050106525421143, Loss_D_A: 0.2529052495956421\n",
      "Epoch: 4, Batch: 780, Loss_G: 1.2038588523864746, Loss_D_A: 0.2610247731208801\n",
      "Epoch: 4, Batch: 790, Loss_G: 1.393757939338684, Loss_D_A: 0.255969375371933\n",
      "Epoch: 4, Batch: 800, Loss_G: 1.0622031688690186, Loss_D_A: 0.25726842880249023\n",
      "Epoch: 4, Batch: 810, Loss_G: 1.1679675579071045, Loss_D_A: 0.279590368270874\n",
      "Epoch: 4, Batch: 820, Loss_G: 1.1369552612304688, Loss_D_A: 0.24914568662643433\n",
      "Epoch: 4, Batch: 830, Loss_G: 1.0275003910064697, Loss_D_A: 0.25074440240859985\n",
      "Epoch: 4, Batch: 840, Loss_G: 1.1910619735717773, Loss_D_A: 0.2568383812904358\n",
      "Epoch: 4, Batch: 850, Loss_G: 1.2342454195022583, Loss_D_A: 0.2559123933315277\n",
      "Epoch: 4, Batch: 860, Loss_G: 1.297480583190918, Loss_D_A: 0.24934245645999908\n",
      "Epoch: 4, Batch: 870, Loss_G: 1.0576945543289185, Loss_D_A: 0.25316718220710754\n",
      "Epoch: 4, Batch: 880, Loss_G: 1.0237363576889038, Loss_D_A: 0.29550236463546753\n",
      "Epoch: 4, Batch: 890, Loss_G: 1.0547330379486084, Loss_D_A: 0.27969399094581604\n",
      "Epoch: 4, Batch: 900, Loss_G: 1.2628507614135742, Loss_D_A: 0.2533867657184601\n",
      "Epoch: 4, Batch: 910, Loss_G: 1.1303284168243408, Loss_D_A: 0.2606630027294159\n",
      "Epoch: 4, Batch: 920, Loss_G: 1.2347432374954224, Loss_D_A: 0.25942665338516235\n",
      "Epoch: 4, Batch: 930, Loss_G: 1.1985297203063965, Loss_D_A: 0.28213346004486084\n",
      "Epoch: 4, Batch: 940, Loss_G: 1.1797460317611694, Loss_D_A: 0.25551706552505493\n",
      "Epoch: 4, Batch: 950, Loss_G: 1.180182933807373, Loss_D_A: 0.2510092258453369\n",
      "Epoch: 4, Batch: 960, Loss_G: 1.323049545288086, Loss_D_A: 0.2532072067260742\n",
      "Epoch: 4, Batch: 970, Loss_G: 1.0847753286361694, Loss_D_A: 0.26123303174972534\n",
      "Epoch: 4, Batch: 980, Loss_G: 1.504577398300171, Loss_D_A: 0.28171005845069885\n",
      "Epoch: 4, Batch: 990, Loss_G: 1.569113850593567, Loss_D_A: 0.2856891453266144\n",
      "Epoch: 4, Batch: 1000, Loss_G: 1.2696070671081543, Loss_D_A: 0.25044381618499756\n",
      "Epoch: 4, Batch: 1010, Loss_G: 1.5883781909942627, Loss_D_A: 0.31865012645721436\n",
      "Epoch: 4, Batch: 1020, Loss_G: 1.0319042205810547, Loss_D_A: 0.29829567670822144\n",
      "Epoch: 4, Batch: 1030, Loss_G: 1.296891689300537, Loss_D_A: 0.4969954788684845\n",
      "Epoch: 4, Batch: 1040, Loss_G: 1.2411832809448242, Loss_D_A: 0.26329168677330017\n",
      "Epoch: 4, Batch: 1050, Loss_G: 1.0567454099655151, Loss_D_A: 0.2533373236656189\n",
      "Epoch: 4, Batch: 1060, Loss_G: 1.148350715637207, Loss_D_A: 0.2509843409061432\n",
      "Epoch: 4, Batch: 1070, Loss_G: 1.0977160930633545, Loss_D_A: 0.25255244970321655\n",
      "Epoch: 4, Batch: 1080, Loss_G: 1.0923150777816772, Loss_D_A: 0.2558237910270691\n",
      "Epoch: 4, Batch: 1090, Loss_G: 0.9959242343902588, Loss_D_A: 0.25103670358657837\n",
      "Epoch: 4, Batch: 1100, Loss_G: 1.3516587018966675, Loss_D_A: 0.2561928629875183\n",
      "Epoch: 4, Batch: 1110, Loss_G: 1.144180178642273, Loss_D_A: 0.2526797652244568\n",
      "Epoch: 4, Batch: 1120, Loss_G: 1.0488409996032715, Loss_D_A: 0.2526860535144806\n",
      "Epoch: 4, Batch: 1130, Loss_G: 1.1552658081054688, Loss_D_A: 0.30219581723213196\n",
      "Epoch: 4, Batch: 1140, Loss_G: 1.019284725189209, Loss_D_A: 0.25714311003685\n",
      "Epoch: 4, Batch: 1150, Loss_G: 0.907055139541626, Loss_D_A: 0.25675085186958313\n",
      "Epoch: 4, Batch: 1160, Loss_G: 1.2430063486099243, Loss_D_A: 0.25022605061531067\n",
      "Epoch: 4, Batch: 1170, Loss_G: 1.1905099153518677, Loss_D_A: 0.26804590225219727\n",
      "Epoch: 4, Batch: 1180, Loss_G: 0.9837623834609985, Loss_D_A: 0.25181126594543457\n",
      "Epoch: 4, Batch: 1190, Loss_G: 1.3253593444824219, Loss_D_A: 0.2510501444339752\n",
      "Epoch: 4, Batch: 1200, Loss_G: 1.1426793336868286, Loss_D_A: 0.260059118270874\n",
      "Epoch: 4, Batch: 1210, Loss_G: 1.1429202556610107, Loss_D_A: 0.25919288396835327\n",
      "Epoch: 4, Batch: 1220, Loss_G: 1.6149358749389648, Loss_D_A: 0.533596932888031\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_A = G_AA(real_A)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        loss_perceptual = perceptual_loss(fake_A,real_A)\n",
    "\n",
    "        \n",
    "        # 循环一致性损失\n",
    "        loss_cycle_BAB = criterion_cycle(fake_A, real_A) * 10.0\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_GAN_A2B + loss_cycle_BAB + loss_perceptual\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 使用测试集中的数据生成图像\n",
    "        for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "            real_A = real_A.to(device)\n",
    "            fake_A = G_AA(real_A)\n",
    "            vutils.save_image(fake_A, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at models\\Generator_PerceptualLoss.ckpt\n"
     ]
    }
   ],
   "source": [
    "save_model_path='models'\n",
    "checkpoint_path = os.path.join(save_model_path, \"Generator_PerceptualLoss.ckpt\")\n",
    "torch.save(G_AA.state_dict(), checkpoint_path)\n",
    "print(\"Model saved at %s\" % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AA = Generator(input_nc=3, output_nc=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (10): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResidualBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (19): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (22): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (25): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AA.load_state_dict(torch.load(\"models\\Generator_PerceptualLoss.ckpt\"))\n",
    "G_AA.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # 根据你的模型调整尺寸\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # 添加批次维度\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path1,image_path2=None):\n",
    "    image1 = load_image(image_path1).to(device)\n",
    "    image2 = load_image(image_path2).to(device)\n",
    "\n",
    "    model= model.to(device)\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():  # 不计算梯度\n",
    "            image=image1*(i/10)+image2*(1-i/10)\n",
    "            output = model(image)\n",
    "            vutils.save_image(output, f'test2 {i}.png', normalize=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = predict(G_AA, 'human_dog\\\\testA\\\\200601.jpg','human_dog\\\\testB\\\\flickr_dog_000079.jpg')\n",
    "# output_image = output_image - output_image.min()\n",
    "# output_image = output_image / output_image.max()\n",
    "\n",
    "# output_image = output_image.squeeze()  # 假设输出是图像格式，调整通道\n",
    "# output_image = output_image.permute(1,2,0)\n",
    "# output_image=output_image.to('cpu')\n",
    "# # 步骤 5: 可视化输出图像\n",
    "# plt.imshow(output_image.numpy())\n",
    "# plt.title('Output Image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(output_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
