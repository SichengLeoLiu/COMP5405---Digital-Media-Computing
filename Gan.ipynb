{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import glob\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg19, resnet18\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.Conv2d(64, output_nc, kernel_size=7, padding=3), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_patch(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator_patch, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, kernel_size=4, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        model += [nn.Conv2d(512, 1, kernel_size=4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Discriminator_classify(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator_classify, self).__init__()\n",
    "\n",
    "        # 之前的卷积层保持不变\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # 添加一个全局平均池化层\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # 最后一个卷积层，将特征图压缩为1个值\n",
    "        self.final_conv = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "        # 选择性添加，如果您需要输出概率\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.sigmoid(x)  # 如果您需要输出概率\n",
    "        return x.view(-1)  # 改变输出形状以匹配期望的输出形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(128*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"%sA\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"%sB\" % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)\n",
    "G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator_classify(input_nc=3).to(device)\n",
    "D_B = Discriminator_classify(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Inversion\n",
    "\n",
    "Train the model to convert A to A. Then interpolation can be used to generate intermediate image. \n",
    "\n",
    "Using contrastive loss to make the model to learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "        self.transform = transforms_                             ## transform变为tensor数据\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "        if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # # 如果是灰度图，把灰度图转换为RGB图\n",
    "        # if image_A.mode != \"RGB\":\n",
    "        #     image_A = to_rgb(image_A)\n",
    "        # if image_B.mode != \"RGB\":\n",
    "        #     image_B = to_rgb(image_B)\n",
    "        \n",
    "        # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return item_A, item_B\n",
    "\n",
    "    ## 获取A,B数据的长度\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog_colab\", transforms_=transform, unaligned=True, mode=\"train\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=BATCH_SIZE,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog_colab\", transforms_=transform, unaligned=True, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=BATCH_SIZE,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm1 = nn.InstanceNorm2d(in_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm2 = nn.InstanceNorm2d(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        return identity + out\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(DownsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downsample_blocks = nn.ModuleList([\n",
    "            DownsampleBlock(64, 128),\n",
    "            DownsampleBlock(128, 256)\n",
    "        ])\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(n_residual_blocks)]\n",
    "        )\n",
    "        self.upsample_blocks = nn.ModuleList([\n",
    "            UpsampleBlock(256, 128),\n",
    "            UpsampleBlock(128, 64)\n",
    "        ])\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        # print(\"1\",x.shape)\n",
    "        for down_block in self.downsample_blocks:\n",
    "            x = down_block(x)\n",
    "        # print(\"2\",x.shape)\n",
    "        x = self.residual_blocks(x)\n",
    "        # print(\"3\",x.shape)\n",
    "        for up_block in self.upsample_blocks:\n",
    "            x = up_block(x)\n",
    "        # print(\"4\",x.shape)\n",
    "        x = self.output_conv(x)\n",
    "        # print(\"5\",x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)\n",
    "G_BA = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator_classify(input_nc=3).to(device)\n",
    "D_B = Discriminator_classify(input_nc=3).to(device)\n",
    "\n",
    "D_A_P = Discriminator_patch(input_nc=3).to(device)\n",
    "D_B_P = Discriminator_patch(input_nc=3).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = Adam(G_AB.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_D_A = Adam(D_A.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "optimizer_D_B = Adam(D_B.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "\n",
    "optimizer_D_A_P = Adam(D_A_P.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "optimizer_D_B_P = Adam(D_B_P.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
    "\n",
    "G_AB.train()\n",
    "G_BA.train()\n",
    "D_A.train()\n",
    "D_B.train()\n",
    "D_A_P.train()\n",
    "D_B_P.train()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "\n",
    "output_dir = './cyclegan_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 加载ResNet模型\n",
    "resnet = resnet18(pretrained=True)\n",
    "\n",
    "# 获取全连接层之前的特征提取部分\n",
    "features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# 定义新的全连接层和ReLU激活函数\n",
    "num_ftrs = resnet.fc.in_features\n",
    "fc_layer = nn.Linear(num_ftrs, 256)\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "# 定义模型结构\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, features, fc_layer, relu, num_classes=1000):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.features = features\n",
    "        self.fc_layer = fc_layer\n",
    "        self.relu = relu\n",
    "        self.fc_out = nn.Linear(256, num_classes)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet = resnet18(pretrained=True)\n",
    "# num_ftrs = resnet.fc.in_features\n",
    "# resnet.fc = nn.Linear(num_ftrs, 256)\n",
    "# resnet.fc = nn.Linear(32168, 65536)\n",
    "# resnet = torch.load(\"models\\\\animal_rec.pth\")\n",
    "# resnet.fc_out = torch.nn.Identity()\n",
    "# resnet.relu = torch.nn.Identity()\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        # self.vgg = vgg19(pretrained=True).features[:21]  # 只使用到第三个池化层之前的部分\n",
    "        self.resnet = resnet\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        # 冻结VGG参数\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "\n",
    "        self.resnet = self.resnet.to(device)\n",
    "        gen_features = self.resnet(generated)\n",
    "\n",
    "        target_features = self.resnet(target)\n",
    "\n",
    "        return self.loss(gen_features, target_features)\n",
    "\n",
    "# 实例化感知损失\n",
    "perceptual_loss = PerceptualLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet1 = torch.load(\"models\\\\my_animal_rec.pth\")\n",
    "# resnet1 = resnet1.to(device)\n",
    "# resnet1.fc_out = torch.nn.Identity()\n",
    "# resnet1.flatten = torch.nn.Identity()\n",
    "# resnet1.fc_outpout = torch.nn.Identity()\n",
    "# resnet1.average_pool1 = torch.nn.Identity()\n",
    "# resnet1.average_pool2 = torch.nn.Identity()\n",
    "\n",
    "# resnet1.eval()\n",
    "# print(resnet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Sample: 0, Loss_G: 25.777156829833984, Loss_D_A: 0.2497054934501648, Loss_D_B: 0.25125768780708313\n",
      "Epoch: 0, Batch: 10, Sample: 160, Loss_G: 23.995515823364258, Loss_D_A: 0.19202102720737457, Loss_D_B: 0.17317256331443787\n",
      "Epoch: 0, Batch: 20, Sample: 320, Loss_G: 22.434913635253906, Loss_D_A: 0.09354539215564728, Loss_D_B: 0.16209763288497925\n",
      "Epoch: 0, Batch: 30, Sample: 480, Loss_G: 22.679304122924805, Loss_D_A: 0.060604073107242584, Loss_D_B: 0.1714388132095337\n",
      "Epoch: 0, Batch: 40, Sample: 640, Loss_G: 22.715530395507812, Loss_D_A: 0.04219002276659012, Loss_D_B: 0.175270676612854\n",
      "Epoch: 0, Batch: 50, Sample: 800, Loss_G: 21.981338500976562, Loss_D_A: 0.02990548312664032, Loss_D_B: 0.1674671769142151\n",
      "Epoch: 0, Batch: 60, Sample: 960, Loss_G: 22.20755386352539, Loss_D_A: 0.02948971465229988, Loss_D_B: 0.14459393918514252\n",
      "Epoch: 0, Batch: 70, Sample: 1120, Loss_G: 21.58616828918457, Loss_D_A: 0.017695844173431396, Loss_D_B: 0.19405043125152588\n",
      "Epoch: 0, Batch: 80, Sample: 1280, Loss_G: 22.051342010498047, Loss_D_A: 0.01574835553765297, Loss_D_B: 0.16787464916706085\n",
      "Epoch: 0, Batch: 90, Sample: 1440, Loss_G: 22.017154693603516, Loss_D_A: 0.010387539863586426, Loss_D_B: 0.10549597442150116\n",
      "Epoch: 0, Batch: 100, Sample: 1600, Loss_G: 22.348114013671875, Loss_D_A: 0.009266248904168606, Loss_D_B: 0.14894507825374603\n",
      "Epoch: 0, Batch: 110, Sample: 1760, Loss_G: 21.635757446289062, Loss_D_A: 0.007230855990201235, Loss_D_B: 0.18016482889652252\n",
      "Epoch: 0, Batch: 120, Sample: 1920, Loss_G: 22.767393112182617, Loss_D_A: 0.007674196269363165, Loss_D_B: 0.11454583704471588\n",
      "Epoch: 0, Batch: 130, Sample: 2080, Loss_G: 21.73309326171875, Loss_D_A: 0.0058268215507268906, Loss_D_B: 0.080364890396595\n",
      "Epoch: 0, Batch: 140, Sample: 2240, Loss_G: 22.211410522460938, Loss_D_A: 0.005723460111767054, Loss_D_B: 0.0876377671957016\n",
      "Epoch: 0, Batch: 150, Sample: 2400, Loss_G: 20.862016677856445, Loss_D_A: 0.006997808814048767, Loss_D_B: 0.11010763794183731\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m optimizer_D_A_P\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m*\u001b[39mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss_G: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_G\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss_D_A: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_D_A\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss_D_B: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_D_B\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        # print(real_A.shape)\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "        valid = torch.ones(real_A.size(0), 1, device=device)\n",
    "        fake = torch.zeros(real_A.size(0), 1, device=device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_B = G_AB(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        # pred_fake_patch = D_B_P(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        # loss_GAN_A2B_Patch = criterion_GAN(pred_fake_patch, torch.ones_like(pred_fake_patch))\n",
    "        \n",
    "        fake_A = G_BA(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        # pred_fake_patch = D_A_P(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        # loss_GAN_B2A_Patch = criterion_GAN(pred_fake_patch, torch.ones_like(pred_fake_patch))\n",
    "        \n",
    "        # contain domain B\n",
    "        # generated_B = G_AB(real_B)\n",
    "        # loss_cycle_BB = criterion_cycle(generated_B, real_B)\n",
    "        \n",
    "        \n",
    "        # 循环一致性损失\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * 10.0\n",
    "        loss_perceptual_ABA = perceptual_loss(fake_A,real_A) *2\n",
    "        # 在训练循环中使用颜色一致性损失\n",
    "\n",
    "                \n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0\n",
    "        loss_perceptual_BAB = perceptual_loss(fake_B,real_B) *2\n",
    "\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_id_A+loss_id_B+ loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB +loss_perceptual_ABA+loss_perceptual_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        # # 训练判别器 D_B\n",
    "        # optimizer_D_B_P.zero_grad()\n",
    "\n",
    "        # pred_real = D_B_P(real_B)\n",
    "        # loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        # pred_fake = D_B_P(fake_B.detach())\n",
    "        # loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        # loss_D_B_P = (loss_D_real + loss_D_fake) * 0.5\n",
    "        # loss_D_B_P.backward()\n",
    "        # optimizer_D_B_P.step()\n",
    "        \n",
    "        # # 训练判别器 D_B\n",
    "        # optimizer_D_A_P.zero_grad()\n",
    "\n",
    "        # pred_real = D_A_P(real_A)\n",
    "        # loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        # pred_fake = D_A_P(fake_A.detach())\n",
    "        # loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # # 总损失\n",
    "        # loss_D_A_P = (loss_D_real + loss_D_fake) * 0.5\n",
    "        # loss_D_A_P.backward()\n",
    "        # optimizer_D_A_P.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Sample: {i*BATCH_SIZE}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}, Loss_D_B: {loss_D_B.item()}')\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     # 使用测试集中的数据生成图像\n",
    "    #     for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "    #         real_A = real_A.to(device)\n",
    "    #         fake_B = G_AB(real_A)\n",
    "    #         vutils.save_image(fake_B, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at models\\Cycle_GAN_Human2Dog_PerceptualLoss2.ckpt\n"
     ]
    }
   ],
   "source": [
    "save_model_path='models'\n",
    "checkpoint_path = os.path.join(save_model_path, \"Cycle_GAN_Human2Dog_PerceptualLoss2.ckpt\")\n",
    "# checkpoint_path = os.path.join(save_model_path, \"Cycle_GAN_Monet2Photo_PerceptualLoss2.ckpt\")\n",
    "torch.save(G_AB.state_dict(), checkpoint_path)\n",
    "print(\"Model saved at %s\" % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB = Generator(input_nc=3, output_nc=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (initial_conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (downsample_blocks): ModuleList(\n",
       "    (0): DownsampleBlock(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DownsampleBlock(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (residual_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (8): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (upsample_blocks): ModuleList(\n",
       "    (0): UpsampleBlock(\n",
       "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): UpsampleBlock(\n",
       "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (output_conv): Sequential(\n",
       "    (0): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AB.load_state_dict(torch.load(\"models\\Cycle_GAN_Human2Dog_PerceptualLoss2.ckpt\"))\n",
    "G_AB.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for epoch in range(20):\n",
    "        for real_A, real_B in enumerate:\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "\n",
    "            # 真实数据标签\n",
    "            valid = torch.ones(real_A.size(0), 1, device=device)\n",
    "            fake = torch.zeros(real_A.size(0), 1, device=device)\n",
    "\n",
    "            # ------------------\n",
    "            #  训练生成器\n",
    "            # ------------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # 身份损失\n",
    "            loss_id_A = criterion_identity(G_B(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_A(real_B), real_B)\n",
    "\n",
    "            # 对抗损失\n",
    "            fake_B = G_A(real_A)\n",
    "            loss_G_A = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_B(real_B)\n",
    "            loss_G_B = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            # 循环损失\n",
    "            recovered_A = G_B(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
    "            recovered_B = G_A(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
    "\n",
    "            # 总损失\n",
    "            loss_G = loss_id_A + loss_id_B + loss_G_A + loss_G_B + 10 * (loss_cycle_A + loss_cycle_B)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ------------------\n",
    "            #  训练判别器 D_A\n",
    "            # ------------------\n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A.detach()), fake)\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # ------------------\n",
    "            #  训练判别器 D_B\n",
    "            # ------------------\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B.detach()), fake)\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # 根据你的模型调整尺寸\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # 添加批次维度\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image1,image2=None):\n",
    "    # image1 = load_image(image_path1).to(device)\n",
    "    # image2 = load_image(image_path2).to(device)\n",
    "\n",
    "    model= model.to(device)\n",
    "    for i in range(11):\n",
    "        with torch.no_grad():  # 不计算梯度\n",
    "            image=image1*(i/10)+image2*(1-i/10)\n",
    "            output = model(image)\n",
    "            vutils.save_image(output, f'test8 {i}.png', normalize=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = load_image('human_dog_colab\\\\testA\\\\200601.jpg').to(device)\n",
    "image2 = load_image('human_dog_colab\\\\testB\\\\flickr_dog_000043.jpg').to(device)\n",
    "\n",
    "# image1_feature = resnet1(image1)\n",
    "# image1_feature = image1_feature.unsqueeze(1).unsqueeze(1).expand(-1,-1,256,256)\n",
    "# image1_with_feature = torch.cat([image1,image1_feature],dim=1)\n",
    "\n",
    "# image2_feature = resnet1(image2)\n",
    "# image2_feature = image2_feature.unsqueeze(1).unsqueeze(1).expand(-1,-1,256,256)\n",
    "# image2_with_feature = torch.cat([image2,image2_feature],dim=1)\n",
    "\n",
    "output_image = predict(G_AB,image1 ,image2)\n",
    "\n",
    "# output_image = output_image - output_image.min()\n",
    "# output_image = output_image / output_image.max()\n",
    "\n",
    "# output_image = output_image.squeeze()  # 假设输出是图像格式，调整通道\n",
    "# output_image = output_image.permute(1,2,0)\n",
    "# output_image=output_image.to('cpu')\n",
    "# # 步骤 5: 可视化输出图像\n",
    "# plt.imshow(output_image.numpy())\n",
    "# plt.title('Output Image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(output_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss_G: 15.61037826538086, Loss_D_A: 1.1615573167800903, Loss_D_B: 0.9101475477218628\n",
      "Epoch: 0, Batch: 10, Loss_G: 6.970175743103027, Loss_D_A: 0.3666701912879944, Loss_D_B: 0.24359501898288727\n",
      "Epoch: 0, Batch: 20, Loss_G: 5.965689659118652, Loss_D_A: 0.24649454653263092, Loss_D_B: 0.24288181960582733\n",
      "Epoch: 0, Batch: 30, Loss_G: 5.182547569274902, Loss_D_A: 0.24978424608707428, Loss_D_B: 0.2856913208961487\n",
      "Epoch: 0, Batch: 40, Loss_G: 7.472266674041748, Loss_D_A: 0.3758556544780731, Loss_D_B: 0.32502102851867676\n",
      "Epoch: 0, Batch: 50, Loss_G: 7.623085975646973, Loss_D_A: 0.23080357909202576, Loss_D_B: 0.24449694156646729\n",
      "Epoch: 0, Batch: 60, Loss_G: 6.9660868644714355, Loss_D_A: 0.26649582386016846, Loss_D_B: 0.23975417017936707\n",
      "Epoch: 0, Batch: 70, Loss_G: 5.563512325286865, Loss_D_A: 0.24589692056179047, Loss_D_B: 0.2512208819389343\n",
      "Epoch: 0, Batch: 80, Loss_G: 6.209434986114502, Loss_D_A: 0.30268868803977966, Loss_D_B: 0.2863529324531555\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_B = G_AB(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "        fake_A = G_BA(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        \n",
    "        # 循环一致性损失\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * 10.0\n",
    "        \n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}, Loss_D_B: {loss_D_B.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 使用测试集中的数据生成图像\n",
    "        for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "            real_A = real_A.to(device)\n",
    "            fake_B = G_AB(real_A)\n",
    "            vutils.save_image(fake_B, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        # print(real_A.shape)\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # 训练生成器 G_A 和 G_B\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 对抗性损失\n",
    "        fake_B = G_AB(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        pred_fake_patch = D_B_P(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        loss_GAN_A2B_Patch = criterion_GAN(pred_fake_patch, torch.ones_like(pred_fake_patch))\n",
    "        \n",
    "        fake_A = G_BA(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        pred_fake_patch = D_A_P(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "        loss_GAN_B2A_Patch = criterion_GAN(pred_fake_patch, torch.ones_like(pred_fake_patch))\n",
    "        \n",
    "        # contain domain B\n",
    "        # generated_B = G_AB(real_B)\n",
    "        # loss_cycle_BB = criterion_cycle(generated_B, real_B)\n",
    "        \n",
    "        \n",
    "        # 循环一致性损失\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * 10.0\n",
    "        loss_perceptual_ABA = perceptual_loss(fake_A,real_A) *2\n",
    "        # 在训练循环中使用颜色一致性损失\n",
    "\n",
    "                \n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0\n",
    "        loss_perceptual_BAB = perceptual_loss(fake_B,real_B) *2\n",
    "\n",
    "\n",
    "        # 总损失\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB +loss_GAN_A2B_Patch+loss_GAN_B2A_Patch+loss_perceptual_ABA+loss_perceptual_BAB\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # 训练判别器 D_A\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_B_P.zero_grad()\n",
    "\n",
    "        pred_real = D_B_P(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_B_P(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_B_P = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B_P.backward()\n",
    "        optimizer_D_B_P.step()\n",
    "        \n",
    "        # 训练判别器 D_B\n",
    "        optimizer_D_A_P.zero_grad()\n",
    "\n",
    "        pred_real = D_A_P(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "        pred_fake = D_A_P(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "        # 总损失\n",
    "        loss_D_A_P = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A_P.backward()\n",
    "        optimizer_D_A_P.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Sample: {i*BATCH_SIZE}, Loss_G: {loss_G.item()}, Loss_D_A: {loss_D_A.item()}, Loss_D_B: {loss_D_B.item()}')\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     # 使用测试集中的数据生成图像\n",
    "    #     for i, (real_A, real_B) in enumerate(test_dataloader):\n",
    "    #         real_A = real_A.to(device)\n",
    "    #         fake_B = G_AB(real_A)\n",
    "    #         vutils.save_image(fake_B, f'{output_dir}/fake_B_epoch_{epoch}_batch_{i}.png', normalize=True)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
