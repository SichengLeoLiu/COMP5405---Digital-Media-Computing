{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import glob\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg19, resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(64*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, root='data', transforms_=None, unaligned=False, mode=\"train\"):          ## (root = \"./datasets/facades\", unaligned=True:非对其数据)\n",
    "#         self.transform = transforms_                             ## transform变为tensor数据\n",
    "#         self.unaligned = unaligned\n",
    "\n",
    "#         self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + \"/*.*\"))     ## \"./datasets/facades/trainA/*.*\"\n",
    "#         self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + \"/*.*\"))     ## \"./datasets/facades/trainB/*.*\"\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         image_A = Image.open(self.files_A[index % len(self.files_A)])                   ## 在A中取一张照片\n",
    "\n",
    "#         if self.unaligned:                                                              ## 如果采用非配对数据，在B中随机取一张\n",
    "#             image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "#         else:\n",
    "#             image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "#         # # 如果是灰度图，把灰度图转换为RGB图\n",
    "#         # if image_A.mode != \"RGB\":\n",
    "#         #     image_A = to_rgb(image_A)\n",
    "#         # if image_B.mode != \"RGB\":\n",
    "#         #     image_B = to_rgb(image_B)\n",
    "        \n",
    "#         # 把RGB图像转换为tensor图, 方便计算，返回字典数据\n",
    "#         item_A = self.transform(image_A)\n",
    "#         item_B = self.transform(image_B)\n",
    "#         return item_A, item_B, \"human\", \"dog\"\n",
    "\n",
    "#     ## 获取A,B数据的长度\n",
    "#     def __len__(self):\n",
    "#         return max(len(self.files_A), len(self.files_B))\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, mode='train'):\n",
    "        \"\"\"\n",
    "        root_dir: 包含所有图片的目录路径。\n",
    "        transform: 应用于图像的预处理函数。\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.face_images = []  # 存储人脸图像路径\n",
    "        self.dog_images = []   # 存储狗图像路径\n",
    "\n",
    "        # 加载人脸图片\n",
    "        face_dir = os.path.join(root_dir, f'{mode}A')\n",
    "        self.face_images = [os.path.join(face_dir, img_name) for img_name in os.listdir(face_dir)\n",
    "                            if img_name.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        # 加载狗图片\n",
    "        dog_dir = os.path.join(root_dir, f'{mode}B')\n",
    "        self.dog_images = [os.path.join(dog_dir, img_name) for img_name in os.listdir(dog_dir)\n",
    "                           if img_name.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回最小长度以保证均匀抽样\n",
    "        return max(len(self.face_images), len(self.dog_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 随机选择人脸或狗的图片进行返回\n",
    "        if random.random() > 0.5:\n",
    "            img_path = self.face_images[idx % len(self.face_images)]\n",
    "            label = 0\n",
    "        else:\n",
    "            img_path = self.dog_images[idx % len(self.dog_images)]\n",
    "            label = 1\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 确保标签也为tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog_colab\", transform=transform, mode=\"train\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=BATCH_SIZE,                                                                  ## batch_size = 1\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(        ## 改成自己存放文件的目录\n",
    "    ImageDataset(\"human_dog_colab\", transform=transform, mode=\"test\"),  ## \"./datasets/facades\" , unaligned:设置非对其数据\n",
    "    batch_size=BATCH_SIZE,                                                                  ## batch_size = 1\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, n_residual_blocks=6, num_classes=2):\n",
    "        super(Generator, self).__init__()\n",
    "        # Embedding layer for conditional input\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "\n",
    "        # Initial convolution block with modified input channels\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(input_nc + 50, 64, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Downsampling\n",
    "        model = []\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.Conv2d(64, output_nc, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Embed label and expand to match image size\n",
    "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        label_embedding = label_embedding.expand(-1, -1, x.size(2), x.size(3))\n",
    "        # Concatenate label embedding and image in the channel dimension\n",
    "        x = torch.cat((x, label_embedding), 1)\n",
    "        x = self.initial(x)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3, num_classes=2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, input_nc)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 输入层\n",
    "            nn.Conv2d(input_nc + input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 第一层下采样\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 第二层下采样\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 第三层下采样\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 最后一层，没有批量归一化\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "        self.linear = nn.Linear(25,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # 将标签嵌入并调整到与图像的维度匹配\n",
    "        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
    "        label_embedding = label_embedding.expand(-1, -1, img.size(2), img.size(3))\n",
    "        # 将标签嵌入合并到图像通道中\n",
    "        img = torch.cat((img, label_embedding), 1)\n",
    "        img = self.model(img)\n",
    "        img = torch.flatten(img,start_dim=1)\n",
    "        img = self.linear(img)\n",
    "        img = self.sigmoid(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "adversarial_loss = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Sample: 0, Loss_G: 2.0980844497680664, d_loss: 0.7068885564804077\n",
      "Epoch: 0, Batch: 10, Sample: 160, Loss_G: 3.3038153648376465, d_loss: 0.5538271069526672\n",
      "Epoch: 0, Batch: 20, Sample: 320, Loss_G: 3.8670549392700195, d_loss: 0.506575345993042\n",
      "Epoch: 0, Batch: 30, Sample: 480, Loss_G: 3.6204676628112793, d_loss: 1.0642335414886475\n",
      "Epoch: 0, Batch: 40, Sample: 640, Loss_G: 2.079232931137085, d_loss: 0.6405194401741028\n",
      "Epoch: 0, Batch: 50, Sample: 800, Loss_G: 2.1151273250579834, d_loss: 0.8064865469932556\n",
      "Epoch: 0, Batch: 60, Sample: 960, Loss_G: 2.91995906829834, d_loss: 0.5950760841369629\n",
      "Epoch: 0, Batch: 70, Sample: 1120, Loss_G: 2.012485980987549, d_loss: 0.5202176570892334\n",
      "Epoch: 0, Batch: 80, Sample: 1280, Loss_G: 1.9626606702804565, d_loss: 0.7711570858955383\n",
      "Epoch: 0, Batch: 90, Sample: 1440, Loss_G: 1.8230597972869873, d_loss: 0.5065494775772095\n",
      "Epoch: 0, Batch: 100, Sample: 1600, Loss_G: 2.388469696044922, d_loss: 0.6479138731956482\n",
      "Epoch: 0, Batch: 110, Sample: 1760, Loss_G: 1.9930505752563477, d_loss: 0.698867678642273\n",
      "Epoch: 0, Batch: 120, Sample: 1920, Loss_G: 2.1568942070007324, d_loss: 0.43391185998916626\n",
      "Epoch: 0, Batch: 130, Sample: 2080, Loss_G: 1.422521710395813, d_loss: 0.7292811870574951\n",
      "Epoch: 0, Batch: 140, Sample: 2240, Loss_G: 1.7234652042388916, d_loss: 0.5431662797927856\n",
      "Epoch: 0, Batch: 150, Sample: 2400, Loss_G: 1.9144139289855957, d_loss: 0.4712477922439575\n",
      "Epoch: 0, Batch: 160, Sample: 2560, Loss_G: 2.6595351696014404, d_loss: 0.4250028729438782\n",
      "Epoch: 0, Batch: 170, Sample: 2720, Loss_G: 1.6186347007751465, d_loss: 0.5438393354415894\n",
      "Epoch: 0, Batch: 180, Sample: 2880, Loss_G: 2.578319549560547, d_loss: 0.6489672660827637\n",
      "Epoch: 0, Batch: 190, Sample: 3040, Loss_G: 2.5112407207489014, d_loss: 0.4158688187599182\n",
      "Epoch: 0, Batch: 200, Sample: 3200, Loss_G: 1.7032368183135986, d_loss: 0.7030259370803833\n",
      "Epoch: 0, Batch: 210, Sample: 3360, Loss_G: 2.494751453399658, d_loss: 0.6826087236404419\n",
      "Epoch: 0, Batch: 220, Sample: 3520, Loss_G: 2.225031852722168, d_loss: 0.6365704536437988\n",
      "Epoch: 0, Batch: 230, Sample: 3680, Loss_G: 2.2631053924560547, d_loss: 0.699058473110199\n",
      "Epoch: 0, Batch: 240, Sample: 3840, Loss_G: 2.1932687759399414, d_loss: 0.5451644659042358\n",
      "Epoch: 0, Batch: 250, Sample: 4000, Loss_G: 1.882442593574524, d_loss: 0.6541730165481567\n",
      "Epoch: 0, Batch: 260, Sample: 4160, Loss_G: 2.2682809829711914, d_loss: 0.429524302482605\n",
      "Epoch: 0, Batch: 270, Sample: 4320, Loss_G: 2.295037269592285, d_loss: 0.8869296908378601\n",
      "Epoch: 0, Batch: 280, Sample: 4480, Loss_G: 2.2066195011138916, d_loss: 0.6511807441711426\n",
      "Epoch: 0, Batch: 290, Sample: 4640, Loss_G: 1.8889445066452026, d_loss: 0.7057667970657349\n",
      "Epoch: 0, Batch: 300, Sample: 4800, Loss_G: 1.9688116312026978, d_loss: 0.4783894121646881\n",
      "Epoch: 0, Batch: 310, Sample: 4960, Loss_G: 1.832395315170288, d_loss: 0.4360508918762207\n",
      "Epoch: 0, Batch: 320, Sample: 5120, Loss_G: 2.65679669380188, d_loss: 0.5104265809059143\n",
      "Epoch: 0, Batch: 330, Sample: 5280, Loss_G: 1.699454426765442, d_loss: 0.585516631603241\n",
      "Epoch: 0, Batch: 340, Sample: 5440, Loss_G: 1.8271692991256714, d_loss: 0.5593667030334473\n",
      "Epoch: 0, Batch: 350, Sample: 5600, Loss_G: 2.4522290229797363, d_loss: 0.40180182456970215\n",
      "Epoch: 0, Batch: 360, Sample: 5760, Loss_G: 2.0696146488189697, d_loss: 0.48907485604286194\n",
      "Epoch: 0, Batch: 370, Sample: 5920, Loss_G: 2.363213062286377, d_loss: 0.6331636905670166\n",
      "Epoch: 0, Batch: 380, Sample: 6080, Loss_G: 2.523656129837036, d_loss: 0.6449154615402222\n",
      "Epoch: 0, Batch: 390, Sample: 6240, Loss_G: 2.117149829864502, d_loss: 0.7167584896087646\n",
      "Epoch: 0, Batch: 400, Sample: 6400, Loss_G: 1.8746166229248047, d_loss: 0.7369530200958252\n",
      "Epoch: 0, Batch: 410, Sample: 6560, Loss_G: 1.708786964416504, d_loss: 0.7123591303825378\n",
      "Epoch: 0, Batch: 420, Sample: 6720, Loss_G: 1.5347826480865479, d_loss: 0.7669674158096313\n",
      "Epoch: 0, Batch: 430, Sample: 6880, Loss_G: 2.190361976623535, d_loss: 0.5835030674934387\n",
      "Epoch: 0, Batch: 440, Sample: 7040, Loss_G: 1.6443265676498413, d_loss: 0.6423351764678955\n",
      "Epoch: 0, Batch: 450, Sample: 7200, Loss_G: 1.475601315498352, d_loss: 0.7713967561721802\n",
      "Epoch: 0, Batch: 460, Sample: 7360, Loss_G: 1.8059301376342773, d_loss: 0.519770622253418\n",
      "Epoch: 0, Batch: 470, Sample: 7520, Loss_G: 1.5891607999801636, d_loss: 0.5909458994865417\n",
      "Epoch: 0, Batch: 480, Sample: 7680, Loss_G: 1.8409438133239746, d_loss: 0.5626792311668396\n",
      "Epoch: 0, Batch: 490, Sample: 7840, Loss_G: 1.7647085189819336, d_loss: 0.6229783892631531\n",
      "Epoch: 0, Batch: 500, Sample: 8000, Loss_G: 1.8392314910888672, d_loss: 0.7419030666351318\n",
      "Epoch: 0, Batch: 510, Sample: 8160, Loss_G: 1.8868792057037354, d_loss: 0.6207016706466675\n",
      "Epoch: 0, Batch: 520, Sample: 8320, Loss_G: 1.7959294319152832, d_loss: 0.672967791557312\n",
      "Epoch: 0, Batch: 530, Sample: 8480, Loss_G: 2.0685713291168213, d_loss: 0.5549999475479126\n",
      "Epoch: 0, Batch: 540, Sample: 8640, Loss_G: 1.929709553718567, d_loss: 0.4260134696960449\n",
      "Epoch: 0, Batch: 550, Sample: 8800, Loss_G: 2.0220947265625, d_loss: 0.5585611462593079\n",
      "Epoch: 0, Batch: 560, Sample: 8960, Loss_G: 2.51641845703125, d_loss: 0.45698654651641846\n",
      "Epoch: 0, Batch: 570, Sample: 9120, Loss_G: 1.8776566982269287, d_loss: 0.8770215511322021\n",
      "Epoch: 0, Batch: 580, Sample: 9280, Loss_G: 2.3663692474365234, d_loss: 0.43216103315353394\n",
      "Epoch: 0, Batch: 590, Sample: 9440, Loss_G: 1.643218755722046, d_loss: 0.728195309638977\n",
      "Epoch: 0, Batch: 600, Sample: 9600, Loss_G: 2.0799977779388428, d_loss: 0.502129077911377\n",
      "Epoch: 0, Batch: 610, Sample: 9760, Loss_G: 1.9770395755767822, d_loss: 0.5407348871231079\n",
      "Epoch: 0, Batch: 620, Sample: 9920, Loss_G: 1.5383360385894775, d_loss: 0.5993168354034424\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for i, (imgsA, labelsA) in enumerate(train_dataloader):\n",
    "        valid = torch.ones(imgsA.size(0), 1, requires_grad=False).to(device)\n",
    "        fake = torch.zeros(imgsA.size(0), 1, requires_grad=False).to(device)\n",
    "        imgsA = imgsA.to(device)\n",
    "        labelsA = labelsA.to(device)\n",
    "        # print(valid.shape)\n",
    "        # print(discriminator(imgsA, labelsA).shape)\n",
    "        # 训练鉴别器\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(imgsA, labelsA), valid)\n",
    "        noise = torch.randn(imgsA.size(0), 3,64,64).to(device)\n",
    "        gen_imgs = generator(noise, labelsA)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), labelsA), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # 训练生成器\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs, labelsA), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if i%10==0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Sample: {i*BATCH_SIZE}, Loss_G: {g_loss.item()}, d_loss: {d_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 128, 128).to(device)\n",
    "labels = torch.tensor([0]).to(device)\n",
    "output = generator(x, labels)\n",
    "vutils.save_image(output, f'test8 {i}.png', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
